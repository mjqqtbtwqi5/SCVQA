{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bf6ba567be86410ab2f25b2b7d45124e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8ee9cdc7b884b108aeff32a750bb7fc",
              "IPY_MODEL_15d45429ce784e6081159563cd81a509",
              "IPY_MODEL_ec615ea2a6264c11969e55a0c65a6e7e"
            ],
            "layout": "IPY_MODEL_5706824926354694b467dc05fa738803"
          }
        },
        "b8ee9cdc7b884b108aeff32a750bb7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b238816af48d40cca9e0b3ef0ff477ea",
            "placeholder": "​",
            "style": "IPY_MODEL_442ccede224a4f9aafb03be02a2717fa",
            "value": "100%"
          }
        },
        "15d45429ce784e6081159563cd81a509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10b3dbad06294b009bc8a956a08697bb",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ee18f6a76e4482b87cb6c080950714c",
            "value": 1000
          }
        },
        "ec615ea2a6264c11969e55a0c65a6e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c359276257a84fec9cd15713e0cbcd1b",
            "placeholder": "​",
            "style": "IPY_MODEL_5acef0e429684c9fba0d6f9e363d447e",
            "value": " 1000/1000 [43:47&lt;00:00,  2.61s/it]"
          }
        },
        "5706824926354694b467dc05fa738803": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b238816af48d40cca9e0b3ef0ff477ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "442ccede224a4f9aafb03be02a2717fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10b3dbad06294b009bc8a956a08697bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ee18f6a76e4482b87cb6c080950714c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c359276257a84fec9cd15713e0cbcd1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5acef0e429684c9fba0d6f9e363d447e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "uRXpIJ0q6d1J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR2PT9Ya0InI",
        "outputId": "f196592a-5faa-4791-aa05-12045827f9a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Append project path"
      ],
      "metadata": {
        "id": "wNxdwvmP6iLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "PROJECT_PATH_UTIL = \"/content/drive/MyDrive/Colab Notebooks/SCVQA/util\"\n",
        "#                    /content/drive/MyDrive/.../util\n",
        "PROJECT_PATH_VSFA = \"/content/drive/MyDrive/Colab Notebooks/SCVQA/VSFA\"\n",
        "#                    /content/drive/MyDrive/.../VSFA\n",
        "sys.path.append(PROJECT_PATH_UTIL)\n",
        "sys.path.append(PROJECT_PATH_VSFA)"
      ],
      "metadata": {
        "id": "2r3bOPsO6h_K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install requirements"
      ],
      "metadata": {
        "id": "6AfNotnR6sdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install av scikit-video"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J5pLsoq6sEZ",
        "outputId": "3df8fd87-69c5-4a57-a810-c35de773f054"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: scikit-video in /usr/local/lib/python3.10/dist-packages (1.1.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-video) (1.25.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from scikit-video) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scikit-video) (1.11.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries and py script coded"
      ],
      "metadata": {
        "id": "G3EidBIl6r2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "from dataset import FeatureDataset\n",
        "from model import LSTM, Transformer\n",
        "from engine import Engine\n",
        "\n",
        "from VSFA import VSFA"
      ],
      "metadata": {
        "id": "Ao2BfgxX67XI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom function"
      ],
      "metadata": {
        "id": "m8M9SrxioC98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mos_normalization(feature_data_list: list, mos_max: float, mos_min: float):\n",
        "    for i in range(len(feature_data_list)):\n",
        "        data_tup = feature_data_list[i]\n",
        "        data_list = list(data_tup)\n",
        "        mos = data_list[1]\n",
        "        mos = np.float32((mos - mos_min) / (mos_max - mos_min))  # normalization\n",
        "        data_list[1] = mos\n",
        "        feature_data_list[i] = tuple(data_list)\n",
        "\n",
        "\n",
        "def get_mos_max_min(feature_data_list: list):\n",
        "    mos_list = [data[1] for data in feature_data_list]\n",
        "    return max(mos_list), min(mos_list)"
      ],
      "metadata": {
        "id": "kcySwzhOoCkI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup parameters"
      ],
      "metadata": {
        "id": "MMBiBWgb7NwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_LSTM = \"LSTM\"\n",
        "_TRANSFORMER = \"Transformer\"\n",
        "_VSFA_GRU = \"VSFA_GRU\"\n",
        "\n",
        "_CSCVQ = \"CSCVQ\"\n",
        "_SCVD = \"SCVD\"\n",
        "\n",
        "_ResNet50 = \"ResNet50\"\n",
        "\n",
        "MODEL = _VSFA_GRU\n",
        "DATABASE = _SCVD\n",
        "CNN_EXTRACTION = _ResNet50\n",
        "BATCH_SIZE = 32  # CSCVQ:8, SCVD:32\n",
        "NUM_WORKERS = 0\n",
        "NUM_EPOCHS = 1000\n",
        "LEARNING_RATE = 0.00001\n",
        "SEED = 22035001\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "info = {\n",
        "    \"DATE_TIME\": None,\n",
        "    \"TOTAL_TIME\": None,\n",
        "    \"DIR\": None,\n",
        "    \"LOSS_VAL_CRITERION\": None,\n",
        "    \"RMSE_VAL_CRITERION\": None,\n",
        "    \"PLCC_VAL_CRITERION\": None,\n",
        "    \"SROCC_VAL_CRITERION\": None,\n",
        "    \"TRAIN_DATA_SIZE\": None,\n",
        "    \"TEST_DATA_SIZE\": None,\n",
        "    \"MODEL\": MODEL,\n",
        "    \"DATABASE\": DATABASE,\n",
        "    \"CNN_EXTRACTION\": CNN_EXTRACTION,\n",
        "    \"DEVICE\": DEVICE,\n",
        "    \"BATCH_SIZE\": BATCH_SIZE,\n",
        "    \"NUM_WORKERS\": NUM_WORKERS,\n",
        "    \"NUM_EPOCHS\": NUM_EPOCHS,\n",
        "    \"SEED\": SEED,\n",
        "    \"LEARNING_RATE\": LEARNING_RATE,\n",
        "}\n",
        "\n",
        "FEATURE_DIR = Path(f\"/content/drive/MyDrive/Colab Notebooks/SCVQA/feature/{DATABASE}/{CNN_EXTRACTION}/\")\n",
        "#                    /content/drive/MyDrive/.../feature/{DATABASE}/{CNN_EXTRACTION}/\n",
        "\n",
        "MODEL_DIR = Path(f\"/content/drive/MyDrive/Colab Notebooks/SCVQA/model/{MODEL}/{DATABASE}/{CNN_EXTRACTION}/\")\n",
        "#                  /content/drive/MyDrive/.../model/{MODEL}/{DATABASE}/{CNN_EXTRACTION}/\n",
        "\n",
        "MODEL_DIR_HIST_FILE = Path(f\"/content/drive/MyDrive/Colab Notebooks/SCVQA/model/{MODEL}/{DATABASE}/{CNN_EXTRACTION}/history.csv\")\n",
        "#                            /content/drive/MyDrive/.../model/{MODEL}/{DATABASE}/{CNN_EXTRACTION}/history.csv\n",
        "\n",
        "\n",
        "(\n",
        "    # VSFA GRU\n",
        "    print(\n",
        "        f\"[VSFA GRU-based] | database: {DATABASE}, CNN extraction: {CNN_EXTRACTION}\"\n",
        "    )\n",
        "    if MODEL == _VSFA_GRU\n",
        "    else\n",
        "    # proposed LSTM/Transformer\n",
        "    print(\n",
        "        f\"[{MODEL}-based] | database: {DATABASE}, CNN extraction: {CNN_EXTRACTION}\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"device: {DEVICE}, batch_size: {BATCH_SIZE}, num_workers: {NUM_WORKERS}, num_epochs: {NUM_EPOCHS}, seed: {SEED}, learning_rate: {LEARNING_RATE}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hq9HOS2V7NIm",
        "outputId": "31d4533b-afaa-4b27-eb12-1eb8a9fefa0b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VSFA GRU-based] | database: SCVD, CNN extraction: ResNet50\n",
            "device: cuda, batch_size: 32, num_workers: 0, num_epochs: 1000, seed: 22035001, learning_rate: 1e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "KnoXLGKJ7rqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_data_list = list()\n",
        "MOS_MAX, MOS_MIN = None, None\n",
        "\n",
        "if not os.path.exists(FEATURE_DIR):\n",
        "    print(f\"Video feature not exists in {FEATURE_DIR}/\")\n",
        "    sys.exit()\n",
        "else:\n",
        "    video_feature_dir_list = [f.path for f in os.scandir(FEATURE_DIR) if f.is_dir()]\n",
        "\n",
        "    for video_feature_dir in video_feature_dir_list:\n",
        "        feature_file = f\"{video_feature_dir}/feature.npy\"\n",
        "        mos_file = f\"{video_feature_dir}/mos.npy\"\n",
        "\n",
        "        feature = np.load(feature_file)\n",
        "        feature = torch.from_numpy(feature)\n",
        "        # [frames, feature] | Tensor | torch.Size([300, 4096])\n",
        "\n",
        "        mos = np.load(mos_file)\n",
        "        mos = np.float32(mos.item())\n",
        "        # mos | float\n",
        "\n",
        "        feature_data_list.append((feature, mos))\n",
        "\n",
        "    MOS_MAX, MOS_MIN = get_mos_max_min(feature_data_list=feature_data_list)\n",
        "    mos_normalization(\n",
        "        feature_data_list=feature_data_list, mos_max=MOS_MAX, mos_min=MOS_MIN\n",
        "    )\n",
        "\n",
        "random.seed(SEED)\n",
        "random.shuffle(feature_data_list)\n",
        "\n",
        "TRAIN_SPLIT = int(0.8 * len(feature_data_list))\n",
        "train_data_list = feature_data_list[:TRAIN_SPLIT]\n",
        "test_data_list = feature_data_list[TRAIN_SPLIT:]\n",
        "\n",
        "train_dataset = FeatureDataset(dataset=train_data_list)\n",
        "test_dataset = FeatureDataset(dataset=test_data_list)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    shuffle=True,\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "info[\"TRAIN_DATA_SIZE\"] = len(train_dataset)\n",
        "info[\"TEST_DATA_SIZE\"] = len(test_dataset)\n",
        "\n",
        "print(\n",
        "    f\"Number of training data: {info['TRAIN_DATA_SIZE']} & testing data: {info['TEST_DATA_SIZE']}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnlqpFH379k5",
        "outputId": "e2cb2743-f722-457a-d5a4-1aad9543177f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training data: 640 & testing data: 160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and testing step"
      ],
      "metadata": {
        "id": "qsfPOm8M8DbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "model = None\n",
        "if MODEL == _LSTM:\n",
        "    model = LSTM(device=DEVICE).to(device=DEVICE)\n",
        "elif MODEL == _TRANSFORMER:\n",
        "    model = Transformer(device=DEVICE).to(device=DEVICE)\n",
        "else:\n",
        "    model = VSFA().to(device=DEVICE)\n",
        "\n",
        "if os.path.exists(MODEL_DIR_HIST_FILE):\n",
        "    hist_df = pd.read_csv(MODEL_DIR_HIST_FILE)\n",
        "    model_file = Path(MODEL_DIR / hist_df[\"DIR\"].iloc[-1] / \"model.pt\")\n",
        "    if os.path.exists(model_file):\n",
        "        print(f\"Load model from {model_file}\")\n",
        "        model.load_state_dict(torch.load(f=str(model_file)))\n",
        "\n",
        "loss_fn = nn.L1Loss() if MODEL == _VSFA_GRU else nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "engine = Engine(device=DEVICE, epochs=NUM_EPOCHS, mos_max=MOS_MAX, mos_min=MOS_MIN)\n",
        "\n",
        "start_time = timer()\n",
        "model_results = engine.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    train_dataloader=train_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        ")\n",
        "end_time = timer()\n",
        "total_time = (\n",
        "    f\"{datetime.timedelta(seconds=int(end_time-start_time))} (Hour:Minute:Second)\"\n",
        ")\n",
        "print(f\"Total training & testing time: {total_time}\")\n",
        "\n",
        "info[\"TOTAL_TIME\"] = total_time\n",
        "\n",
        "info[\"LOSS_VAL_CRITERION\"] = model_results[f\"test_{type(loss_fn).__name__}\"][-1]\n",
        "info[\"RMSE_VAL_CRITERION\"] = model_results[\"test_RMSE\"][-1]\n",
        "info[\"PLCC_VAL_CRITERION\"] = model_results[\"test_PLCC\"][-1]\n",
        "info[\"SROCC_VAL_CRITERION\"] = model_results[\"test_SROCC\"][-1]\n",
        "\n",
        "now = datetime.datetime.now()\n",
        "date_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "info[\"DATE_TIME\"] = date_time\n",
        "\n",
        "# Save model, result, history, prediction\n",
        "dir = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "info[\"DIR\"] = dir\n",
        "\n",
        "model_dir = Path(MODEL_DIR / dir)\n",
        "model_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# model\n",
        "model_file = model_dir / \"model.pt\"\n",
        "torch.save(\n",
        "    obj=model.state_dict(),\n",
        "    f=str(model_file),\n",
        ")\n",
        "\n",
        "# result\n",
        "result_file = model_dir / \"result.csv\"\n",
        "model_results_df = pd.DataFrame(model_results)\n",
        "model_results_df.to_csv(str(result_file), index=False)\n",
        "\n",
        "# history\n",
        "info_df = pd.DataFrame(info, index=[0])\n",
        "if os.path.exists(MODEL_DIR_HIST_FILE):\n",
        "    info_df.to_csv(str(MODEL_DIR_HIST_FILE), mode=\"a\", index=False, header=False)\n",
        "else:\n",
        "    info_df.to_csv(str(MODEL_DIR_HIST_FILE), index=False)\n",
        "\n",
        "# prediction of last epoch\n",
        "prediction_file = model_dir / \"prediction.csv\"\n",
        "model_prediction_df = pd.DataFrame(engine.get_prediction()[-1])\n",
        "model_prediction_df.to_csv(str(prediction_file), index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bf6ba567be86410ab2f25b2b7d45124e",
            "b8ee9cdc7b884b108aeff32a750bb7fc",
            "15d45429ce784e6081159563cd81a509",
            "ec615ea2a6264c11969e55a0c65a6e7e",
            "5706824926354694b467dc05fa738803",
            "b238816af48d40cca9e0b3ef0ff477ea",
            "442ccede224a4f9aafb03be02a2717fa",
            "10b3dbad06294b009bc8a956a08697bb",
            "8ee18f6a76e4482b87cb6c080950714c",
            "c359276257a84fec9cd15713e0cbcd1b",
            "5acef0e429684c9fba0d6f9e363d447e"
          ]
        },
        "id": "BcmFrV918DMT",
        "outputId": "cf46c8b6-2774-43c1-93f3-08e08168d744"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf6ba567be86410ab2f25b2b7d45124e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m串流輸出內容已截斷至最後 5000 行。\u001b[0m\n",
            "Training batch[19]: last record -> y: 27.261600708901653 | y_pred: 26.821956018637167\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.609971938188664\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.88925173852442\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.649293954212794\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.79140970195397\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.71747338842104\n",
            "[Training] Epoch: 815 | L1Loss: 0.05225 | RMSE: 0.08562 | PLCC: 0.94164 | SROCC: 0.94172\n",
            "[Testing]  Epoch: 815 | L1Loss: 0.07604 | RMSE: 0.10385 | PLCC: 0.91668 | SROCC: 0.92998\n",
            "Training batch[0]: last record -> y: 70.80740411708621 | y_pred: 69.05732102989737\n",
            "Training batch[1]: last record -> y: 66.50039818303662 | y_pred: 57.568104890837276\n",
            "Training batch[2]: last record -> y: 34.048697754381124 | y_pred: 35.87233121744566\n",
            "Training batch[3]: last record -> y: 68.83770070426726 | y_pred: 68.35735662236266\n",
            "Training batch[4]: last record -> y: 38.542598864858405 | y_pred: 39.87852468541894\n",
            "Training batch[5]: last record -> y: 54.38159841678544 | y_pred: 49.503936637725246\n",
            "Training batch[6]: last record -> y: 41.19950146484996 | y_pred: 42.03793586857\n",
            "Training batch[7]: last record -> y: 48.53810250450829 | y_pred: 41.08002813635642\n",
            "Training batch[8]: last record -> y: 50.21400001022266 | y_pred: 53.27163532915006\n",
            "Training batch[9]: last record -> y: 61.02579752021575 | y_pred: 60.85291224550065\n",
            "Training batch[10]: last record -> y: 56.15299868224588 | y_pred: 59.59963390985945\n",
            "Training batch[11]: last record -> y: 70.58750076313868 | y_pred: 65.56361304712641\n",
            "Training batch[12]: last record -> y: 46.59859789153563 | y_pred: 44.92655664664619\n",
            "Training batch[13]: last record -> y: 66.95040034282079 | y_pred: 63.33740235671644\n",
            "Training batch[14]: last record -> y: 30.038900499706983 | y_pred: 30.60689089337592\n",
            "Training batch[15]: last record -> y: 38.3894998425161 | y_pred: 38.03107181645828\n",
            "Training batch[16]: last record -> y: 72.15129975776699 | y_pred: 68.01919889142505\n",
            "Training batch[17]: last record -> y: 33.262500568553776 | y_pred: 47.09044321506656\n",
            "Training batch[18]: last record -> y: 53.64059811945481 | y_pred: 53.909828806406495\n",
            "Training batch[19]: last record -> y: 41.8042976006501 | y_pred: 37.625863712253135\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.44605512328201\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.51886737417078\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.75136571167741\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.3557665587864\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.400311768549443\n",
            "[Training] Epoch: 816 | L1Loss: 0.05328 | RMSE: 0.08674 | PLCC: 0.94061 | SROCC: 0.94252\n",
            "[Testing]  Epoch: 816 | L1Loss: 0.07580 | RMSE: 0.10327 | PLCC: 0.91504 | SROCC: 0.92879\n",
            "Training batch[0]: last record -> y: 40.97670028687969 | y_pred: 40.75883119896366\n",
            "Training batch[1]: last record -> y: 37.43249908741063 | y_pred: 49.50125108494058\n",
            "Training batch[2]: last record -> y: 47.959997868779965 | y_pred: 45.9356876799676\n",
            "Training batch[3]: last record -> y: 38.89670106038284 | y_pred: 38.52572330143187\n",
            "Training batch[4]: last record -> y: 69.23919566992163 | y_pred: 68.29037861428992\n",
            "Training batch[5]: last record -> y: 48.79289874727124 | y_pred: 54.49727659284076\n",
            "Training batch[6]: last record -> y: 69.64639944538771 | y_pred: 66.32152750265413\n",
            "Training batch[7]: last record -> y: 36.83800132288775 | y_pred: 37.01933724423964\n",
            "Training batch[8]: last record -> y: 41.65330199330322 | y_pred: 40.846261219201324\n",
            "Training batch[9]: last record -> y: 52.99509736563937 | y_pred: 52.79172222218335\n",
            "Training batch[10]: last record -> y: 40.03480134387053 | y_pred: 39.78137843169327\n",
            "Training batch[11]: last record -> y: 39.217899605891034 | y_pred: 47.56119810460291\n",
            "Training batch[12]: last record -> y: 63.3563009293664 | y_pred: 62.54278749061882\n",
            "Training batch[13]: last record -> y: 53.02539747675837 | y_pred: 53.32663930965714\n",
            "Training batch[14]: last record -> y: 34.47520052324171 | y_pred: 37.41267263215184\n",
            "Training batch[15]: last record -> y: 56.6087997063064 | y_pred: 56.17400066989126\n",
            "Training batch[16]: last record -> y: 37.98649968080997 | y_pred: 39.57834099247964\n",
            "Training batch[17]: last record -> y: 45.79939989643424 | y_pred: 46.770214363168634\n",
            "Training batch[18]: last record -> y: 60.41220177672835 | y_pred: 60.293252693868226\n",
            "Training batch[19]: last record -> y: 67.38690076537137 | y_pred: 69.76584061157246\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.77406725390915\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.94856868464137\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.03976658893487\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.4420178304066\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.662313822745148\n",
            "[Training] Epoch: 817 | L1Loss: 0.05139 | RMSE: 0.08529 | PLCC: 0.94250 | SROCC: 0.94340\n",
            "[Testing]  Epoch: 817 | L1Loss: 0.07675 | RMSE: 0.10419 | PLCC: 0.91419 | SROCC: 0.92872\n",
            "Training batch[0]: last record -> y: 54.97959865673647 | y_pred: 63.17184043971474\n",
            "Training batch[1]: last record -> y: 60.54589727817256 | y_pred: 61.35648394681948\n",
            "Training batch[2]: last record -> y: 60.352402395979425 | y_pred: 53.376394401368316\n",
            "Training batch[3]: last record -> y: 68.26640161308069 | y_pred: 67.86453999710602\n",
            "Training batch[4]: last record -> y: 36.45529879426431 | y_pred: 39.0363079941842\n",
            "Training batch[5]: last record -> y: 30.740801078231357 | y_pred: 30.905649796035448\n",
            "Training batch[6]: last record -> y: 56.61489768005458 | y_pred: 61.34767790667047\n",
            "Training batch[7]: last record -> y: 68.4131003359721 | y_pred: 64.40533573921402\n",
            "Training batch[8]: last record -> y: 72.168403673586 | y_pred: 66.92315886726851\n",
            "Training batch[9]: last record -> y: 65.59839658409192 | y_pred: 61.95414326629725\n",
            "Training batch[10]: last record -> y: 56.49319871979219 | y_pred: 53.85206529980451\n",
            "Training batch[11]: last record -> y: 21.597000101274666 | y_pred: 14.827109388466283\n",
            "Training batch[12]: last record -> y: 70.78559807172087 | y_pred: 66.49530367332295\n",
            "Training batch[13]: last record -> y: 59.352698135366836 | y_pred: 59.48609452733922\n",
            "Training batch[14]: last record -> y: 28.998800379243562 | y_pred: 28.479246422625067\n",
            "Training batch[15]: last record -> y: 32.16310119124324 | y_pred: 35.81818436246806\n",
            "Training batch[16]: last record -> y: 45.65990070636735 | y_pred: 42.975758238938965\n",
            "Training batch[17]: last record -> y: 65.45760286109589 | y_pred: 64.03309560964271\n",
            "Training batch[18]: last record -> y: 60.042196927618534 | y_pred: 59.980985621513355\n",
            "Training batch[19]: last record -> y: 30.995199312422926 | y_pred: 30.688309778099153\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.90647626300142\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.84522329974834\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.00641940676428\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.43559823357043\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.93492959269787\n",
            "[Training] Epoch: 818 | L1Loss: 0.05112 | RMSE: 0.08464 | PLCC: 0.94275 | SROCC: 0.94345\n",
            "[Testing]  Epoch: 818 | L1Loss: 0.08085 | RMSE: 0.10897 | PLCC: 0.91645 | SROCC: 0.93269\n",
            "Training batch[0]: last record -> y: 40.115501400992116 | y_pred: 36.331557527393215\n",
            "Training batch[1]: last record -> y: 46.11660066695879 | y_pred: 39.18880075700565\n",
            "Training batch[2]: last record -> y: 39.67380194122427 | y_pred: 56.50422074301741\n",
            "Training batch[3]: last record -> y: 49.39419884010499 | y_pred: 55.09742849125041\n",
            "Training batch[4]: last record -> y: 54.78369803384521 | y_pred: 54.07700205507899\n",
            "Training batch[5]: last record -> y: 27.153799082271064 | y_pred: 25.923173866803182\n",
            "Training batch[6]: last record -> y: 30.080500033964597 | y_pred: 29.565033906794383\n",
            "Training batch[7]: last record -> y: 43.297297704920425 | y_pred: 44.810436238844886\n",
            "Training batch[8]: last record -> y: 65.49870307550941 | y_pred: 66.62428096406643\n",
            "Training batch[9]: last record -> y: 56.977000249892626 | y_pred: 54.755736122575854\n",
            "Training batch[10]: last record -> y: 66.61560035692173 | y_pred: 66.9216922659873\n",
            "Training batch[11]: last record -> y: 58.383400121492286 | y_pred: 56.150805212785826\n",
            "Training batch[12]: last record -> y: 65.80919800464949 | y_pred: 60.867552528465694\n",
            "Training batch[13]: last record -> y: 64.42900076602791 | y_pred: 61.28690722419583\n",
            "Training batch[14]: last record -> y: 43.91579820049958 | y_pred: 43.885924239972155\n",
            "Training batch[15]: last record -> y: 28.777399065763746 | y_pred: 28.501016285393007\n",
            "Training batch[16]: last record -> y: 34.00809927198486 | y_pred: 40.14995527429244\n",
            "Training batch[17]: last record -> y: 67.35350020768419 | y_pred: 67.78207583734684\n",
            "Training batch[18]: last record -> y: 57.104697480745926 | y_pred: 53.4711767253973\n",
            "Training batch[19]: last record -> y: 70.18830218633252 | y_pred: 64.934653368726\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.60534699818345\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.07922163167325\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.988826469998\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.946866221531195\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.08099898656988\n",
            "[Training] Epoch: 819 | L1Loss: 0.05219 | RMSE: 0.08481 | PLCC: 0.94279 | SROCC: 0.94409\n",
            "[Testing]  Epoch: 819 | L1Loss: 0.07631 | RMSE: 0.10407 | PLCC: 0.91606 | SROCC: 0.92764\n",
            "Training batch[0]: last record -> y: 56.49319871979219 | y_pred: 54.90280149578757\n",
            "Training batch[1]: last record -> y: 73.27570372205287 | y_pred: 65.18842363383828\n",
            "Training batch[2]: last record -> y: 28.118200384631052 | y_pred: 27.90141999086785\n",
            "Training batch[3]: last record -> y: 62.24660157266317 | y_pred: 64.63381034845634\n",
            "Training batch[4]: last record -> y: 31.007400888323332 | y_pred: 29.29965867660991\n",
            "Training batch[5]: last record -> y: 29.49510018254307 | y_pred: 48.04834773104926\n",
            "Training batch[6]: last record -> y: 53.07020278914547 | y_pred: 58.98681649424498\n",
            "Training batch[7]: last record -> y: 65.86109832235752 | y_pred: 64.59904932510699\n",
            "Training batch[8]: last record -> y: 65.06269795251433 | y_pred: 65.09926649761815\n",
            "Training batch[9]: last record -> y: 48.99319917400612 | y_pred: 56.583816024831776\n",
            "Training batch[10]: last record -> y: 55.39879897338437 | y_pred: 53.34987336153313\n",
            "Training batch[11]: last record -> y: 65.49870307550941 | y_pred: 65.97665749787029\n",
            "Training batch[12]: last record -> y: 54.78369803384521 | y_pred: 56.00580465979897\n",
            "Training batch[13]: last record -> y: 60.85200205216165 | y_pred: 60.001524471912035\n",
            "Training batch[14]: last record -> y: 52.05239758114112 | y_pred: 50.820265963534894\n",
            "Training batch[15]: last record -> y: 45.92639920518661 | y_pred: 44.902468685471604\n",
            "Training batch[16]: last record -> y: 25.033300272477504 | y_pred: 23.41322809223115\n",
            "Training batch[17]: last record -> y: 56.15299868224588 | y_pred: 58.11311129458113\n",
            "Training batch[18]: last record -> y: 44.302501720337546 | y_pred: 44.77956846297559\n",
            "Training batch[19]: last record -> y: 35.50039984603882 | y_pred: 40.73323160927555\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.953579418498634\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.9709665164886\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.1105590468311\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.99093823327769\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.2255790173692\n",
            "[Training] Epoch: 820 | L1Loss: 0.05158 | RMSE: 0.08568 | PLCC: 0.94199 | SROCC: 0.94360\n",
            "[Testing]  Epoch: 820 | L1Loss: 0.07647 | RMSE: 0.10413 | PLCC: 0.91609 | SROCC: 0.93077\n",
            "Training batch[0]: last record -> y: 56.49319871979219 | y_pred: 54.639906783669176\n",
            "Training batch[1]: last record -> y: 63.91980066066935 | y_pred: 64.03560748595987\n",
            "Training batch[2]: last record -> y: 38.56610147201286 | y_pred: 37.72446691856737\n",
            "Training batch[3]: last record -> y: 37.76599810791879 | y_pred: 36.63699975782106\n",
            "Training batch[4]: last record -> y: 54.38009965319543 | y_pred: 65.36508476360632\n",
            "Training batch[5]: last record -> y: 68.87090185563989 | y_pred: 66.67764466682479\n",
            "Training batch[6]: last record -> y: 54.441201607450694 | y_pred: 61.16159965288625\n",
            "Training batch[7]: last record -> y: 62.23139844929415 | y_pred: 62.445174883415575\n",
            "Training batch[8]: last record -> y: 57.048500278582424 | y_pred: 57.41443016316748\n",
            "Training batch[9]: last record -> y: 58.70890198391771 | y_pred: 56.773737674517406\n",
            "Training batch[10]: last record -> y: 52.35879824837116 | y_pred: 50.702815644264774\n",
            "Training batch[11]: last record -> y: 47.25970218946463 | y_pred: 47.28500749402724\n",
            "Training batch[12]: last record -> y: 26.82169912219564 | y_pred: 31.814473020676303\n",
            "Training batch[17]: last record -> y: 60.175200939423576 | y_pred: 60.39947515113681\n",
            "Training batch[18]: last record -> y: 54.18170323514141 | y_pred: 52.5291073221515\n",
            "Training batch[19]: last record -> y: 42.544398961449815 | y_pred: 53.96102476955184\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.38359591959545\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.925347497688904\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.235686509211405\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.39840252344504\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.56041759803881\n",
            "[Training] Epoch: 821 | L1Loss: 0.05332 | RMSE: 0.08531 | PLCC: 0.94182 | SROCC: 0.94155\n",
            "[Testing]  Epoch: 821 | L1Loss: 0.07604 | RMSE: 0.10382 | PLCC: 0.91533 | SROCC: 0.92928\n",
            "Training batch[0]: last record -> y: 53.07020278914547 | y_pred: 57.561151399675055\n",
            "Training batch[1]: last record -> y: 29.39489931353927 | y_pred: 30.647163732395597\n",
            "Training batch[2]: last record -> y: 65.45760286109589 | y_pred: 67.7335268322156\n",
            "Training batch[3]: last record -> y: 31.0084992311688 | y_pred: 31.49853703294525\n",
            "Training batch[4]: last record -> y: 61.25039978747873 | y_pred: 52.264604494593186\n",
            "Training batch[5]: last record -> y: 28.55800066006435 | y_pred: 27.151291226242506\n",
            "Training batch[6]: last record -> y: 63.91980066066935 | y_pred: 61.39918584421116\n",
            "Training batch[7]: last record -> y: 72.626600789652 | y_pred: 67.42254625221267\n",
            "Training batch[8]: last record -> y: 26.646600678606347 | y_pred: 47.87777492633677\n",
            "Training batch[9]: last record -> y: 50.21400001022266 | y_pred: 54.13201246804783\n",
            "Training batch[10]: last record -> y: 32.322799919350985 | y_pred: 28.76809692200152\n",
            "Training batch[11]: last record -> y: 54.441799826394345 | y_pred: 57.149284089433195\n",
            "Training batch[12]: last record -> y: 24.490699609431772 | y_pred: 24.10229349737179\n",
            "Training batch[13]: last record -> y: 34.048697754381124 | y_pred: 33.60920945334317\n",
            "Training batch[14]: last record -> y: 66.50039818303662 | y_pred: 55.918815263191846\n",
            "Training batch[15]: last record -> y: 47.6147001052891 | y_pred: 48.25388418165653\n",
            "Training batch[16]: last record -> y: 62.10859953807085 | y_pred: 59.16308524384567\n",
            "Training batch[17]: last record -> y: 35.50089996994063 | y_pred: 34.745899419590046\n",
            "Training batch[18]: last record -> y: 53.191200611076056 | y_pred: 60.860557226302035\n",
            "Training batch[19]: last record -> y: 29.276899018788697 | y_pred: 36.02453773571881\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.42728826224459\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.058402661480955\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 34.44865696979036\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.04698105635896\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.090425759278702\n",
            "[Training] Epoch: 822 | L1Loss: 0.05642 | RMSE: 0.08878 | PLCC: 0.93904 | SROCC: 0.93905\n",
            "[Testing]  Epoch: 822 | L1Loss: 0.08064 | RMSE: 0.10515 | PLCC: 0.91350 | SROCC: 0.92639\n",
            "Training batch[0]: last record -> y: 34.274600987035 | y_pred: 34.4310191596453\n",
            "Training batch[1]: last record -> y: 48.248902240023654 | y_pred: 46.53619979623579\n",
            "Training batch[2]: last record -> y: 28.917199777475616 | y_pred: 32.988394879645966\n",
            "Training batch[3]: last record -> y: 58.14030131043933 | y_pred: 62.58009898505588\n",
            "Training batch[4]: last record -> y: 48.407398097782334 | y_pred: 57.47211326399747\n",
            "Training batch[5]: last record -> y: 65.26629501590105 | y_pred: 66.27855544186855\n",
            "Training batch[6]: last record -> y: 33.93240045388143 | y_pred: 35.89435918274171\n",
            "Training batch[7]: last record -> y: 33.262500568553776 | y_pred: 47.600265661100366\n",
            "Training batch[8]: last record -> y: 49.39419884010499 | y_pred: 56.38825632241378\n",
            "Training batch[9]: last record -> y: 38.67589876990439 | y_pred: 53.16540000695795\n",
            "Training batch[10]: last record -> y: 49.544400038424556 | y_pred: 51.084479330314025\n",
            "Training batch[11]: last record -> y: 41.601001254850644 | y_pred: 42.57043435042215\n",
            "Training batch[12]: last record -> y: 65.81910077952853 | y_pred: 66.05521072087959\n",
            "Training batch[13]: last record -> y: 70.23500185870785 | y_pred: 69.74894575076064\n",
            "Training batch[14]: last record -> y: 48.22529832159648 | y_pred: 49.14924748006524\n",
            "Training batch[15]: last record -> y: 62.41920060282996 | y_pred: 60.03056060429526\n",
            "Training batch[16]: last record -> y: 62.10859953807085 | y_pred: 57.16564183968808\n",
            "Training batch[17]: last record -> y: 65.45760286109589 | y_pred: 66.1841815791613\n",
            "Training batch[18]: last record -> y: 30.471100017513493 | y_pred: 52.404680998103004\n",
            "Training batch[19]: last record -> y: 26.044099725567833 | y_pred: 27.87884607037995\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.13681934853139\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.290674185130456\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.8537335162365\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.67474253490309\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.926313156317235\n",
            "[Training] Epoch: 823 | L1Loss: 0.05711 | RMSE: 0.08849 | PLCC: 0.93717 | SROCC: 0.93593\n",
            "[Testing]  Epoch: 823 | L1Loss: 0.07511 | RMSE: 0.10258 | PLCC: 0.91682 | SROCC: 0.93017\n",
            "Training batch[0]: last record -> y: 44.8014983332298 | y_pred: 44.92590375177758\n",
            "Training batch[1]: last record -> y: 65.1140965382051 | y_pred: 65.5182674079515\n",
            "Training batch[2]: last record -> y: 61.4574993262936 | y_pred: 62.217678008360735\n",
            "Training batch[3]: last record -> y: 45.04560060831727 | y_pred: 44.43050770935531\n",
            "Training batch[4]: last record -> y: 22.98349944379808 | y_pred: 23.40563919445573\n",
            "Training batch[5]: last record -> y: 47.959997868779965 | y_pred: 47.51479754169941\n",
            "Training batch[6]: last record -> y: 43.31629919695854 | y_pred: 43.341866624337854\n",
            "Training batch[7]: last record -> y: 61.02579752021575 | y_pred: 58.235000012465434\n",
            "Training batch[8]: last record -> y: 61.25039978747873 | y_pred: 51.20778640601634\n",
            "Training batch[9]: last record -> y: 73.66169645486639 | y_pred: 73.3826208851915\n",
            "Training batch[10]: last record -> y: 63.439498389766186 | y_pred: 67.3097787651036\n",
            "Training batch[11]: last record -> y: 55.37589940951989 | y_pred: 59.86195934911211\n",
            "Training batch[12]: last record -> y: 26.15419975119093 | y_pred: 46.23992864816182\n",
            "Training batch[13]: last record -> y: 60.042196927618534 | y_pred: 60.916809104390495\n",
            "Training batch[14]: last record -> y: 61.21029982086884 | y_pred: 58.070689209276\n",
            "Training batch[15]: last record -> y: 57.104697480745926 | y_pred: 53.017089952395736\n",
            "Training batch[16]: last record -> y: 63.66260189343916 | y_pred: 66.37647359100538\n",
            "Training batch[17]: last record -> y: 28.55800066006435 | y_pred: 28.895083365830374\n",
            "Training batch[18]: last record -> y: 54.78369803384521 | y_pred: 55.91024722412794\n",
            "Training batch[19]: last record -> y: 69.84349972239715 | y_pred: 67.94985052119387\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.37557287781215\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.127485846518084\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 33.93156905819899\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.39891535841684\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.907580621586447\n",
            "[Training] Epoch: 824 | L1Loss: 0.05395 | RMSE: 0.08756 | PLCC: 0.93963 | SROCC: 0.93957\n",
            "[Testing]  Epoch: 824 | L1Loss: 0.08015 | RMSE: 0.10564 | PLCC: 0.91203 | SROCC: 0.92411\n",
            "Training batch[0]: last record -> y: 37.80239940901686 | y_pred: 37.23275024427164\n",
            "Training batch[1]: last record -> y: 53.64059811945481 | y_pred: 53.66084429284342\n",
            "Training batch[2]: last record -> y: 26.044099725567833 | y_pred: 27.702782355497845\n",
            "Training batch[3]: last record -> y: 39.64730019877436 | y_pred: 40.843799194462804\n",
            "Training batch[4]: last record -> y: 64.74600213023791 | y_pred: 62.7292227460307\n",
            "Training batch[5]: last record -> y: 36.49660163122326 | y_pred: 36.7551962426553\n",
            "Training batch[6]: last record -> y: 56.801496963241334 | y_pred: 53.586883847530544\n",
            "Training batch[7]: last record -> y: 26.387500716897307 | y_pred: 28.05439036397496\n",
            "Training batch[8]: last record -> y: 43.67010067489571 | y_pred: 43.56177962697802\n",
            "Training batch[9]: last record -> y: 26.000999417575912 | y_pred: 29.01650894646798\n",
            "Training batch[10]: last record -> y: 35.50039984603882 | y_pred: 40.31071214235965\n",
            "Training batch[11]: last record -> y: 64.4866034610859 | y_pred: 60.91859411252881\n",
            "Training batch[12]: last record -> y: 39.47439884290486 | y_pred: 39.12411592155024\n",
            "Training batch[13]: last record -> y: 57.949199304020794 | y_pred: 69.54201023972064\n",
            "Training batch[14]: last record -> y: 48.61320149555263 | y_pred: 48.1834422929262\n",
            "Training batch[15]: last record -> y: 64.4571009752251 | y_pred: 64.9478656451804\n",
            "Training batch[16]: last record -> y: 40.42119932177491 | y_pred: 38.76551904337123\n",
            "Training batch[17]: last record -> y: 31.31949991261473 | y_pred: 31.01237559343673\n",
            "Training batch[18]: last record -> y: 58.27209923566443 | y_pred: 60.1525554577986\n",
            "Training batch[19]: last record -> y: 61.434799168743666 | y_pred: 62.44694381039949\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.650406392810055\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.06981254823427\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.67964054682602\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.82298665673227\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.04425877514285\n",
            "[Training] Epoch: 825 | L1Loss: 0.05367 | RMSE: 0.08577 | PLCC: 0.94124 | SROCC: 0.94242\n",
            "[Testing]  Epoch: 825 | L1Loss: 0.07705 | RMSE: 0.10481 | PLCC: 0.91611 | SROCC: 0.92938\n",
            "Training batch[0]: last record -> y: 30.038900499706983 | y_pred: 30.669848612848853\n",
            "Training batch[1]: last record -> y: 40.39899768001135 | y_pred: 58.85947948125022\n",
            "Training batch[2]: last record -> y: 56.6087997063064 | y_pred: 56.051680977069054\n",
            "Training batch[3]: last record -> y: 49.09339763083676 | y_pred: 45.63492828170081\n",
            "Training batch[4]: last record -> y: 33.93240045388143 | y_pred: 34.12241698237801\n",
            "Training batch[5]: last record -> y: 52.83100204991888 | y_pred: 52.73976722855036\n",
            "Training batch[6]: last record -> y: 43.907299310399594 | y_pred: 52.22874352028293\n",
            "Training batch[7]: last record -> y: 59.38040274816581 | y_pred: 60.79909183795735\n",
            "Training batch[8]: last record -> y: 60.984999631504934 | y_pred: 60.6821721967824\n",
            "Training batch[9]: last record -> y: 34.47520052324171 | y_pred: 35.42033338640056\n",
            "Training batch[10]: last record -> y: 56.92869889453914 | y_pred: 59.7797235417454\n",
            "Training batch[11]: last record -> y: 65.81910077952853 | y_pred: 64.5075539890372\n",
            "Training batch[12]: last record -> y: 33.94779976733412 | y_pred: 33.82454575944132\n",
            "Training batch[13]: last record -> y: 53.07020278914547 | y_pred: 59.18936828259575\n",
            "Training batch[14]: last record -> y: 43.501399716555284 | y_pred: 44.99778490382664\n",
            "Training batch[15]: last record -> y: 40.68579863625962 | y_pred: 47.15259205247321\n",
            "Training batch[16]: last record -> y: 44.8014983332298 | y_pred: 45.02961272461357\n",
            "Training batch[17]: last record -> y: 58.416900382336735 | y_pred: 58.49483287278622\n",
            "Training batch[18]: last record -> y: 60.984999631504934 | y_pred: 60.79920762226902\n",
            "Training batch[19]: last record -> y: 44.271098442026755 | y_pred: 41.07439812420125\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.69159022922645\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.33629979024545\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.66098479960749\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.82277599360964\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.37403340029215\n",
            "[Training] Epoch: 826 | L1Loss: 0.05221 | RMSE: 0.08586 | PLCC: 0.94145 | SROCC: 0.94171\n",
            "[Testing]  Epoch: 826 | L1Loss: 0.07785 | RMSE: 0.10626 | PLCC: 0.91576 | SROCC: 0.93048\n",
            "Training batch[0]: last record -> y: 36.81190160929782 | y_pred: 34.71543688881161\n",
            "Training batch[1]: last record -> y: 47.44130023363323 | y_pred: 48.427091079459615\n",
            "Training batch[2]: last record -> y: 62.891102078674976 | y_pred: 64.56219453545486\n",
            "Training batch[3]: last record -> y: 53.07020278914547 | y_pred: 58.35414850164011\n",
            "Training batch[4]: last record -> y: 55.44500012997332 | y_pred: 54.64488550907117\n",
            "Training batch[5]: last record -> y: 47.66609869097988 | y_pred: 47.70611181935601\n",
            "Training batch[6]: last record -> y: 25.810800367976896 | y_pred: 19.516413008077308\n",
            "Training batch[7]: last record -> y: 64.73490291747157 | y_pred: 61.570102785627796\n",
            "Training batch[8]: last record -> y: 59.352698135366836 | y_pred: 59.220919507527015\n",
            "Training batch[9]: last record -> y: 35.17409875023975 | y_pred: 52.08758314696661\n",
            "Training batch[10]: last record -> y: 63.25490281841758 | y_pred: 64.19608454194054\n",
            "Training batch[11]: last record -> y: 40.39899768001135 | y_pred: 59.58291594174602\n",
            "Training batch[12]: last record -> y: 61.02579752021575 | y_pred: 59.92073275221037\n",
            "Training batch[13]: last record -> y: 63.38619901162542 | y_pred: 62.804263844919205\n",
            "Training batch[14]: last record -> y: 49.328201782450606 | y_pred: 50.86447305697834\n",
            "Training batch[15]: last record -> y: 50.848597741355434 | y_pred: 51.75498306298937\n",
            "Training batch[16]: last record -> y: 30.471100017513493 | y_pred: 52.88565867749094\n",
            "Training batch[17]: last record -> y: 58.812097967928366 | y_pred: 57.687436705172104\n",
            "Training batch[18]: last record -> y: 59.43330009744659 | y_pred: 62.24966342446078\n",
            "Training batch[19]: last record -> y: 28.118200384631052 | y_pred: 26.785708694592472\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 43.905406558526806\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.19215927131188\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 34.951008111490296\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 37.82084288499732\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.980944456071114\n",
            "[Training] Epoch: 827 | L1Loss: 0.05320 | RMSE: 0.08628 | PLCC: 0.94115 | SROCC: 0.94192\n",
            "[Testing]  Epoch: 827 | L1Loss: 0.07912 | RMSE: 0.10518 | PLCC: 0.91428 | SROCC: 0.92812\n",
            "Training batch[0]: last record -> y: 26.15419975119093 | y_pred: 42.99543835569273\n",
            "Training batch[1]: last record -> y: 31.80629896989518 | y_pred: 30.52267147561406\n",
            "Training batch[2]: last record -> y: 61.12310136925453 | y_pred: 63.16408610706344\n",
            "Training batch[3]: last record -> y: 53.082598142956385 | y_pred: 54.55069497152408\n",
            "Training batch[4]: last record -> y: 40.77389924063573 | y_pred: 41.49428188991112\n",
            "Training batch[5]: last record -> y: 62.20869829174421 | y_pred: 57.66511284663511\n",
            "Training batch[6]: last record -> y: 47.28070096087913 | y_pred: 46.57520946057741\n",
            "Training batch[7]: last record -> y: 40.115501400992116 | y_pred: 35.32537738590406\n",
            "Training batch[8]: last record -> y: 63.91980066066935 | y_pred: 63.77144236264394\n",
            "Training batch[9]: last record -> y: 53.64059811945481 | y_pred: 55.04566325523933\n",
            "Training batch[10]: last record -> y: 65.86109832235752 | y_pred: 64.40917270265368\n",
            "Training batch[11]: last record -> y: 42.995300057764894 | y_pred: 50.28955570605194\n",
            "Training batch[12]: last record -> y: 25.707799769992192 | y_pred: 23.32412402382053\n",
            "Training batch[13]: last record -> y: 27.960300333642863 | y_pred: 27.473662087906405\n",
            "Training batch[14]: last record -> y: 43.03680069292295 | y_pred: 42.46253141251884\n",
            "Training batch[15]: last record -> y: 39.67210055508883 | y_pred: 41.403063149697005\n",
            "Training batch[16]: last record -> y: 62.93809764429125 | y_pred: 67.45818530659221\n",
            "Training batch[17]: last record -> y: 47.787299135455896 | y_pred: 50.63496282139272\n",
            "Training batch[18]: last record -> y: 49.1077999127167 | y_pred: 48.03008275588263\n",
            "Training batch[19]: last record -> y: 34.8158990765744 | y_pred: 33.05315529652705\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.17418873512429\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.54716377345164\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.57289866827034\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.23127254003589\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.0613140454698\n",
            "[Training] Epoch: 828 | L1Loss: 0.05342 | RMSE: 0.08678 | PLCC: 0.94025 | SROCC: 0.93987\n",
            "[Testing]  Epoch: 828 | L1Loss: 0.07528 | RMSE: 0.10301 | PLCC: 0.91639 | SROCC: 0.92822\n",
            "Training batch[0]: last record -> y: 45.587398821758484 | y_pred: 45.144721627803165\n",
            "Training batch[1]: last record -> y: 59.38040274816581 | y_pred: 60.63297351301344\n",
            "Training batch[2]: last record -> y: 30.080500033964597 | y_pred: 31.23902097136613\n",
            "Training batch[3]: last record -> y: 65.65200150416626 | y_pred: 66.16449824617666\n",
            "Training batch[4]: last record -> y: 48.53829869459196 | y_pred: 48.53369305197202\n",
            "Training batch[5]: last record -> y: 28.126199954886943 | y_pred: 28.29809623860649\n",
            "Training batch[6]: last record -> y: 29.981600130351694 | y_pred: 29.144757760917173\n",
            "Training batch[7]: last record -> y: 63.459201020136106 | y_pred: 61.18483692099312\n",
            "Training batch[8]: last record -> y: 58.50000135581013 | y_pred: 64.7917658794272\n",
            "Training batch[9]: last record -> y: 64.35050221894357 | y_pred: 65.59052003466718\n",
            "Training batch[10]: last record -> y: 47.782002003196794 | y_pred: 47.58472161725808\n",
            "Training batch[11]: last record -> y: 32.25839972032844 | y_pred: 31.942122812124126\n",
            "Training batch[12]: last record -> y: 49.328201782450606 | y_pred: 49.05120068169322\n",
            "Training batch[13]: last record -> y: 62.27109960327493 | y_pred: 54.81080442770053\n",
            "Training batch[14]: last record -> y: 40.48429855540655 | y_pred: 39.48989142705307\n",
            "Training batch[15]: last record -> y: 45.92639920518661 | y_pred: 46.33136609207577\n",
            "Training batch[16]: last record -> y: 43.48930025598531 | y_pred: 44.07457869469113\n",
            "Training batch[17]: last record -> y: 49.19730118564098 | y_pred: 62.144778919238206\n",
            "Training batch[18]: last record -> y: 69.84349972239715 | y_pred: 70.92102068914755\n",
            "Training batch[19]: last record -> y: 68.3981030513794 | y_pred: 63.91540407305661\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.31447735601864\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.91464211935181\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.31871335552103\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 37.817247138873654\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.195907277358344\n",
            "[Training] Epoch: 829 | L1Loss: 0.05390 | RMSE: 0.08629 | PLCC: 0.94118 | SROCC: 0.94136\n",
            "[Testing]  Epoch: 829 | L1Loss: 0.07721 | RMSE: 0.10354 | PLCC: 0.91550 | SROCC: 0.92846\n",
            "Training batch[0]: last record -> y: 38.542598864858405 | y_pred: 34.58439960218959\n",
            "Training batch[1]: last record -> y: 45.04560060831727 | y_pred: 43.44499989184635\n",
            "Training batch[2]: last record -> y: 67.8711011081009 | y_pred: 67.0949763533315\n",
            "Training batch[3]: last record -> y: 43.03680069292295 | y_pred: 42.925861633069076\n",
            "Training batch[4]: last record -> y: 67.31629806509704 | y_pred: 69.70724088494171\n",
            "Training batch[5]: last record -> y: 50.21070015733994 | y_pred: 48.65219186250897\n",
            "Training batch[6]: last record -> y: 62.27109960327493 | y_pred: 55.117304798087844\n",
            "Training batch[7]: last record -> y: 25.112100743235686 | y_pred: 25.50153202888862\n",
            "Training batch[8]: last record -> y: 48.6593994359107 | y_pred: 46.94687227670579\n",
            "Training batch[9]: last record -> y: 64.75720104616153 | y_pred: 65.23261786235821\n",
            "Training batch[10]: last record -> y: 35.50039984603882 | y_pred: 42.09978559650483\n",
            "Training batch[11]: last record -> y: 56.32199875005813 | y_pred: 55.15375755887999\n",
            "Training batch[12]: last record -> y: 53.079102099989996 | y_pred: 55.35088678196735\n",
            "Training batch[13]: last record -> y: 58.50000135581013 | y_pred: 62.506167485820924\n",
            "Training batch[14]: last record -> y: 23.335699037742657 | y_pred: 22.607087661761184\n",
            "Training batch[15]: last record -> y: 72.168403673586 | y_pred: 66.8088668867224\n",
            "Training batch[16]: last record -> y: 69.64639944538771 | y_pred: 66.06549622723333\n",
            "Training batch[17]: last record -> y: 47.55770206163652 | y_pred: 39.261799549285456\n",
            "Training batch[18]: last record -> y: 28.55800066006435 | y_pred: 29.389641580108446\n",
            "Training batch[19]: last record -> y: 39.02769814411886 | y_pred: 40.03829095437516\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.74206254042383\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.541990466081415\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.0028444122878\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.650812169041615\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.35699702532162\n",
            "[Training] Epoch: 830 | L1Loss: 0.05308 | RMSE: 0.08656 | PLCC: 0.94047 | SROCC: 0.94076\n",
            "[Testing]  Epoch: 830 | L1Loss: 0.07523 | RMSE: 0.10264 | PLCC: 0.91648 | SROCC: 0.92954\n",
            "Training batch[0]: last record -> y: 63.36859979625092 | y_pred: 58.6439148227596\n",
            "Training batch[1]: last record -> y: 45.05469932547635 | y_pred: 43.161559896867516\n",
            "Training batch[2]: last record -> y: 54.46919889725973 | y_pred: 55.4462319464003\n",
            "Training batch[3]: last record -> y: 31.38129978897092 | y_pred: 31.87137779363337\n",
            "Training batch[4]: last record -> y: 65.65200150416626 | y_pred: 64.48260246987138\n",
            "Training batch[5]: last record -> y: 54.38009965319543 | y_pred: 63.38603820008143\n",
            "Training batch[6]: last record -> y: 43.31629919695854 | y_pred: 44.181772453685426\n",
            "Training batch[7]: last record -> y: 62.891102078674976 | y_pred: 63.77370337295247\n",
            "Training batch[8]: last record -> y: 21.597000101274666 | y_pred: 14.416259613269034\n",
            "Training batch[9]: last record -> y: 73.27570372205287 | y_pred: 66.9892041683861\n",
            "Training batch[10]: last record -> y: 54.441799826394345 | y_pred: 56.895327282929884\n",
            "Training batch[11]: last record -> y: 46.40150083075707 | y_pred: 43.24559840164238\n",
            "Training batch[12]: last record -> y: 27.66959969745278 | y_pred: 28.054486850901355\n",
            "Training batch[13]: last record -> y: 36.78459902535883 | y_pred: 36.78274647637204\n",
            "Training batch[14]: last record -> y: 63.66260189343916 | y_pred: 63.255375604356914\n",
            "Training batch[15]: last record -> y: 33.643200189396794 | y_pred: 33.6315188388412\n",
            "Training batch[16]: last record -> y: 31.31949991261473 | y_pred: 31.456965640707836\n",
            "Training batch[17]: last record -> y: 27.51029978197414 | y_pred: 27.890015236167926\n",
            "Training batch[18]: last record -> y: 51.13240117042369 | y_pred: 56.541934266314456\n",
            "Training batch[19]: last record -> y: 38.48249876652221 | y_pred: 40.185528395938945\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 47.65994925753762\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 47.785864696483486\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.645055115766695\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 40.90194704065493\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 27.19474491760235\n",
            "[Training] Epoch: 831 | L1Loss: 0.05267 | RMSE: 0.08494 | PLCC: 0.94267 | SROCC: 0.94396\n",
            "[Testing]  Epoch: 831 | L1Loss: 0.08386 | RMSE: 0.11270 | PLCC: 0.91614 | SROCC: 0.93084\n",
            "Training batch[0]: last record -> y: 67.08080242384403 | y_pred: 68.15934293578334\n",
            "Training batch[1]: last record -> y: 55.44500012997332 | y_pred: 56.362333501522244\n",
            "Training batch[2]: last record -> y: 51.224099129038905 | y_pred: 60.04371498859382\n",
            "Training batch[3]: last record -> y: 31.80629896989518 | y_pred: 31.867055179330862\n",
            "Training batch[4]: last record -> y: 45.587398821758484 | y_pred: 45.36651934173892\n",
            "Training batch[5]: last record -> y: 65.29879824517275 | y_pred: 64.61430712440097\n",
            "Training batch[6]: last record -> y: 39.32759880874073 | y_pred: 56.80365183793083\n",
            "Training batch[7]: last record -> y: 43.297297704920425 | y_pred: 47.41438359739982\n",
            "Training batch[8]: last record -> y: 51.931000946581435 | y_pred: 62.878996185643246\n",
            "Training batch[9]: last record -> y: 60.352402395979425 | y_pred: 54.66246864329128\n",
            "Training batch[10]: last record -> y: 64.07659834852348 | y_pred: 60.063739242051724\n",
            "Training batch[11]: last record -> y: 22.563999213620633 | y_pred: 23.402683277262724\n",
            "Training batch[12]: last record -> y: 67.35739827951056 | y_pred: 64.57416213055876\n",
            "Training batch[13]: last record -> y: 68.96460030986236 | y_pred: 66.58680222562361\n",
            "Training batch[14]: last record -> y: 24.91699975574693 | y_pred: 25.5166832905347\n",
            "Training batch[15]: last record -> y: 28.547899282928483 | y_pred: 29.58498177476889\n",
            "Training batch[16]: last record -> y: 28.55800066006435 | y_pred: 28.602850999683767\n",
            "Training batch[17]: last record -> y: 42.544398961449815 | y_pred: 55.8376375957846\n",
            "Training batch[18]: last record -> y: 66.92800251097356 | y_pred: 66.62271144339707\n",
            "Training batch[19]: last record -> y: 54.78369803384521 | y_pred: 54.61207352163501\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.25337716373235\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.65506349512407\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.75951081638061\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.2610532298678\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.134289922104585\n",
            "[Training] Epoch: 832 | L1Loss: 0.05811 | RMSE: 0.08922 | PLCC: 0.93678 | SROCC: 0.93587\n",
            "[Testing]  Epoch: 832 | L1Loss: 0.07546 | RMSE: 0.10333 | PLCC: 0.91655 | SROCC: 0.92944\n",
            "Training batch[0]: last record -> y: 52.886501329981456 | y_pred: 52.903161405939045\n",
            "Training batch[1]: last record -> y: 27.960300333642863 | y_pred: 28.12811441631817\n",
            "Training batch[2]: last record -> y: 62.10859953807085 | y_pred: 59.09861267962833\n",
            "Training batch[3]: last record -> y: 58.416900382336735 | y_pred: 59.387616754029295\n",
            "Training batch[4]: last record -> y: 70.7566005341082 | y_pred: 64.7893408413438\n",
            "Training batch[5]: last record -> y: 68.26640161308069 | y_pred: 66.68266198699735\n",
            "Training batch[6]: last record -> y: 64.93820087138647 | y_pred: 62.40014443486689\n",
            "Training batch[7]: last record -> y: 37.43249908741063 | y_pred: 49.470899514127495\n",
            "Training batch[8]: last record -> y: 28.923200460239684 | y_pred: 28.75597575187311\n",
            "Training batch[9]: last record -> y: 57.412301018325024 | y_pred: 57.381878690432586\n",
            "Training batch[10]: last record -> y: 51.99429958652763 | y_pred: 52.5919589060054\n",
            "Training batch[11]: last record -> y: 53.70280002467098 | y_pred: 52.899256901650915\n",
            "Training batch[12]: last record -> y: 40.334599089104245 | y_pred: 40.54742030265436\n",
            "Training batch[13]: last record -> y: 61.02579752021575 | y_pred: 59.564991887052656\n",
            "Training batch[14]: last record -> y: 48.6593994359107 | y_pred: 48.69712260790038\n",
            "Training batch[15]: last record -> y: 63.56399868712492 | y_pred: 67.91534036385315\n",
            "Training batch[16]: last record -> y: 43.4957005554362 | y_pred: 45.84022833933841\n",
            "Training batch[17]: last record -> y: 67.85620031043459 | y_pred: 68.79547802527668\n",
            "Training batch[18]: last record -> y: 48.261201106908175 | y_pred: 45.804159918136406\n",
            "Training batch[19]: last record -> y: 55.43380121404971 | y_pred: 54.60210963836926\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.260745548678074\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.09829066455984\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.835904494207625\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.64675328567125\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.691684041111017\n",
            "[Training] Epoch: 833 | L1Loss: 0.05248 | RMSE: 0.08567 | PLCC: 0.94183 | SROCC: 0.94329\n",
            "[Testing]  Epoch: 833 | L1Loss: 0.07788 | RMSE: 0.10597 | PLCC: 0.91611 | SROCC: 0.93046\n",
            "Training batch[0]: last record -> y: 26.202400197300562 | y_pred: 27.358215480474428\n",
            "Training batch[1]: last record -> y: 41.19950146484996 | y_pred: 43.613052779664486\n",
            "Training batch[2]: last record -> y: 61.434799168743666 | y_pred: 62.153639635312175\n",
            "Training batch[3]: last record -> y: 48.289101909790816 | y_pred: 60.10781125379822\n",
            "Training batch[4]: last record -> y: 50.21070015733994 | y_pred: 47.30047113209753\n",
            "Training batch[5]: last record -> y: 53.02539747675837 | y_pred: 53.04596205700409\n",
            "Training batch[6]: last record -> y: 53.863000484795975 | y_pred: 55.16312322320209\n",
            "Training batch[7]: last record -> y: 70.11219651502279 | y_pred: 66.84019619172295\n",
            "Training batch[8]: last record -> y: 45.65990070636735 | y_pred: 43.506312509224244\n",
            "Training batch[9]: last record -> y: 55.70909771244078 | y_pred: 55.61591707138996\n",
            "Training batch[10]: last record -> y: 32.369998911570406 | y_pred: 32.681293074084124\n",
            "Training batch[11]: last record -> y: 62.27109960327493 | y_pred: 57.08060147899414\n",
            "Training batch[12]: last record -> y: 33.643200189396794 | y_pred: 34.96096073794797\n",
            "Training batch[13]: last record -> y: 68.87090185563989 | y_pred: 67.52814797692145\n",
            "Training batch[14]: last record -> y: 38.522899450719365 | y_pred: 42.47722637140885\n",
            "Training batch[15]: last record -> y: 39.941398782888996 | y_pred: 56.37003637447947\n",
            "Training batch[16]: last record -> y: 60.20750154614984 | y_pred: 56.64660971652984\n",
            "Training batch[17]: last record -> y: 46.75879995977607 | y_pred: 47.133574479280696\n",
            "Training batch[18]: last record -> y: 47.6147001052891 | y_pred: 49.30292542396592\n",
            "Training batch[19]: last record -> y: 56.76089848084507 | y_pred: 59.25249324607444\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.83640600279125\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 47.00190037894447\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.69091519802885\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.90307739195566\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.404354570765406\n",
            "[Training] Epoch: 834 | L1Loss: 0.05180 | RMSE: 0.08464 | PLCC: 0.94347 | SROCC: 0.94448\n",
            "[Testing]  Epoch: 834 | L1Loss: 0.07960 | RMSE: 0.10768 | PLCC: 0.91608 | SROCC: 0.93026\n",
            "Training batch[0]: last record -> y: 29.17069987919399 | y_pred: 44.69575187813143\n",
            "Training batch[1]: last record -> y: 52.85669973464883 | y_pred: 53.369633884058885\n",
            "Training batch[2]: last record -> y: 46.59859789153563 | y_pred: 45.05226624681575\n",
            "Training batch[3]: last record -> y: 47.66609869097988 | y_pred: 48.36986146718368\n",
            "Training batch[4]: last record -> y: 56.92869889453914 | y_pred: 60.66420954731848\n",
            "Training batch[5]: last record -> y: 48.6593994359107 | y_pred: 47.458002120592255\n",
            "Training batch[6]: last record -> y: 52.59410091577138 | y_pred: 54.148585705771666\n",
            "Training batch[7]: last record -> y: 41.8042976006501 | y_pred: 37.6878356569614\n",
            "Training batch[8]: last record -> y: 40.04259909563871 | y_pred: 49.57413409290871\n",
            "Training batch[9]: last record -> y: 25.808300150496677 | y_pred: 40.45443263545633\n",
            "Training batch[10]: last record -> y: 66.92800251097356 | y_pred: 69.51239518577904\n",
            "Training batch[11]: last record -> y: 63.459201020136106 | y_pred: 62.38999079397922\n",
            "Training batch[12]: last record -> y: 50.734000218875735 | y_pred: 59.92325749345105\n",
            "Training batch[13]: last record -> y: 37.80239940901686 | y_pred: 37.678632412298725\n",
            "Training batch[14]: last record -> y: 21.597000101274666 | y_pred: 15.861289231937633\n",
            "Training batch[15]: last record -> y: 44.8014983332298 | y_pred: 44.81863119512673\n",
            "Training batch[16]: last record -> y: 55.98460004960816 | y_pred: 56.859556363084266\n",
            "Training batch[17]: last record -> y: 56.6087997063064 | y_pred: 56.45192482891116\n",
            "Training batch[18]: last record -> y: 62.23139844929415 | y_pred: 59.18615526794679\n",
            "Training batch[19]: last record -> y: 31.165001025781976 | y_pred: 32.5963942275489\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 47.0661108203451\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 47.409768306087244\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.40088836220809\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 40.24688282431828\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.8931050761322\n",
            "[Training] Epoch: 835 | L1Loss: 0.05111 | RMSE: 0.08544 | PLCC: 0.94236 | SROCC: 0.94359\n",
            "[Testing]  Epoch: 835 | L1Loss: 0.08201 | RMSE: 0.11076 | PLCC: 0.91729 | SROCC: 0.93277\n",
            "Training batch[0]: last record -> y: 34.048697754381124 | y_pred: 35.883510835984\n",
            "Training batch[1]: last record -> y: 45.44269898635889 | y_pred: 46.509421457930216\n",
            "Training batch[2]: last record -> y: 73.29170125444921 | y_pred: 70.98406846308512\n",
            "Training batch[3]: last record -> y: 56.90710190418099 | y_pred: 57.64610492213524\n",
            "Training batch[4]: last record -> y: 54.18170323514141 | y_pred: 52.96992392654283\n",
            "Training batch[5]: last record -> y: 65.59839658409192 | y_pred: 61.532144828783885\n",
            "Training batch[6]: last record -> y: 53.079102099989996 | y_pred: 55.167368647963485\n",
            "Training batch[7]: last record -> y: 28.995599425460398 | y_pred: 28.249897802641158\n",
            "Training batch[8]: last record -> y: 41.8042976006501 | y_pred: 36.777290140684386\n",
            "Training batch[9]: last record -> y: 49.50770284408554 | y_pred: 47.59280400545913\n",
            "Training batch[10]: last record -> y: 40.115501400992116 | y_pred: 36.07591863213975\n",
            "Training batch[11]: last record -> y: 38.89670106038284 | y_pred: 39.06054872632558\n",
            "Training batch[12]: last record -> y: 50.21070015733994 | y_pred: 46.84807288030788\n",
            "Training batch[13]: last record -> y: 52.99509736563937 | y_pred: 51.57369377698524\n",
            "Training batch[14]: last record -> y: 59.78609811134925 | y_pred: 60.59696459208271\n",
            "Training batch[15]: last record -> y: 61.02579752021575 | y_pred: 60.254243029526606\n",
            "Training batch[16]: last record -> y: 63.56399868712492 | y_pred: 67.94371395267513\n",
            "Training batch[17]: last record -> y: 40.97670028687969 | y_pred: 42.13947710179298\n",
            "Training batch[18]: last record -> y: 55.782900562440545 | y_pred: 53.33634267822163\n",
            "Training batch[19]: last record -> y: 64.41010219337795 | y_pred: 67.56011087940533\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.56464222631462\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.154051759732056\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.381558506913166\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.1365786623561\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.814621900466335\n",
            "[Training] Epoch: 836 | L1Loss: 0.05379 | RMSE: 0.08755 | PLCC: 0.94011 | SROCC: 0.94060\n",
            "[Testing]  Epoch: 836 | L1Loss: 0.07883 | RMSE: 0.10390 | PLCC: 0.91503 | SROCC: 0.92637\n",
            "Training batch[0]: last record -> y: 29.695998828221605 | y_pred: 47.76916602575534\n",
            "Training batch[1]: last record -> y: 58.416900382336735 | y_pred: 57.16298201675045\n",
            "Training batch[2]: last record -> y: 68.06819816887946 | y_pred: 66.1703807324559\n",
            "Training batch[3]: last record -> y: 59.229098382654456 | y_pred: 60.91121929512133\n",
            "Training batch[4]: last record -> y: 47.28070096087913 | y_pred: 50.052747842599956\n",
            "Training batch[5]: last record -> y: 63.33969874556465 | y_pred: 62.69744960116873\n",
            "Training batch[6]: last record -> y: 48.61320149555263 | y_pred: 48.31835674587387\n",
            "Training batch[7]: last record -> y: 61.18830080165071 | y_pred: 62.02033651403531\n",
            "Training batch[8]: last record -> y: 41.601200661165194 | y_pred: 42.89068571593623\n",
            "Training batch[9]: last record -> y: 54.23470028757947 | y_pred: 53.35088969049116\n",
            "Training batch[10]: last record -> y: 46.40150083075707 | y_pred: 42.62207897777523\n",
            "Training batch[11]: last record -> y: 61.352299630444804 | y_pred: 62.359783953555734\n",
            "Training batch[12]: last record -> y: 39.04919864755061 | y_pred: 38.76445447095\n",
            "Training batch[13]: last record -> y: 48.53810250450829 | y_pred: 42.15065189598499\n",
            "Training batch[14]: last record -> y: 57.606300848766296 | y_pred: 64.7139009298262\n",
            "Training batch[15]: last record -> y: 64.29779784351558 | y_pred: 61.75710731390541\n",
            "Training batch[16]: last record -> y: 43.48559998235805 | y_pred: 38.7932638590562\n",
            "Training batch[17]: last record -> y: 43.4957005554362 | y_pred: 43.59634124401282\n",
            "Training batch[18]: last record -> y: 47.35749812182803 | y_pred: 46.994221627718844\n",
            "Training batch[19]: last record -> y: 65.36119955670347 | y_pred: 65.27873861317516\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.042746357264946\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 48.22947942174028\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.465513697392225\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.48141022622292\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.4040361639083\n",
            "[Training] Epoch: 837 | L1Loss: 0.05387 | RMSE: 0.08458 | PLCC: 0.94268 | SROCC: 0.94361\n",
            "[Testing]  Epoch: 837 | L1Loss: 0.08588 | RMSE: 0.11629 | PLCC: 0.91497 | SROCC: 0.93063\n",
            "Training batch[0]: last record -> y: 26.82169912219564 | y_pred: 33.66427132600609\n",
            "Training batch[1]: last record -> y: 54.97959865673647 | y_pred: 64.49767372777433\n",
            "Training batch[2]: last record -> y: 64.8636003961285 | y_pred: 62.330841091868024\n",
            "Training batch[3]: last record -> y: 46.59859789153563 | y_pred: 44.243789425626005\n",
            "Training batch[4]: last record -> y: 52.87200256117512 | y_pred: 51.76312012711537\n",
            "Training batch[5]: last record -> y: 66.61560035692173 | y_pred: 65.70814403040481\n",
            "Training batch[6]: last record -> y: 52.05239758114112 | y_pred: 51.20739724207988\n",
            "Training batch[7]: last record -> y: 57.655602451923414 | y_pred: 58.25643619127959\n",
            "Training batch[8]: last record -> y: 56.977000249892626 | y_pred: 56.90201382692908\n",
            "Training batch[9]: last record -> y: 50.21070015733994 | y_pred: 50.80367986088754\n",
            "Training batch[10]: last record -> y: 39.217899605891034 | y_pred: 45.86422624604836\n",
            "Training batch[11]: last record -> y: 53.07020278914547 | y_pred: 57.638154399400264\n",
            "Training batch[12]: last record -> y: 39.685200263462434 | y_pred: 38.986681551708216\n",
            "Training batch[13]: last record -> y: 56.89680031667285 | y_pred: 57.10996245069623\n",
            "Training batch[14]: last record -> y: 51.062699014795726 | y_pred: 49.923014737599715\n",
            "Training batch[15]: last record -> y: 58.07179880892954 | y_pred: 58.944986195421734\n",
            "Training batch[16]: last record -> y: 49.19860054291644 | y_pred: 56.90420086392737\n",
            "Training batch[17]: last record -> y: 68.26640161308069 | y_pred: 67.2564375759614\n",
            "Training batch[18]: last record -> y: 30.96270010343983 | y_pred: 27.991932768403842\n",
            "Training batch[19]: last record -> y: 48.559300682237335 | y_pred: 46.5064882553678\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.01827551076212\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.098793850839\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.71659343151998\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.65361993859972\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.501459911869148\n",
            "[Training] Epoch: 838 | L1Loss: 0.05434 | RMSE: 0.08615 | PLCC: 0.94119 | SROCC: 0.94195\n",
            "[Testing]  Epoch: 838 | L1Loss: 0.07736 | RMSE: 0.10368 | PLCC: 0.91405 | SROCC: 0.92775\n",
            "Training batch[0]: last record -> y: 57.949199304020794 | y_pred: 66.56845684468499\n",
            "Training batch[1]: last record -> y: 58.68100118103507 | y_pred: 57.433032842576495\n",
            "Training batch[2]: last record -> y: 67.7326970446486 | y_pred: 67.47509303232755\n",
            "Training batch[3]: last record -> y: 66.65419834371073 | y_pred: 65.88107111612135\n",
            "Training batch[4]: last record -> y: 52.35879824837116 | y_pred: 52.58037404237621\n",
            "Training batch[5]: last record -> y: 58.383400121492286 | y_pred: 56.69453798910126\n",
            "Training batch[6]: last record -> y: 49.09339763083676 | y_pred: 45.73925155463485\n",
            "Training batch[7]: last record -> y: 40.97670028687969 | y_pred: 40.74897023508606\n",
            "Training batch[8]: last record -> y: 46.59859789153563 | y_pred: 44.61284227039539\n",
            "Training batch[9]: last record -> y: 41.72359915164395 | y_pred: 43.92069652012958\n",
            "Training batch[10]: last record -> y: 68.26640161308069 | y_pred: 67.70171830881395\n",
            "Training batch[11]: last record -> y: 59.352698135366836 | y_pred: 60.37490957967657\n",
            "Training batch[12]: last record -> y: 48.33070063999071 | y_pred: 49.454866603191476\n",
            "Training batch[13]: last record -> y: 44.28150134080761 | y_pred: 40.94136355820285\n",
            "Training batch[14]: last record -> y: 54.96030127145741 | y_pred: 51.61059681010056\n",
            "Training batch[15]: last record -> y: 69.6393977107623 | y_pred: 66.96688352607998\n",
            "Training batch[16]: last record -> y: 61.352299630444804 | y_pred: 62.95198532923041\n",
            "Training batch[17]: last record -> y: 44.09870203440539 | y_pred: 44.620532278429096\n",
            "Training batch[18]: last record -> y: 47.35749812182803 | y_pred: 48.00548502211359\n",
            "Training batch[19]: last record -> y: 25.808300150496677 | y_pred: 40.85510746223633\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.79956070797823\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.87816378256616\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.80039554332518\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.86611148848465\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.665641415619206\n",
            "[Training] Epoch: 839 | L1Loss: 0.05213 | RMSE: 0.08565 | PLCC: 0.94264 | SROCC: 0.94341\n",
            "[Testing]  Epoch: 839 | L1Loss: 0.07623 | RMSE: 0.10404 | PLCC: 0.91540 | SROCC: 0.92907\n",
            "Training batch[0]: last record -> y: 38.541598617054774 | y_pred: 39.22461187973727\n",
            "Training batch[1]: last record -> y: 49.50770284408554 | y_pred: 47.769886461472424\n",
            "Training batch[2]: last record -> y: 44.95980121713569 | y_pred: 56.35092874682232\n",
            "Training batch[3]: last record -> y: 63.38619901162542 | y_pred: 62.441254297973046\n",
            "Training batch[4]: last record -> y: 47.782002003196794 | y_pred: 46.798760020342684\n",
            "Training batch[5]: last record -> y: 53.02539747675837 | y_pred: 53.62325620255069\n",
            "Training batch[6]: last record -> y: 28.016500752037246 | y_pred: 27.568122788847404\n",
            "Training batch[7]: last record -> y: 56.32199875005813 | y_pred: 54.11525912139473\n",
            "Training batch[8]: last record -> y: 63.52500188770682 | y_pred: 58.75571100814295\n",
            "Training batch[9]: last record -> y: 66.65419834371073 | y_pred: 64.87628199448773\n",
            "Training batch[10]: last record -> y: 42.544398961449815 | y_pred: 57.7827111125257\n",
            "Training batch[11]: last record -> y: 28.995599425460398 | y_pred: 29.18463661165407\n",
            "Training batch[12]: last record -> y: 39.217899605891034 | y_pred: 46.31076452517493\n",
            "Training batch[13]: last record -> y: 53.30369793479122 | y_pred: 59.75510007812932\n",
            "Training batch[14]: last record -> y: 70.80740411708621 | y_pred: 68.84204904841681\n",
            "Training batch[15]: last record -> y: 48.25579783903004 | y_pred: 48.097372738350714\n",
            "Training batch[16]: last record -> y: 61.653901681202115 | y_pred: 65.93902438034524\n",
            "Training batch[17]: last record -> y: 53.03879951083468 | y_pred: 54.770855623942\n",
            "Training batch[18]: last record -> y: 52.01419840698122 | y_pred: 54.3507451139551\n",
            "Training batch[19]: last record -> y: 53.863000484795975 | y_pred: 54.179593387684235\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.075334662801424\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.64266477122874\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.962189491997606\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.01856550268201\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.646246183473636\n",
            "[Training] Epoch: 840 | L1Loss: 0.05242 | RMSE: 0.08674 | PLCC: 0.94014 | SROCC: 0.94051\n",
            "[Testing]  Epoch: 840 | L1Loss: 0.07730 | RMSE: 0.10324 | PLCC: 0.91525 | SROCC: 0.92740\n",
            "Training batch[0]: last record -> y: 64.42900076602791 | y_pred: 59.73548106976227\n",
            "Training batch[1]: last record -> y: 46.59859789153563 | y_pred: 45.67878802220923\n",
            "Training batch[2]: last record -> y: 67.75769680727763 | y_pred: 68.79315590658143\n",
            "Training batch[3]: last record -> y: 32.16310119124324 | y_pred: 37.90228267522127\n",
            "Training batch[4]: last record -> y: 30.080500033964597 | y_pred: 31.063780611589266\n",
            "Training batch[5]: last record -> y: 29.17069987919399 | y_pred: 43.118875688745675\n",
            "Training batch[6]: last record -> y: 42.995300057764894 | y_pred: 49.564552941117654\n",
            "Training batch[7]: last record -> y: 67.37490100795867 | y_pred: 63.9885958391892\n",
            "Training batch[8]: last record -> y: 69.85279784587078 | y_pred: 59.968268644614454\n",
            "Training batch[9]: last record -> y: 47.35749812182803 | y_pred: 47.71653883986846\n",
            "Training batch[10]: last record -> y: 39.685200263462434 | y_pred: 41.010277737265255\n",
            "Training batch[11]: last record -> y: 40.69929876537776 | y_pred: 56.45977564848886\n",
            "Training batch[12]: last record -> y: 44.09870203440539 | y_pred: 44.08784564707048\n",
            "Training batch[13]: last record -> y: 29.981600130351694 | y_pred: 27.644127952942142\n",
            "Training batch[14]: last record -> y: 39.02769814411886 | y_pred: 39.29865755516846\n",
            "Training batch[15]: last record -> y: 57.048500278582424 | y_pred: 57.0584448644629\n",
            "Training batch[16]: last record -> y: 73.29170125444921 | y_pred: 70.59436741606714\n",
            "Training batch[17]: last record -> y: 58.416900382336735 | y_pred: 60.06359772789301\n",
            "Training batch[18]: last record -> y: 31.38129978897092 | y_pred: 32.70881999012698\n",
            "Training batch[19]: last record -> y: 46.75879995977607 | y_pred: 47.57137425910673\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.466463716330054\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.62666739268593\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.360839547585215\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.91755992575406\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.339345948224178\n",
            "[Training] Epoch: 841 | L1Loss: 0.05221 | RMSE: 0.08518 | PLCC: 0.94296 | SROCC: 0.94344\n",
            "[Testing]  Epoch: 841 | L1Loss: 0.07702 | RMSE: 0.10472 | PLCC: 0.91411 | SROCC: 0.92939\n",
            "Training batch[0]: last record -> y: 56.92869889453914 | y_pred: 60.64462913372199\n",
            "Training batch[1]: last record -> y: 54.96030127145741 | y_pred: 50.97965593347817\n",
            "Training batch[2]: last record -> y: 62.407599658046365 | y_pred: 62.219311853647696\n",
            "Training batch[3]: last record -> y: 50.848597741355434 | y_pred: 50.24052426628873\n",
            "Training batch[4]: last record -> y: 32.14319995861649 | y_pred: 30.769574283740155\n",
            "Training batch[5]: last record -> y: 70.18830218633252 | y_pred: 65.8950713691413\n",
            "Training batch[6]: last record -> y: 39.941398782888996 | y_pred: 57.78502036629743\n",
            "Training batch[7]: last record -> y: 26.000999417575912 | y_pred: 28.886458238668354\n",
            "Training batch[8]: last record -> y: 63.8380022607023 | y_pred: 65.58429662791468\n",
            "Training batch[9]: last record -> y: 34.47520052324171 | y_pred: 36.63742108406632\n",
            "Training batch[10]: last record -> y: 64.08940216365613 | y_pred: 60.83293945173682\n",
            "Training batch[11]: last record -> y: 41.601001254850644 | y_pred: 42.902695122041564\n",
            "Training batch[12]: last record -> y: 72.54129669802592 | y_pred: 68.16838376078658\n",
            "Training batch[13]: last record -> y: 57.048500278582424 | y_pred: 57.40644104566195\n",
            "Training batch[14]: last record -> y: 56.724497179747004 | y_pred: 61.287228847283814\n",
            "Training batch[15]: last record -> y: 43.48559998235805 | y_pred: 39.827274046348634\n",
            "Training batch[16]: last record -> y: 59.38040274816581 | y_pred: 58.84942554351983\n",
            "Training batch[17]: last record -> y: 39.685200263462434 | y_pred: 39.47871984909193\n",
            "Training batch[18]: last record -> y: 61.4574993262936 | y_pred: 64.32556034847039\n",
            "Training batch[19]: last record -> y: 57.950199551824426 | y_pred: 59.50724767783595\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.589745216038864\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 47.18936322012246\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.07333469833486\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.08939188485601\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.56986165798861\n",
            "[Training] Epoch: 842 | L1Loss: 0.05219 | RMSE: 0.08548 | PLCC: 0.94224 | SROCC: 0.94271\n",
            "[Testing]  Epoch: 842 | L1Loss: 0.08053 | RMSE: 0.10916 | PLCC: 0.91618 | SROCC: 0.93035\n",
            "Training batch[0]: last record -> y: 47.35749812182803 | y_pred: 48.58258297757652\n",
            "Training batch[1]: last record -> y: 29.695998828221605 | y_pred: 48.2056214210736\n",
            "Training batch[2]: last record -> y: 60.984999631504934 | y_pred: 59.1531245768108\n",
            "Training batch[3]: last record -> y: 61.352299630444804 | y_pred: 58.582787138657295\n",
            "Training batch[4]: last record -> y: 48.95560143502075 | y_pred: 43.35668219188585\n",
            "Training batch[5]: last record -> y: 37.4706001665287 | y_pred: 37.41821098172693\n",
            "Training batch[6]: last record -> y: 64.97390103415273 | y_pred: 64.98566279048032\n",
            "Training batch[7]: last record -> y: 38.89670106038284 | y_pred: 40.87489693084001\n",
            "Training batch[8]: last record -> y: 65.45760286109589 | y_pred: 67.00076651839913\n",
            "Training batch[9]: last record -> y: 61.02579752021575 | y_pred: 58.822997774380156\n",
            "Training batch[10]: last record -> y: 43.36610092401747 | y_pred: 48.00975296049114\n",
            "Training batch[11]: last record -> y: 27.831899552284597 | y_pred: 26.7481712599361\n",
            "Training batch[12]: last record -> y: 58.70890198391771 | y_pred: 56.50836324839065\n",
            "Training batch[13]: last record -> y: 43.08390078604282 | y_pred: 43.59040568992407\n",
            "Training batch[14]: last record -> y: 30.038900499706983 | y_pred: 31.949765380752353\n",
            "Training batch[15]: last record -> y: 33.262500568553776 | y_pred: 49.64741269727506\n",
            "Training batch[16]: last record -> y: 26.635299647352298 | y_pred: 40.64614411762659\n",
            "Training batch[17]: last record -> y: 67.19999594025103 | y_pred: 61.39321651969817\n",
            "Training batch[18]: last record -> y: 57.412301018325024 | y_pred: 55.97340434991543\n",
            "Training batch[19]: last record -> y: 28.995599425460398 | y_pred: 28.035609987809835\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.62509143955481\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.48740957378857\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.33568379775852\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.55093051095264\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.009406243066962\n",
            "[Training] Epoch: 843 | L1Loss: 0.05716 | RMSE: 0.08709 | PLCC: 0.93947 | SROCC: 0.93918\n",
            "[Testing]  Epoch: 843 | L1Loss: 0.07741 | RMSE: 0.10564 | PLCC: 0.91623 | SROCC: 0.93107\n",
            "Training batch[0]: last record -> y: 30.256500228286825 | y_pred: 31.623740880808953\n",
            "Training batch[1]: last record -> y: 68.06819816887946 | y_pred: 70.42057516424393\n",
            "Training batch[2]: last record -> y: 44.64339807186934 | y_pred: 46.83314796090997\n",
            "Training batch[3]: last record -> y: 45.45109978141704 | y_pred: 44.733029602144256\n",
            "Training batch[4]: last record -> y: 61.76470083501272 | y_pred: 61.3097456796736\n",
            "Training batch[5]: last record -> y: 60.042196927618534 | y_pred: 59.30578297552256\n",
            "Training batch[6]: last record -> y: 27.51029978197414 | y_pred: 27.909881090255\n",
            "Training batch[7]: last record -> y: 28.114999430847888 | y_pred: 28.622035012824313\n",
            "Training batch[8]: last record -> y: 49.146899631522956 | y_pred: 56.31423155248331\n",
            "Training batch[9]: last record -> y: 69.18840173563626 | y_pred: 68.1634179003081\n",
            "Training batch[10]: last record -> y: 54.37429757268819 | y_pred: 57.13096443834161\n",
            "Training batch[11]: last record -> y: 67.8711011081009 | y_pred: 65.65032906410875\n",
            "Training batch[12]: last record -> y: 48.22529832159648 | y_pred: 49.16595901571691\n",
            "Training batch[13]: last record -> y: 67.08080242384403 | y_pred: 66.75215187138724\n",
            "Training batch[14]: last record -> y: 49.1077999127167 | y_pred: 48.96925111887481\n",
            "Training batch[15]: last record -> y: 62.23139844929415 | y_pred: 58.728562803286195\n",
            "Training batch[16]: last record -> y: 39.67380194122427 | y_pred: 57.922903400347195\n",
            "Training batch[17]: last record -> y: 61.21029982086884 | y_pred: 58.935739531642184\n",
            "Training batch[18]: last record -> y: 43.742801965819126 | y_pred: 60.74223852469436\n",
            "Training batch[19]: last record -> y: 54.38159841678544 | y_pred: 50.160945065628766\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.24271214213479\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.20822128217935\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.23829326433952\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.484066679076136\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.339645861753723\n",
            "[Training] Epoch: 844 | L1Loss: 0.05240 | RMSE: 0.08671 | PLCC: 0.94185 | SROCC: 0.94376\n",
            "[Testing]  Epoch: 844 | L1Loss: 0.07538 | RMSE: 0.10199 | PLCC: 0.91703 | SROCC: 0.93168\n",
            "Training batch[0]: last record -> y: 69.6393977107623 | y_pred: 66.38194439973199\n",
            "Training batch[1]: last record -> y: 67.50070066259286 | y_pred: 64.71084872672122\n",
            "Training batch[2]: last record -> y: 69.45530065520006 | y_pred: 66.68426045374463\n",
            "Training batch[3]: last record -> y: 67.78130394193568 | y_pred: 67.9591325635131\n",
            "Training batch[4]: last record -> y: 63.49129900431694 | y_pred: 60.987411804664816\n",
            "Training batch[5]: last record -> y: 64.97390103415273 | y_pred: 63.537268592282544\n",
            "Training batch[6]: last record -> y: 64.29779784351558 | y_pred: 63.382645076503195\n",
            "Training batch[7]: last record -> y: 68.25509656153804 | y_pred: 69.07036606234601\n",
            "Training batch[8]: last record -> y: 31.38129978897092 | y_pred: 30.97517103867642\n",
            "Training batch[9]: last record -> y: 60.85200205216165 | y_pred: 58.64865233084561\n",
            "Training batch[10]: last record -> y: 58.70890198391771 | y_pred: 57.80478732128495\n",
            "Training batch[11]: last record -> y: 63.25490281841758 | y_pred: 64.2472290453918\n",
            "Training batch[12]: last record -> y: 43.742801965819126 | y_pred: 60.20033256751867\n",
            "Training batch[13]: last record -> y: 39.74829788897864 | y_pred: 31.14543669333989\n",
            "Training batch[14]: last record -> y: 63.37459806684183 | y_pred: 61.67170030289117\n",
            "Training batch[15]: last record -> y: 52.85669973464883 | y_pred: 52.47062659606331\n",
            "Training batch[16]: last record -> y: 40.48429855540655 | y_pred: 39.7598409416064\n",
            "Training batch[17]: last record -> y: 58.07179880892954 | y_pred: 59.02520220979591\n",
            "Training batch[18]: last record -> y: 63.890597284280375 | y_pred: 60.72467468785953\n",
            "Training batch[19]: last record -> y: 65.45760286109589 | y_pred: 64.29538245412482\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.10213390660772\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.39348904578185\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.68318322514017\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.94060743623902\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.517432270300844\n",
            "[Training] Epoch: 845 | L1Loss: 0.05036 | RMSE: 0.08355 | PLCC: 0.94437 | SROCC: 0.94588\n",
            "[Testing]  Epoch: 845 | L1Loss: 0.07566 | RMSE: 0.10345 | PLCC: 0.91510 | SROCC: 0.93065\n",
            "Training batch[0]: last record -> y: 65.36119955670347 | y_pred: 64.01215794661493\n",
            "Training batch[1]: last record -> y: 55.782900562440545 | y_pred: 52.516756995572905\n",
            "Training batch[2]: last record -> y: 60.352402395979425 | y_pred: 54.20932100970663\n",
            "Training batch[3]: last record -> y: 51.062699014795726 | y_pred: 49.685640817512876\n",
            "Training batch[4]: last record -> y: 61.21029982086884 | y_pred: 57.10794587393457\n",
            "Training batch[5]: last record -> y: 46.69590013245897 | y_pred: 54.749972636839175\n",
            "Training batch[6]: last record -> y: 53.07039897922914 | y_pred: 51.95037552140161\n",
            "Training batch[7]: last record -> y: 47.48310158637855 | y_pred: 59.42100766302383\n",
            "Training batch[8]: last record -> y: 36.69869993101997 | y_pred: 37.54339472814763\n",
            "Training batch[9]: last record -> y: 64.93270111658194 | y_pred: 66.64310877963703\n",
            "Training batch[10]: last record -> y: 52.48210210784259 | y_pred: 53.75846654873931\n",
            "Training batch[11]: last record -> y: 51.99429958652763 | y_pred: 52.26986946454349\n",
            "Training batch[12]: last record -> y: 61.434799168743666 | y_pred: 60.00217415054976\n",
            "Training batch[13]: last record -> y: 64.74639772663613 | y_pred: 62.916799763404924\n",
            "Training batch[14]: last record -> y: 38.79510193500403 | y_pred: 42.78231160020903\n",
            "Training batch[15]: last record -> y: 59.352698135366836 | y_pred: 60.528384901031814\n",
            "Training batch[16]: last record -> y: 40.63280158382156 | y_pred: 37.233925776658225\n",
            "Training batch[17]: last record -> y: 30.038900499706983 | y_pred: 30.24427613877026\n",
            "Training batch[18]: last record -> y: 33.643200189396794 | y_pred: 33.7474173267118\n",
            "Training batch[19]: last record -> y: 47.55770206163652 | y_pred: 37.496937881203735\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.26752536337278\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.621445841852506\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.970506818906415\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.00657056961563\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.99358062516916\n",
            "[Training] Epoch: 846 | L1Loss: 0.05191 | RMSE: 0.08474 | PLCC: 0.94249 | SROCC: 0.94277\n",
            "[Testing]  Epoch: 846 | L1Loss: 0.07616 | RMSE: 0.10304 | PLCC: 0.91631 | SROCC: 0.92906\n",
            "Training batch[0]: last record -> y: 64.07659834852348 | y_pred: 61.980123979344626\n",
            "Training batch[1]: last record -> y: 58.50000135581013 | y_pred: 65.87981356984733\n",
            "Training batch[2]: last record -> y: 45.65990070636735 | y_pred: 45.74230214962438\n",
            "Training batch[3]: last record -> y: 25.665500705518212 | y_pred: 25.265860298994937\n",
            "Training batch[4]: last record -> y: 62.979599887564746 | y_pred: 64.40395919239745\n",
            "Training batch[5]: last record -> y: 31.38129978897092 | y_pred: 30.405305582404537\n",
            "Training batch[6]: last record -> y: 61.21029982086884 | y_pred: 57.29929874636173\n",
            "Training batch[7]: last record -> y: 59.44990228124834 | y_pred: 61.10995984987949\n",
            "Training batch[8]: last record -> y: 46.75879995977607 | y_pred: 47.42483634775931\n",
            "Training batch[9]: last record -> y: 67.78130394193568 | y_pred: 67.53456114129585\n",
            "Training batch[10]: last record -> y: 48.33070063999071 | y_pred: 48.608788826785485\n",
            "Training batch[11]: last record -> y: 29.17069987919399 | y_pred: 41.84293900655598\n",
            "Training batch[12]: last record -> y: 63.56399868712492 | y_pred: 65.96240959507259\n",
            "Training batch[13]: last record -> y: 65.80919800464949 | y_pred: 59.62994366967109\n",
            "Training batch[14]: last record -> y: 64.4866034610859 | y_pred: 61.67146873426782\n",
            "Training batch[15]: last record -> y: 58.07179880892954 | y_pred: 57.98354864981752\n",
            "Training batch[16]: last record -> y: 38.541598617054774 | y_pred: 39.034773852054514\n",
            "Training batch[17]: last record -> y: 61.02579752021575 | y_pred: 59.98437231262983\n",
            "Training batch[18]: last record -> y: 37.76599810791879 | y_pred: 36.87612330750653\n",
            "Training batch[19]: last record -> y: 48.23669664383465 | y_pred: 46.24381546318011\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.921634359138125\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.4678339845384\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.90023393615081\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.06568022269437\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.17250573330128\n",
            "[Training] Epoch: 847 | L1Loss: 0.05270 | RMSE: 0.08538 | PLCC: 0.94210 | SROCC: 0.94289\n",
            "[Testing]  Epoch: 847 | L1Loss: 0.07864 | RMSE: 0.10682 | PLCC: 0.91646 | SROCC: 0.93232\n",
            "Training batch[0]: last record -> y: 30.420999180982733 | y_pred: 27.085882738839132\n",
            "Training batch[1]: last record -> y: 52.01739855670667 | y_pred: 52.751664066574904\n",
            "Training batch[2]: last record -> y: 29.01670111626305 | y_pred: 29.130167329530764\n",
            "Training batch[3]: last record -> y: 51.13240117042369 | y_pred: 55.263273436569534\n",
            "Training batch[4]: last record -> y: 64.93270111658194 | y_pred: 65.62421326936442\n",
            "Training batch[5]: last record -> y: 49.4674999580875 | y_pred: 57.94125842997846\n",
            "Training batch[6]: last record -> y: 45.05469932547635 | y_pred: 43.323997245569444\n",
            "Training batch[7]: last record -> y: 25.608800967279734 | y_pred: 25.116085653295812\n",
            "Training batch[8]: last record -> y: 43.67010067489571 | y_pred: 43.17618409867816\n",
            "Training batch[9]: last record -> y: 53.70280002467098 | y_pred: 53.46054064987766\n",
            "Training batch[10]: last record -> y: 27.261600708901653 | y_pred: 27.673227605885245\n",
            "Training batch[11]: last record -> y: 55.8553011357767 | y_pred: 51.33458309222328\n",
            "Training batch[12]: last record -> y: 63.33969874556465 | y_pred: 62.55387705469252\n",
            "Training batch[13]: last record -> y: 39.941398782888996 | y_pred: 57.366276754434466\n",
            "Training batch[14]: last record -> y: 61.01490092999484 | y_pred: 50.19470584117448\n",
            "Training batch[15]: last record -> y: 48.61320149555263 | y_pred: 47.55319933840474\n",
            "Training batch[16]: last record -> y: 53.03879951083468 | y_pred: 53.731379452269266\n",
            "Training batch[17]: last record -> y: 51.062699014795726 | y_pred: 51.106639161076146\n",
            "Training batch[18]: last record -> y: 31.31949991261473 | y_pred: 32.67341893683255\n",
            "Training batch[19]: last record -> y: 42.74340003090924 | y_pred: 42.61675129132277\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.136073182967266\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.32943298346345\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.416089569754604\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.472977115002436\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.312311919563655\n",
            "[Training] Epoch: 848 | L1Loss: 0.05094 | RMSE: 0.08474 | PLCC: 0.94325 | SROCC: 0.94393\n",
            "[Testing]  Epoch: 848 | L1Loss: 0.07669 | RMSE: 0.10360 | PLCC: 0.91480 | SROCC: 0.92806\n",
            "Training batch[0]: last record -> y: 33.262500568553776 | y_pred: 46.000700570972754\n",
            "Training batch[1]: last record -> y: 46.40150083075707 | y_pred: 42.26717433264628\n",
            "Training batch[2]: last record -> y: 48.289101909790816 | y_pred: 61.128070445963886\n",
            "Training batch[3]: last record -> y: 57.16640087017572 | y_pred: 58.35744513829195\n",
            "Training batch[4]: last record -> y: 60.85200205216165 | y_pred: 60.96756766013618\n",
            "Training batch[5]: last record -> y: 68.4131003359721 | y_pred: 64.9585885589338\n",
            "Training batch[6]: last record -> y: 61.12310136925453 | y_pred: 61.93735132487359\n",
            "Training batch[7]: last record -> y: 60.352402395979425 | y_pred: 53.115265400002954\n",
            "Training batch[8]: last record -> y: 48.23669664383465 | y_pred: 45.66432624005802\n",
            "Training batch[9]: last record -> y: 39.04919864755061 | y_pred: 39.458501013665796\n",
            "Training batch[10]: last record -> y: 27.261600708901653 | y_pred: 27.806287097672964\n",
            "Training batch[11]: last record -> y: 32.322799919350985 | y_pred: 29.58540068884099\n",
            "Training batch[12]: last record -> y: 48.559300682237335 | y_pred: 46.839205731772154\n",
            "Training batch[13]: last record -> y: 53.082598142956385 | y_pred: 52.571349298527366\n",
            "Training batch[14]: last record -> y: 40.42119932177491 | y_pred: 39.645529663675006\n",
            "Training batch[15]: last record -> y: 30.038900499706983 | y_pred: 30.795829180470037\n",
            "Training batch[16]: last record -> y: 50.734000218875735 | y_pred: 61.302000995714934\n",
            "Training batch[17]: last record -> y: 64.29779784351558 | y_pred: 63.789971068742716\n",
            "Training batch[18]: last record -> y: 28.118200384631052 | y_pred: 28.835128801941494\n",
            "Training batch[19]: last record -> y: 47.35749812182803 | y_pred: 47.30478088147652\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.299840289284475\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 43.898834190723846\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 33.199874924919186\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.40100590848874\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.824207628428383\n",
            "[Training] Epoch: 849 | L1Loss: 0.05175 | RMSE: 0.08525 | PLCC: 0.94222 | SROCC: 0.94332\n",
            "[Testing]  Epoch: 849 | L1Loss: 0.08261 | RMSE: 0.10742 | PLCC: 0.91224 | SROCC: 0.92548\n",
            "Training batch[0]: last record -> y: 54.76450035172343 | y_pred: 53.840026947621254\n",
            "Training batch[1]: last record -> y: 46.59859789153563 | y_pred: 42.25005433567287\n",
            "Training batch[2]: last record -> y: 28.917199777475616 | y_pred: 32.048692622327735\n",
            "Training batch[3]: last record -> y: 67.50669893318377 | y_pred: 68.99869878965046\n",
            "Training batch[4]: last record -> y: 58.32300252179971 | y_pred: 58.46457778889953\n",
            "Training batch[5]: last record -> y: 37.30360059432371 | y_pred: 39.40828921716968\n",
            "Training batch[6]: last record -> y: 43.92589877357773 | y_pred: 44.259402618432205\n",
            "Training batch[7]: last record -> y: 43.4957005554362 | y_pred: 44.28132605622466\n",
            "Training batch[8]: last record -> y: 68.86179992224993 | y_pred: 67.30671048084423\n",
            "Training batch[9]: last record -> y: 51.13240117042369 | y_pred: 53.75820281780716\n",
            "Training batch[10]: last record -> y: 45.04560060831727 | y_pred: 43.75490625073542\n",
            "Training batch[11]: last record -> y: 27.51029978197414 | y_pred: 30.311587830796782\n",
            "Training batch[12]: last record -> y: 61.02579752021575 | y_pred: 62.31020897077383\n",
            "Training batch[13]: last record -> y: 37.43249908741063 | y_pred: 51.01312081578294\n",
            "Training batch[14]: last record -> y: 66.13350021295673 | y_pred: 66.32905991537473\n",
            "Training batch[15]: last record -> y: 38.89670106038284 | y_pred: 37.966721469014374\n",
            "Training batch[16]: last record -> y: 30.074199437670984 | y_pred: 29.016240391189513\n",
            "Training batch[17]: last record -> y: 45.30530160317198 | y_pred: 44.74665677238215\n",
            "Training batch[18]: last record -> y: 28.998800379243562 | y_pred: 28.79328644225245\n",
            "Training batch[19]: last record -> y: 65.45760286109589 | y_pred: 65.3613378546313\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.84138794442413\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.93856138225874\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.07072955132219\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.41544050653101\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.89922822384503\n",
            "[Training] Epoch: 850 | L1Loss: 0.05697 | RMSE: 0.08806 | PLCC: 0.93779 | SROCC: 0.93775\n",
            "[Testing]  Epoch: 850 | L1Loss: 0.07672 | RMSE: 0.10426 | PLCC: 0.91702 | SROCC: 0.93102\n",
            "Training batch[0]: last record -> y: 67.7326970446486 | y_pred: 67.29657613734184\n",
            "Training batch[1]: last record -> y: 54.38009965319543 | y_pred: 63.16096314687911\n",
            "Training batch[2]: last record -> y: 22.983800965443066 | y_pred: 24.23615704687947\n",
            "Training batch[3]: last record -> y: 60.54589727817256 | y_pred: 59.817556065584995\n",
            "Training batch[4]: last record -> y: 52.21930066641971 | y_pred: 51.09028784328302\n",
            "Training batch[5]: last record -> y: 52.915900896454104 | y_pred: 50.047830225584676\n",
            "Training batch[6]: last record -> y: 67.19999594025103 | y_pred: 64.30696731775402\n",
            "Training batch[7]: last record -> y: 41.601200661165194 | y_pred: 44.81223411190672\n",
            "Training batch[8]: last record -> y: 36.78459902535883 | y_pred: 36.55528015551056\n",
            "Training batch[9]: last record -> y: 59.43330009744659 | y_pred: 61.35634564889165\n",
            "Training batch[10]: last record -> y: 64.35050221894357 | y_pred: 63.22340626941127\n",
            "Training batch[11]: last record -> y: 38.38710053427974 | y_pred: 37.25108919274851\n",
            "Training batch[12]: last record -> y: 55.71429835777349 | y_pred: 50.9407974319879\n",
            "Training batch[13]: last record -> y: 66.61560035692173 | y_pred: 70.33180397572937\n",
            "Training batch[14]: last record -> y: 55.55740096676209 | y_pred: 60.55283790441126\n",
            "Training batch[15]: last record -> y: 58.14030131043933 | y_pred: 63.5694276848501\n",
            "Training batch[16]: last record -> y: 48.60699738618541 | y_pred: 49.09442360848743\n",
            "Training batch[17]: last record -> y: 64.75720104616153 | y_pred: 61.4623654836148\n",
            "Training batch[18]: last record -> y: 43.31629919695854 | y_pred: 42.00536349033439\n",
            "Training batch[19]: last record -> y: 39.49110072986389 | y_pred: 38.901496460624685\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.93502207402901\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.70793848464223\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.220147443568976\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.60194973525313\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.195986630897295\n",
            "[Training] Epoch: 851 | L1Loss: 0.05185 | RMSE: 0.08456 | PLCC: 0.94315 | SROCC: 0.94370\n",
            "[Testing]  Epoch: 851 | L1Loss: 0.07894 | RMSE: 0.10698 | PLCC: 0.91678 | SROCC: 0.93193\n",
            "Training batch[0]: last record -> y: 67.85620031043459 | y_pred: 70.630961691018\n",
            "Training batch[1]: last record -> y: 63.56399868712492 | y_pred: 67.8792220910725\n",
            "Training batch[2]: last record -> y: 38.51679986885574 | y_pred: 39.0042035775416\n",
            "Training batch[3]: last record -> y: 58.07179880892954 | y_pred: 56.89999725016742\n",
            "Training batch[4]: last record -> y: 33.63470129929681 | y_pred: 33.52129458630037\n",
            "Training batch[5]: last record -> y: 60.28530217113325 | y_pred: 57.51562243533999\n",
            "Training batch[6]: last record -> y: 43.907299310399594 | y_pred: 51.27402146475583\n",
            "Training batch[7]: last record -> y: 65.01059822849174 | y_pred: 65.24416734744773\n",
            "Training batch[8]: last record -> y: 48.261201106908175 | y_pred: 47.8102115642439\n",
            "Training batch[9]: last record -> y: 36.83800132288775 | y_pred: 37.76725243796193\n",
            "Training batch[10]: last record -> y: 44.302501720337546 | y_pred: 44.19199202730613\n",
            "Training batch[11]: last record -> y: 31.24950025563038 | y_pred: 31.880165340454823\n",
            "Training batch[12]: last record -> y: 45.45109978141704 | y_pred: 44.36787804543212\n",
            "Training batch[13]: last record -> y: 37.4706001665287 | y_pred: 38.043478427077275\n",
            "Training batch[14]: last record -> y: 52.886501329981456 | y_pred: 54.243458084265285\n",
            "Training batch[15]: last record -> y: 39.43050050762588 | y_pred: 41.085413714964716\n",
            "Training batch[16]: last record -> y: 25.895799721727116 | y_pred: 26.751607400602353\n",
            "Training batch[17]: last record -> y: 39.49110072986389 | y_pred: 39.18766221127419\n",
            "Training batch[18]: last record -> y: 48.248902240023654 | y_pred: 48.24283642858427\n",
            "Training batch[19]: last record -> y: 62.27109960327493 | y_pred: 58.22402623270341\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 47.049034242488574\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.98302271179523\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.43129123886172\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.45180964532028\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.74706190845788\n",
            "[Training] Epoch: 852 | L1Loss: 0.05092 | RMSE: 0.08352 | PLCC: 0.94491 | SROCC: 0.94599\n",
            "[Testing]  Epoch: 852 | L1Loss: 0.07879 | RMSE: 0.10749 | PLCC: 0.91605 | SROCC: 0.92895\n",
            "Training batch[0]: last record -> y: 45.92639920518661 | y_pred: 46.20734822934901\n",
            "Training batch[1]: last record -> y: 63.999000346085495 | y_pred: 62.82682248831043\n",
            "Training batch[2]: last record -> y: 56.25930154528646 | y_pred: 53.42565097729312\n",
            "Training batch[3]: last record -> y: 49.146899631522956 | y_pred: 54.63588971130025\n",
            "Training batch[4]: last record -> y: 34.8158990765744 | y_pred: 32.05193136682374\n",
            "Training batch[5]: last record -> y: 61.76470083501272 | y_pred: 60.361166625127\n",
            "Training batch[6]: last record -> y: 48.61320149555263 | y_pred: 48.341278823354514\n",
            "Training batch[7]: last record -> y: 44.271098442026755 | y_pred: 42.24081571247052\n",
            "Training batch[8]: last record -> y: 33.262500568553776 | y_pred: 48.17546282411331\n",
            "Training batch[9]: last record -> y: 50.848597741355434 | y_pred: 50.59097121541822\n",
            "Training batch[10]: last record -> y: 70.58750076313868 | y_pred: 65.27659017094743\n",
            "Training batch[11]: last record -> y: 63.459201020136106 | y_pred: 61.77902592735154\n",
            "Training batch[12]: last record -> y: 54.647500304776486 | y_pred: 55.26689169630936\n",
            "Training batch[13]: last record -> y: 58.383400121492286 | y_pred: 57.416211955074914\n",
            "Training batch[14]: last record -> y: 29.49510018254307 | y_pred: 47.82766926545969\n",
            "Training batch[15]: last record -> y: 56.801496963241334 | y_pred: 53.21142427084851\n",
            "Training batch[16]: last record -> y: 48.53829869459196 | y_pred: 46.16833052443019\n",
            "Training batch[17]: last record -> y: 34.00809927198486 | y_pred: 37.561834987897214\n",
            "Training batch[18]: last record -> y: 48.261201106908175 | y_pred: 46.208443355963595\n",
            "Training batch[19]: last record -> y: 69.84349972239715 | y_pred: 69.88178251855993\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.74161563818507\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.8718762050496\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 39.81817372107412\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.842038154202555\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.719215781500196\n",
            "[Training] Epoch: 853 | L1Loss: 0.05266 | RMSE: 0.08404 | PLCC: 0.94346 | SROCC: 0.94382\n",
            "[Testing]  Epoch: 853 | L1Loss: 0.08255 | RMSE: 0.11013 | PLCC: 0.91686 | SROCC: 0.93131\n",
            "Training batch[0]: last record -> y: 68.4131003359721 | y_pred: 65.75247334062169\n",
            "Training batch[1]: last record -> y: 50.848597741355434 | y_pred: 52.812180666810036\n",
            "Training batch[2]: last record -> y: 32.322799919350985 | y_pred: 29.423116915163575\n",
            "Training batch[3]: last record -> y: 72.99610069051369 | y_pred: 69.34744435264452\n",
            "Training batch[4]: last record -> y: 47.66609869097988 | y_pred: 48.409015861914895\n",
            "Training batch[5]: last record -> y: 67.31629806509704 | y_pred: 66.80346361884426\n",
            "Training batch[6]: last record -> y: 63.8380022607023 | y_pred: 64.8690422587772\n",
            "Training batch[7]: last record -> y: 55.8553011357767 | y_pred: 50.270589592553506\n",
            "Training batch[8]: last record -> y: 61.18830080165071 | y_pred: 63.768033157911304\n",
            "Training batch[9]: last record -> y: 56.801496963241334 | y_pred: 55.34323215247332\n",
            "Training batch[10]: last record -> y: 26.202400197300562 | y_pred: 28.290821124356285\n",
            "Training batch[11]: last record -> y: 22.983800965443066 | y_pred: 22.87328102067572\n",
            "Training batch[12]: last record -> y: 70.78559807172087 | y_pred: 65.52100120419937\n",
            "Training batch[13]: last record -> y: 54.38009965319543 | y_pred: 62.16068639716991\n",
            "Training batch[14]: last record -> y: 32.369998911570406 | y_pred: 31.47617136340682\n",
            "Training batch[15]: last record -> y: 65.36119955670347 | y_pred: 64.23036634688879\n",
            "Training batch[16]: last record -> y: 68.08779787986123 | y_pred: 65.14684741725455\n",
            "Training batch[17]: last record -> y: 68.86179992224993 | y_pred: 69.8079410737896\n",
            "Training batch[18]: last record -> y: 55.70909771244078 | y_pred: 57.5089358913408\n",
            "Training batch[19]: last record -> y: 63.49129900431694 | y_pred: 61.52358322218174\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.303230350485364\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.434825653165035\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.43429022030364\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.12244991395448\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.20216445453508\n",
            "[Training] Epoch: 854 | L1Loss: 0.05328 | RMSE: 0.08543 | PLCC: 0.94289 | SROCC: 0.94342\n",
            "[Testing]  Epoch: 854 | L1Loss: 0.07703 | RMSE: 0.10434 | PLCC: 0.91408 | SROCC: 0.92952\n",
            "Training batch[0]: last record -> y: 47.01879845598717 | y_pred: 41.75587402852318\n",
            "Training batch[1]: last record -> y: 56.977000249892626 | y_pred: 55.54198235592412\n",
            "Training batch[2]: last record -> y: 27.960300333642863 | y_pred: 28.357476705283318\n",
            "Training batch[3]: last record -> y: 24.490699609431772 | y_pred: 24.700477464511835\n",
            "Training batch[4]: last record -> y: 64.73490291747157 | y_pred: 64.88552865826728\n",
            "Training batch[5]: last record -> y: 48.061598602274216 | y_pred: 61.37798766648211\n",
            "Training batch[6]: last record -> y: 54.546501006456765 | y_pred: 49.039927792459366\n",
            "Training batch[7]: last record -> y: 57.655602451923414 | y_pred: 56.616480065647465\n",
            "Training batch[8]: last record -> y: 48.53829869459196 | y_pred: 47.364165368441945\n",
            "Training batch[9]: last record -> y: 36.90119865156123 | y_pred: 36.47311992956952\n",
            "Training batch[10]: last record -> y: 42.00540208510495 | y_pred: 41.84865424882946\n",
            "Training batch[11]: last record -> y: 43.67010067489571 | y_pred: 44.71462793716523\n",
            "Training batch[12]: last record -> y: 47.797999535593135 | y_pred: 47.008964830072046\n",
            "Training batch[13]: last record -> y: 63.91980066066935 | y_pred: 62.39493092461066\n",
            "Training batch[14]: last record -> y: 63.36859979625092 | y_pred: 57.83369802066386\n",
            "Training batch[15]: last record -> y: 38.469501977536765 | y_pred: 39.22621195459999\n",
            "Training batch[16]: last record -> y: 69.50440285204263 | y_pred: 63.318153214900576\n",
            "Training batch[17]: last record -> y: 38.08380192173331 | y_pred: 45.023169006045805\n",
            "Training batch[18]: last record -> y: 63.38790039776086 | y_pred: 63.3767239954534\n",
            "Training batch[19]: last record -> y: 55.55740096676209 | y_pred: 57.26149838483093\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.235162193997894\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.93157412067228\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.783653453480156\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.96965176305298\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.522390646084517\n",
            "[Training] Epoch: 855 | L1Loss: 0.05134 | RMSE: 0.08315 | PLCC: 0.94495 | SROCC: 0.94614\n",
            "[Testing]  Epoch: 855 | L1Loss: 0.07676 | RMSE: 0.10407 | PLCC: 0.91552 | SROCC: 0.93016\n",
            "Training batch[0]: last record -> y: 69.42099633663565 | y_pred: 70.13017202941023\n",
            "Training batch[1]: last record -> y: 28.415199204941587 | y_pred: 29.795574140319275\n",
            "Training batch[2]: last record -> y: 61.01490092999484 | y_pred: 50.9810710750653\n",
            "Training batch[3]: last record -> y: 54.441201607450694 | y_pred: 59.811204009597304\n",
            "Training batch[4]: last record -> y: 38.67589876990439 | y_pred: 53.47320616708248\n",
            "Training batch[5]: last record -> y: 64.75720104616153 | y_pred: 62.80042044901779\n",
            "Training batch[6]: last record -> y: 21.73069952250026 | y_pred: 21.744534742673196\n",
            "Training batch[7]: last record -> y: 48.6593994359107 | y_pred: 45.816899408651466\n",
            "Training batch[8]: last record -> y: 36.413502265865304 | y_pred: 37.256508541781045\n",
            "Training batch[9]: last record -> y: 55.84130088275674 | y_pred: 57.062651694453734\n",
            "Training batch[10]: last record -> y: 61.12310136925453 | y_pred: 63.28415765450063\n",
            "Training batch[11]: last record -> y: 55.55740096676209 | y_pred: 58.93076723870195\n",
            "Training batch[12]: last record -> y: 48.261201106908175 | y_pred: 47.299486965448295\n",
            "Training batch[13]: last record -> y: 59.88249819951079 | y_pred: 57.57625160365592\n",
            "Training batch[14]: last record -> y: 38.542598864858405 | y_pred: 39.13755011793535\n",
            "Training batch[15]: last record -> y: 40.63280158382156 | y_pred: 36.58956678480513\n",
            "Training batch[16]: last record -> y: 58.169900283226525 | y_pred: 56.45023630769924\n",
            "Training batch[17]: last record -> y: 67.37490100795867 | y_pred: 63.815266724612684\n",
            "Training batch[18]: last record -> y: 67.35350020768419 | y_pred: 67.43432087346378\n",
            "Training batch[19]: last record -> y: 40.48429855540655 | y_pred: 39.7894238332392\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.58600941001839\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.663144275209675\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.492216454272125\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.74188296263526\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.373243413582287\n",
            "[Training] Epoch: 856 | L1Loss: 0.05122 | RMSE: 0.08380 | PLCC: 0.94449 | SROCC: 0.94524\n",
            "[Testing]  Epoch: 856 | L1Loss: 0.07620 | RMSE: 0.10367 | PLCC: 0.91691 | SROCC: 0.93185\n",
            "Training batch[0]: last record -> y: 49.1077999127167 | y_pred: 48.938957440217564\n",
            "Training batch[1]: last record -> y: 28.126199954886943 | y_pred: 28.556308118563834\n",
            "Training batch[2]: last record -> y: 54.97959865673647 | y_pred: 63.55933515234915\n",
            "Training batch[3]: last record -> y: 53.34080037422109 | y_pred: 54.13597808072268\n",
            "Training batch[4]: last record -> y: 40.68579863625962 | y_pred: 45.77779325738345\n",
            "Training batch[5]: last record -> y: 49.9361015810498 | y_pred: 50.20049184052732\n",
            "Training batch[6]: last record -> y: 58.27209923566443 | y_pred: 60.19645379307758\n",
            "Training batch[7]: last record -> y: 64.08940216365613 | y_pred: 60.65560291348402\n",
            "Training batch[8]: last record -> y: 62.796901892435244 | y_pred: 63.483470698355404\n",
            "Training batch[9]: last record -> y: 45.47759991575151 | y_pred: 45.82280923289318\n",
            "Training batch[10]: last record -> y: 43.501399716555284 | y_pred: 43.4957246771678\n",
            "Training batch[11]: last record -> y: 30.96270010343983 | y_pred: 27.980604399187314\n",
            "Training batch[12]: last record -> y: 67.38690076537137 | y_pred: 69.38366232858243\n",
            "Training batch[13]: last record -> y: 25.608800967279734 | y_pred: 25.82526375824355\n",
            "Training batch[14]: last record -> y: 61.97249829592852 | y_pred: 61.15503854189137\n",
            "Training batch[15]: last record -> y: 35.58000156031494 | y_pred: 35.28810931058388\n",
            "Training batch[16]: last record -> y: 72.15129975776699 | y_pred: 69.13166099045407\n",
            "Training batch[17]: last record -> y: 58.169900283226525 | y_pred: 56.71859539608249\n",
            "Training batch[18]: last record -> y: 28.114999430847888 | y_pred: 27.9747428184088\n",
            "Training batch[19]: last record -> y: 59.62590086745513 | y_pred: 58.76273847261541\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.380812271768946\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.705411981432576\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.71689399525371\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.29842744080702\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.639342543890052\n",
            "[Training] Epoch: 857 | L1Loss: 0.05007 | RMSE: 0.08429 | PLCC: 0.94377 | SROCC: 0.94470\n",
            "[Testing]  Epoch: 857 | L1Loss: 0.07702 | RMSE: 0.10365 | PLCC: 0.91496 | SROCC: 0.92700\n",
            "Training batch[0]: last record -> y: 61.76470083501272 | y_pred: 59.799297522880124\n",
            "Training batch[1]: last record -> y: 43.91579820049958 | y_pred: 42.10383965552887\n",
            "Training batch[2]: last record -> y: 56.61489768005458 | y_pred: 61.352679145688626\n",
            "Training batch[3]: last record -> y: 29.03989978959936 | y_pred: 28.88141840487964\n",
            "Training batch[4]: last record -> y: 52.05239758114112 | y_pred: 51.257956391511016\n",
            "Training batch[5]: last record -> y: 61.21029982086884 | y_pred: 57.5137698863532\n",
            "Training batch[6]: last record -> y: 65.2375033170647 | y_pred: 66.61705087704854\n",
            "Training batch[7]: last record -> y: 51.062699014795726 | y_pred: 49.92352933454049\n",
            "Training batch[8]: last record -> y: 38.522899450719365 | y_pred: 42.50463830719775\n",
            "Training batch[9]: last record -> y: 69.42099633663565 | y_pred: 70.27479628338415\n",
            "Training batch[10]: last record -> y: 29.17069987919399 | y_pred: 43.04492167589456\n",
            "Training batch[11]: last record -> y: 56.724497179747004 | y_pred: 60.865693547017145\n",
            "Training batch[12]: last record -> y: 52.01739855670667 | y_pred: 52.971117148199255\n",
            "Training batch[13]: last record -> y: 49.50770284408554 | y_pred: 49.49014865594336\n",
            "Training batch[14]: last record -> y: 68.06819816887946 | y_pred: 68.56855042308757\n",
            "Training batch[15]: last record -> y: 64.08940216365613 | y_pred: 59.88312858076324\n",
            "Training batch[16]: last record -> y: 67.19999594025103 | y_pred: 62.04617571292397\n",
            "Training batch[17]: last record -> y: 26.387500716897307 | y_pred: 27.635422420008126\n",
            "Training batch[18]: last record -> y: 33.262500568553776 | y_pred: 47.72370781849963\n",
            "Training batch[19]: last record -> y: 29.981600130351694 | y_pred: 29.08692671346671\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.109803009140705\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.54130701701945\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.37601870307617\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.396187994630736\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.482260373456597\n",
            "[Training] Epoch: 858 | L1Loss: 0.05088 | RMSE: 0.08377 | PLCC: 0.94439 | SROCC: 0.94593\n",
            "[Testing]  Epoch: 858 | L1Loss: 0.07603 | RMSE: 0.10394 | PLCC: 0.91551 | SROCC: 0.93117\n",
            "Training batch[0]: last record -> y: 67.35350020768419 | y_pred: 67.83508575470842\n",
            "Training batch[1]: last record -> y: 52.59410091577138 | y_pred: 52.51704967258297\n",
            "Training batch[2]: last record -> y: 58.416900382336735 | y_pred: 59.85577453713017\n",
            "Training batch[3]: last record -> y: 63.56399868712492 | y_pred: 68.4030013710094\n",
            "Training batch[4]: last record -> y: 44.76750116471442 | y_pred: 44.714988155023775\n",
            "Training batch[5]: last record -> y: 40.115501400992116 | y_pred: 35.18337918444354\n",
            "Training batch[6]: last record -> y: 54.441201607450694 | y_pred: 59.01727098444621\n",
            "Training batch[7]: last record -> y: 65.84580192829299 | y_pred: 66.26897107384661\n",
            "Training batch[8]: last record -> y: 57.048500278582424 | y_pred: 55.78520981621227\n",
            "Training batch[9]: last record -> y: 30.420999180982733 | y_pred: 27.254630332527313\n",
            "Training batch[10]: last record -> y: 64.42900076602791 | y_pred: 60.904880104057156\n",
            "Training batch[11]: last record -> y: 58.68100118103507 | y_pred: 57.431186726051465\n",
            "Training batch[12]: last record -> y: 61.02579752021575 | y_pred: 60.15235283525317\n",
            "Training batch[13]: last record -> y: 71.94249883281668 | y_pred: 70.29159144103869\n",
            "Training batch[14]: last record -> y: 25.707799769992192 | y_pred: 24.270072603536278\n",
            "Training batch[15]: last record -> y: 25.808300150496677 | y_pred: 40.364143385966486\n",
            "Training batch[16]: last record -> y: 28.55800066006435 | y_pred: 28.35459013806866\n",
            "Training batch[17]: last record -> y: 48.95560143502075 | y_pred: 44.459760937322926\n",
            "Training batch[18]: last record -> y: 44.16859877200159 | y_pred: 43.77783636879326\n",
            "Training batch[19]: last record -> y: 33.643200189396794 | y_pred: 34.369349540639746\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.09975083337929\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.67065593628308\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.177233274939226\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.13343495052459\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.071705439267248\n",
            "[Training] Epoch: 859 | L1Loss: 0.05057 | RMSE: 0.08394 | PLCC: 0.94411 | SROCC: 0.94474\n",
            "[Testing]  Epoch: 859 | L1Loss: 0.07724 | RMSE: 0.10563 | PLCC: 0.91660 | SROCC: 0.93147\n",
            "Training batch[0]: last record -> y: 26.98159966879109 | y_pred: 25.75590654337745\n",
            "Training batch[1]: last record -> y: 45.47759991575151 | y_pred: 45.93631966933549\n",
            "Training batch[2]: last record -> y: 65.80919800464949 | y_pred: 60.32721930819025\n",
            "Training batch[3]: last record -> y: 30.96270010343983 | y_pred: 26.779248492841447\n",
            "Training batch[4]: last record -> y: 24.91699975574693 | y_pred: 24.95392128218981\n",
            "Training batch[5]: last record -> y: 46.75879995977607 | y_pred: 47.66028696178\n",
            "Training batch[6]: last record -> y: 40.69929876537776 | y_pred: 57.58729292426642\n",
            "Training batch[7]: last record -> y: 65.26629501590105 | y_pred: 65.44959444620508\n",
            "Training batch[8]: last record -> y: 39.43050050762588 | y_pred: 39.5865166513762\n",
            "Training batch[9]: last record -> y: 30.256500228286825 | y_pred: 31.200123875105305\n",
            "Training batch[10]: last record -> y: 55.98460004960816 | y_pred: 57.20814433076521\n",
            "Training batch[11]: last record -> y: 29.01670111626305 | y_pred: 29.55111566766186\n",
            "Training batch[12]: last record -> y: 67.08080242384403 | y_pred: 68.8179723440503\n",
            "Training batch[13]: last record -> y: 25.808300150496677 | y_pred: 41.46448511892481\n",
            "Training batch[14]: last record -> y: 65.29879824517275 | y_pred: 65.36584379409396\n",
            "Training batch[15]: last record -> y: 49.1077999127167 | y_pred: 47.54189107063121\n",
            "Training batch[16]: last record -> y: 48.407398097782334 | y_pred: 54.2468865863832\n",
            "Training batch[17]: last record -> y: 33.262500568553776 | y_pred: 45.48333767164115\n",
            "Training batch[18]: last record -> y: 72.99610069051369 | y_pred: 69.67892197204469\n",
            "Training batch[19]: last record -> y: 54.46919889725973 | y_pred: 55.6655981897909\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.95308121041933\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 47.0201975164199\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.3377458633131\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.73431532522852\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.19734227221315\n",
            "[Training] Epoch: 860 | L1Loss: 0.05378 | RMSE: 0.08660 | PLCC: 0.94037 | SROCC: 0.94014\n",
            "[Testing]  Epoch: 860 | L1Loss: 0.07935 | RMSE: 0.10735 | PLCC: 0.91729 | SROCC: 0.93261\n",
            "Training batch[0]: last record -> y: 53.863000484795975 | y_pred: 56.27971174644995\n",
            "Training batch[1]: last record -> y: 66.92800251097356 | y_pred: 68.16749608106375\n",
            "Training batch[2]: last record -> y: 63.91980066066935 | y_pred: 62.07309556538826\n",
            "Training batch[3]: last record -> y: 60.175200939423576 | y_pred: 58.60797344267735\n",
            "Training batch[4]: last record -> y: 40.03480134387053 | y_pred: 39.16443459185996\n",
            "Training batch[5]: last record -> y: 62.93809764429125 | y_pred: 67.93386585372104\n",
            "Training batch[6]: last record -> y: 69.23919566992163 | y_pred: 69.11616519007498\n",
            "Training batch[7]: last record -> y: 46.59859789153563 | y_pred: 46.933200079235576\n",
            "Training batch[8]: last record -> y: 43.91579820049958 | y_pred: 41.86496214750571\n",
            "Training batch[9]: last record -> y: 53.02539747675837 | y_pred: 50.19640722730992\n",
            "Training batch[10]: last record -> y: 48.559300682237335 | y_pred: 47.67510413744344\n",
            "Training batch[11]: last record -> y: 31.165001025781976 | y_pred: 32.41570878889246\n",
            "Training batch[12]: last record -> y: 62.979599887564746 | y_pred: 66.701193909327\n",
            "Training batch[13]: last record -> y: 50.848597741355434 | y_pred: 52.52571419857327\n",
            "Training batch[14]: last record -> y: 38.08380192173331 | y_pred: 46.07759100261717\n",
            "Training batch[15]: last record -> y: 68.96460030986236 | y_pred: 67.02066533885272\n",
            "Training batch[16]: last record -> y: 65.49870307550941 | y_pred: 64.70818568755271\n",
            "Training batch[17]: last record -> y: 44.75650004698991 | y_pred: 44.38644213007058\n",
            "Training batch[18]: last record -> y: 44.76750116471442 | y_pred: 43.504847516058476\n",
            "Training batch[19]: last record -> y: 65.00969768384539 | y_pred: 64.59808767207392\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 47.95225961728306\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 47.89827196573401\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 39.94209348875904\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 40.3900855042433\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.679024306163768\n",
            "[Training] Epoch: 861 | L1Loss: 0.05681 | RMSE: 0.08841 | PLCC: 0.93847 | SROCC: 0.93777\n",
            "[Testing]  Epoch: 861 | L1Loss: 0.08621 | RMSE: 0.11456 | PLCC: 0.91766 | SROCC: 0.93335\n",
            "Training batch[0]: last record -> y: 70.58750076313868 | y_pred: 67.80927550189767\n",
            "Training batch[1]: last record -> y: 57.412301018325024 | y_pred: 58.844160573569525\n",
            "Training batch[2]: last record -> y: 60.175200939423576 | y_pred: 61.01918494952679\n",
            "Training batch[3]: last record -> y: 38.69430204299886 | y_pred: 41.34757834267339\n",
            "Training batch[4]: last record -> y: 66.92800251097356 | y_pred: 63.878877338954226\n",
            "Training batch[5]: last record -> y: 43.08390078604282 | y_pred: 41.82650567487542\n",
            "Training batch[6]: last record -> y: 27.821399362519628 | y_pred: 28.380137464004974\n",
            "Training batch[7]: last record -> y: 48.33070063999071 | y_pred: 48.98190698738699\n",
            "Training batch[8]: last record -> y: 29.03989978959936 | y_pred: 30.981096944072533\n",
            "Training batch[9]: last record -> y: 41.601200661165194 | y_pred: 47.22871702116822\n",
            "Training batch[10]: last record -> y: 64.4571009752251 | y_pred: 66.04332674777856\n",
            "Training batch[11]: last record -> y: 54.38009965319543 | y_pred: 64.82532081619661\n",
            "Training batch[12]: last record -> y: 35.50039984603882 | y_pred: 39.488312257691064\n",
            "Training batch[13]: last record -> y: 32.25839972032844 | y_pred: 30.157578615115483\n",
            "Training batch[14]: last record -> y: 46.75879995977607 | y_pred: 45.27863100860088\n",
            "Training batch[15]: last record -> y: 56.724497179747004 | y_pred: 61.33635034151166\n",
            "Training batch[16]: last record -> y: 65.81910077952853 | y_pred: 67.1098642860743\n",
            "Training batch[17]: last record -> y: 59.05309979644767 | y_pred: 62.30849471971487\n",
            "Training batch[18]: last record -> y: 65.3453982143908 | y_pred: 67.38397399527071\n",
            "Training batch[19]: last record -> y: 47.787299135455896 | y_pred: 51.231470730215506\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.54420951153497\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.76881821125971\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 34.292980138397866\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 37.68019710862177\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.45499743652121\n",
            "[Training] Epoch: 862 | L1Loss: 0.05973 | RMSE: 0.08881 | PLCC: 0.93790 | SROCC: 0.93618\n",
            "[Testing]  Epoch: 862 | L1Loss: 0.08032 | RMSE: 0.10566 | PLCC: 0.91337 | SROCC: 0.92759\n",
            "Training batch[0]: last record -> y: 52.85669973464883 | y_pred: 51.158185693387395\n",
            "Training batch[1]: last record -> y: 63.38619901162542 | y_pred: 59.946481896634396\n",
            "Training batch[2]: last record -> y: 50.21400001022266 | y_pred: 51.594477060930785\n",
            "Training batch[3]: last record -> y: 64.93270111658194 | y_pred: 66.2081006882147\n",
            "Training batch[4]: last record -> y: 37.457900878899636 | y_pred: 46.524412310061166\n",
            "Training batch[5]: last record -> y: 61.76470083501272 | y_pred: 64.56005895815065\n",
            "Training batch[6]: last record -> y: 43.26480090811049 | y_pred: 50.768282023823986\n",
            "Training batch[7]: last record -> y: 61.27290053871411 | y_pred: 61.781196883195435\n",
            "Training batch[8]: last record -> y: 59.24809987469257 | y_pred: 57.349886841870784\n",
            "Training batch[9]: last record -> y: 38.79510193500403 | y_pred: 40.00756790889545\n",
            "Training batch[10]: last record -> y: 56.6087997063064 | y_pred: 54.657345187499686\n",
            "Training batch[11]: last record -> y: 22.98349944379808 | y_pred: 22.45320448212749\n",
            "Training batch[12]: last record -> y: 25.608800967279734 | y_pred: 26.92168048344189\n",
            "Training batch[13]: last record -> y: 66.95040034282079 | y_pred: 65.17663936389454\n",
            "Training batch[14]: last record -> y: 28.923200460239684 | y_pred: 30.2397420572874\n",
            "Training batch[15]: last record -> y: 38.541598617054774 | y_pred: 39.33464235436759\n",
            "Training batch[16]: last record -> y: 25.27400017732296 | y_pred: 45.99589713015371\n",
            "Training batch[17]: last record -> y: 34.10250047265458 | y_pred: 33.60608810127428\n",
            "Training batch[18]: last record -> y: 67.50669893318377 | y_pred: 66.87050916776548\n",
            "Training batch[19]: last record -> y: 39.02769814411886 | y_pred: 40.060072878008896\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.18781284298484\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.92547147643131\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.658913700774406\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.591123902111576\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.73435297213618\n",
            "[Training] Epoch: 863 | L1Loss: 0.05735 | RMSE: 0.08674 | PLCC: 0.93902 | SROCC: 0.93702\n",
            "[Testing]  Epoch: 863 | L1Loss: 0.07858 | RMSE: 0.10667 | PLCC: 0.91684 | SROCC: 0.93237\n",
            "Training batch[0]: last record -> y: 26.15419975119093 | y_pred: 45.44664047730214\n",
            "Training batch[1]: last record -> y: 39.43050050762588 | y_pred: 41.70616557215976\n",
            "Training batch[2]: last record -> y: 54.23470028757947 | y_pred: 54.262771550698744\n",
            "Training batch[3]: last record -> y: 62.07709977283366 | y_pred: 58.435377628741435\n",
            "Training batch[4]: last record -> y: 56.6087997063064 | y_pred: 56.74963524030386\n",
            "Training batch[5]: last record -> y: 64.97390103415273 | y_pred: 63.333140850800646\n",
            "Training batch[6]: last record -> y: 68.8820042846371 | y_pred: 65.88669308769931\n",
            "Training batch[7]: last record -> y: 47.44130023363323 | y_pred: 48.87679091354107\n",
            "Training batch[8]: last record -> y: 65.49870307550941 | y_pred: 67.55093818893602\n",
            "Training batch[9]: last record -> y: 63.12649801677071 | y_pred: 62.87866169763174\n",
            "Training batch[10]: last record -> y: 72.54129669802592 | y_pred: 66.68095095216927\n",
            "Training batch[11]: last record -> y: 58.416900382336735 | y_pred: 59.09044023696265\n",
            "Training batch[12]: last record -> y: 66.13350021295673 | y_pred: 65.99441752478879\n",
            "Training batch[13]: last record -> y: 32.322799919350985 | y_pred: 29.558218713559995\n",
            "Training batch[14]: last record -> y: 22.563999213620633 | y_pred: 24.699829393989546\n",
            "Training batch[15]: last record -> y: 47.35749812182803 | y_pred: 48.217473231865824\n",
            "Training batch[16]: last record -> y: 28.415199204941587 | y_pred: 29.014387842202723\n",
            "Training batch[17]: last record -> y: 57.24819927014278 | y_pred: 53.27837976530509\n",
            "Training batch[18]: last record -> y: 69.45530065520006 | y_pred: 65.71661236631144\n",
            "Training batch[19]: last record -> y: 46.40150083075707 | y_pred: 42.44755020908053\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.575690133240414\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.76964976079569\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.453746962864784\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.07117676126802\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.304937504185034\n",
            "[Training] Epoch: 864 | L1Loss: 0.05075 | RMSE: 0.08452 | PLCC: 0.94370 | SROCC: 0.94480\n",
            "[Testing]  Epoch: 864 | L1Loss: 0.07605 | RMSE: 0.10340 | PLCC: 0.91695 | SROCC: 0.93263\n",
            "Training batch[0]: last record -> y: 22.98349944379808 | y_pred: 21.86702007138571\n",
            "Training batch[1]: last record -> y: 61.02579752021575 | y_pred: 59.76873046459809\n",
            "Training batch[2]: last record -> y: 38.3894998425161 | y_pred: 38.454339057053744\n",
            "Training batch[3]: last record -> y: 56.6087997063064 | y_pred: 58.3404119795523\n",
            "Training batch[4]: last record -> y: 58.416900382336735 | y_pred: 60.04720781532933\n",
            "Training batch[5]: last record -> y: 54.441799826394345 | y_pred: 55.402668099132825\n",
            "Training batch[6]: last record -> y: 68.13459725539383 | y_pred: 69.50518439614643\n",
            "Training batch[7]: last record -> y: 64.08940216365613 | y_pred: 59.990447772761854\n",
            "Training batch[8]: last record -> y: 25.033300272477504 | y_pred: 23.377891162539882\n",
            "Training batch[9]: last record -> y: 69.90390053832061 | y_pred: 65.64793618833414\n",
            "Training batch[10]: last record -> y: 65.01059822849174 | y_pred: 65.9562794590156\n",
            "Training batch[11]: last record -> y: 65.29879824517275 | y_pred: 65.76323163291477\n",
            "Training batch[12]: last record -> y: 52.87200256117512 | y_pred: 53.70421516625811\n",
            "Training batch[13]: last record -> y: 33.06290047609298 | y_pred: 33.07203376773401\n",
            "Training batch[14]: last record -> y: 61.4574993262936 | y_pred: 62.89108921375146\n",
            "Training batch[15]: last record -> y: 53.863000484795975 | y_pred: 55.424818281202306\n",
            "Training batch[16]: last record -> y: 60.984999631504934 | y_pred: 61.71826167733866\n",
            "Training batch[17]: last record -> y: 61.18830080165071 | y_pred: 61.98432116064282\n",
            "Training batch[18]: last record -> y: 36.81190160929782 | y_pred: 33.69401663729832\n",
            "Training batch[19]: last record -> y: 51.13240117042369 | y_pred: 54.74396793378651\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.34872393628075\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.59788212631133\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.735697996799786\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.18466146786329\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.26229430300691\n",
            "[Training] Epoch: 865 | L1Loss: 0.04979 | RMSE: 0.08296 | PLCC: 0.94536 | SROCC: 0.94648\n",
            "[Testing]  Epoch: 865 | L1Loss: 0.07631 | RMSE: 0.10348 | PLCC: 0.91645 | SROCC: 0.93351\n",
            "Training batch[0]: last record -> y: 33.64979989516223 | y_pred: 36.85541399687122\n",
            "Training batch[1]: last record -> y: 66.4921035235975 | y_pred: 65.62788298879832\n",
            "Training batch[2]: last record -> y: 40.42119932177491 | y_pred: 40.9412204359287\n",
            "Training batch[3]: last record -> y: 60.00719790318408 | y_pred: 61.7799103908435\n",
            "Training batch[4]: last record -> y: 35.50039984603882 | y_pred: 40.502531368264385\n",
            "Training batch[5]: last record -> y: 30.080500033964597 | y_pred: 29.410625074426264\n",
            "Training batch[6]: last record -> y: 48.23669664383465 | y_pred: 45.88545336985533\n",
            "Training batch[7]: last record -> y: 61.434799168743666 | y_pred: 61.23179710806971\n",
            "Training batch[8]: last record -> y: 28.995599425460398 | y_pred: 29.669444822019898\n",
            "Training batch[9]: last record -> y: 65.45760286109589 | y_pred: 65.76028556542883\n",
            "Training batch[10]: last record -> y: 65.29879824517275 | y_pred: 65.80147905053786\n",
            "Training batch[11]: last record -> y: 63.56399868712492 | y_pred: 66.23325161369507\n",
            "Training batch[12]: last record -> y: 63.890597284280375 | y_pred: 60.07806433439055\n",
            "Training batch[13]: last record -> y: 25.033300272477504 | y_pred: 23.913524062398864\n",
            "Training batch[14]: last record -> y: 61.02579752021575 | y_pred: 59.56102627437781\n",
            "Training batch[15]: last record -> y: 32.14319995861649 | y_pred: 31.21417317564618\n",
            "Training batch[16]: last record -> y: 61.4574993262936 | y_pred: 65.366030335485\n",
            "Training batch[17]: last record -> y: 56.6087997063064 | y_pred: 57.21755823855051\n",
            "Training batch[18]: last record -> y: 66.13350021295673 | y_pred: 64.29650813493276\n",
            "Training batch[19]: last record -> y: 45.92639920518661 | y_pred: 44.24530105413953\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.99579888125834\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.99824321672702\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.42209909715359\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.84778862116218\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.566496676430575\n",
            "[Training] Epoch: 866 | L1Loss: 0.05367 | RMSE: 0.08602 | PLCC: 0.94140 | SROCC: 0.94269\n",
            "[Testing]  Epoch: 866 | L1Loss: 0.07694 | RMSE: 0.10362 | PLCC: 0.91417 | SROCC: 0.92869\n",
            "Training batch[0]: last record -> y: 47.01879845598717 | y_pred: 41.48370209843188\n",
            "Training batch[1]: last record -> y: 48.23669664383465 | y_pred: 46.824203622833124\n",
            "Training batch[2]: last record -> y: 40.39899768001135 | y_pred: 60.977373948088825\n",
            "Training batch[3]: last record -> y: 53.02539747675837 | y_pred: 54.399705796638955\n",
            "Training batch[4]: last record -> y: 66.22540401034826 | y_pred: 66.25070931491086\n",
            "Training batch[5]: last record -> y: 43.48559998235805 | y_pred: 40.70987855685701\n",
            "Training batch[6]: last record -> y: 38.86930038140201 | y_pred: 43.44606607238302\n",
            "Training batch[7]: last record -> y: 70.7566005341082 | y_pred: 64.69561665727429\n",
            "Training batch[8]: last record -> y: 65.26629501590105 | y_pred: 64.57043130273814\n",
            "Training batch[9]: last record -> y: 67.19999594025103 | y_pred: 64.76696873934361\n",
            "Training batch[10]: last record -> y: 29.976800709821248 | y_pred: 31.41724679745721\n",
            "Training batch[11]: last record -> y: 49.50770284408554 | y_pred: 48.88478646350836\n",
            "Training batch[12]: last record -> y: 51.931000946581435 | y_pred: 59.63984644455013\n",
            "Training batch[13]: last record -> y: 63.91980066066935 | y_pred: 62.11327593777014\n",
            "Training batch[14]: last record -> y: 28.923200460239684 | y_pred: 27.523392253828263\n",
            "Training batch[15]: last record -> y: 61.12310136925453 | y_pred: 62.90933489153281\n",
            "Training batch[16]: last record -> y: 58.416900382336735 | y_pred: 61.304258789792584\n",
            "Training batch[17]: last record -> y: 27.960300333642863 | y_pred: 27.944828654995376\n",
            "Training batch[18]: last record -> y: 68.25509656153804 | y_pred: 69.51012452677787\n",
            "Training batch[19]: last record -> y: 61.54970223315695 | y_pred: 64.78009739379513\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.03667556762571\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.13779049640357\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.74034514271409\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.750508893855\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.8732204210105\n",
            "[Training] Epoch: 867 | L1Loss: 0.05259 | RMSE: 0.08567 | PLCC: 0.94218 | SROCC: 0.94263\n",
            "[Testing]  Epoch: 867 | L1Loss: 0.07670 | RMSE: 0.10241 | PLCC: 0.91605 | SROCC: 0.92980\n",
            "Training batch[0]: last record -> y: 62.93809764429125 | y_pred: 64.62908248906297\n",
            "Training batch[1]: last record -> y: 65.49870307550941 | y_pred: 63.9931435896533\n",
            "Training batch[2]: last record -> y: 61.12310136925453 | y_pred: 61.39113561831891\n",
            "Training batch[3]: last record -> y: 62.94929977644574 | y_pred: 61.48558023810551\n",
            "Training batch[4]: last record -> y: 37.76599810791879 | y_pred: 38.09904363987289\n",
            "Training batch[5]: last record -> y: 49.146899631522956 | y_pred: 54.579557427439795\n",
            "Training batch[6]: last record -> y: 67.77610008037209 | y_pred: 64.37681742000245\n",
            "Training batch[7]: last record -> y: 58.70890198391771 | y_pred: 56.230892577924806\n",
            "Training batch[8]: last record -> y: 53.191200611076056 | y_pred: 58.03983269021478\n",
            "Training batch[9]: last record -> y: 67.38690076537137 | y_pred: 69.49594738105952\n",
            "Training batch[10]: last record -> y: 28.817400133274077 | y_pred: 33.94692173630392\n",
            "Training batch[11]: last record -> y: 71.1275027116335 | y_pred: 66.57126783047397\n",
            "Training batch[12]: last record -> y: 38.89670106038284 | y_pred: 38.43678165268068\n",
            "Training batch[13]: last record -> y: 59.78609811134925 | y_pred: 59.08256047130703\n",
            "Training batch[14]: last record -> y: 45.30530160317198 | y_pred: 44.090404158735396\n",
            "Training batch[15]: last record -> y: 46.69590013245897 | y_pred: 55.452159459911854\n",
            "Training batch[16]: last record -> y: 34.274600987035 | y_pred: 35.20368003375711\n",
            "Training batch[17]: last record -> y: 60.46889749467823 | y_pred: 62.159994907530745\n",
            "Training batch[18]: last record -> y: 44.28150134080761 | y_pred: 44.08935245123769\n",
            "Training batch[19]: last record -> y: 35.84120131875966 | y_pred: 49.81415175477878\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.85058605703341\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.60210809983391\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 34.25408304213704\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.219337261094324\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.617052609630832\n",
            "[Training] Epoch: 868 | L1Loss: 0.05419 | RMSE: 0.08758 | PLCC: 0.93974 | SROCC: 0.94030\n",
            "[Testing]  Epoch: 868 | L1Loss: 0.07812 | RMSE: 0.10428 | PLCC: 0.91367 | SROCC: 0.92943\n",
            "Training batch[0]: last record -> y: 26.15419975119093 | y_pred: 43.39577547823035\n",
            "Training batch[1]: last record -> y: 68.15290082533102 | y_pred: 62.13602755501415\n",
            "Training batch[2]: last record -> y: 28.118200384631052 | y_pred: 28.33132955228791\n",
            "Training batch[3]: last record -> y: 47.28070096087913 | y_pred: 48.21399970251559\n",
            "Training batch[4]: last record -> y: 56.38050198976248 | y_pred: 60.45666616864219\n",
            "Training batch[5]: last record -> y: 63.512198072574165 | y_pred: 61.507325175084134\n",
            "Training batch[6]: last record -> y: 51.224099129038905 | y_pred: 60.29580316495594\n",
            "Training batch[7]: last record -> y: 40.03480134387053 | y_pred: 38.963293120749995\n",
            "Training batch[8]: last record -> y: 26.202400197300562 | y_pred: 26.561272063219803\n",
            "Training batch[9]: last record -> y: 25.520900573275895 | y_pred: 24.920520724502637\n",
            "Training batch[10]: last record -> y: 63.439498389766186 | y_pred: 67.06098722539332\n",
            "Training batch[11]: last record -> y: 62.24660157266317 | y_pred: 67.36129313510605\n",
            "Training batch[12]: last record -> y: 39.32759880874073 | y_pred: 56.51047309584783\n",
            "Training batch[13]: last record -> y: 47.55979904417018 | y_pred: 46.867492482360376\n",
            "Training batch[14]: last record -> y: 37.457900878899636 | y_pred: 43.827404919113405\n",
            "Training batch[15]: last record -> y: 73.29170125444921 | y_pred: 69.39032635896547\n",
            "Training batch[16]: last record -> y: 61.28480059296953 | y_pred: 60.836628468556\n",
            "Training batch[17]: last record -> y: 49.4674999580875 | y_pred: 58.34087190056812\n",
            "Training batch[18]: last record -> y: 48.6593994359107 | y_pred: 48.30056777287746\n",
            "Training batch[19]: last record -> y: 55.782900562440545 | y_pred: 52.23438800547706\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.743347270806794\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.4628292215823\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.19975301585282\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.80901856602111\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.015824881991875\n",
            "[Training] Epoch: 869 | L1Loss: 0.05326 | RMSE: 0.08570 | PLCC: 0.94216 | SROCC: 0.94254\n",
            "[Testing]  Epoch: 869 | L1Loss: 0.07706 | RMSE: 0.10262 | PLCC: 0.91588 | SROCC: 0.92932\n",
            "Training batch[0]: last record -> y: 64.29779784351558 | y_pred: 61.1323287356488\n",
            "Training batch[1]: last record -> y: 65.59839658409192 | y_pred: 61.66773147398544\n",
            "Training batch[2]: last record -> y: 52.915900896454104 | y_pred: 49.20694987828051\n",
            "Training batch[3]: last record -> y: 63.3563009293664 | y_pred: 63.91694786387893\n",
            "Training batch[4]: last record -> y: 69.7550951842029 | y_pred: 70.00507029687697\n",
            "Training batch[5]: last record -> y: 44.53200069911509 | y_pred: 43.83535865807926\n",
            "Training batch[6]: last record -> y: 49.544400038424556 | y_pred: 52.898211626614966\n",
            "Training batch[7]: last record -> y: 48.045601069877875 | y_pred: 45.05083663218966\n",
            "Training batch[8]: last record -> y: 53.70280002467098 | y_pred: 52.665803567006606\n",
            "Training batch[9]: last record -> y: 61.4574993262936 | y_pred: 63.31504633587065\n",
            "Training batch[10]: last record -> y: 40.68579863625962 | y_pred: 46.714123296624166\n",
            "Training batch[11]: last record -> y: 38.542598864858405 | y_pred: 39.12639783735949\n",
            "Training batch[12]: last record -> y: 30.256500228286825 | y_pred: 32.67609484092458\n",
            "Training batch[13]: last record -> y: 54.23470028757947 | y_pred: 53.378883764069315\n",
            "Training batch[14]: last record -> y: 31.007400888323332 | y_pred: 29.39472483301404\n",
            "Training batch[15]: last record -> y: 28.917199777475616 | y_pred: 33.103612330627755\n",
            "Training batch[16]: last record -> y: 56.6087997063064 | y_pred: 56.280789183794695\n",
            "Training batch[17]: last record -> y: 58.169900283226525 | y_pred: 56.385934203718534\n",
            "Training batch[18]: last record -> y: 54.38009965319543 | y_pred: 65.28399071820195\n",
            "Training batch[19]: last record -> y: 63.37459806684183 | y_pred: 62.214828427801194\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.43679237834806\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.10255538670651\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.77514813091841\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.113595630342274\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.209630532493776\n",
            "[Training] Epoch: 870 | L1Loss: 0.05051 | RMSE: 0.08411 | PLCC: 0.94392 | SROCC: 0.94464\n",
            "[Testing]  Epoch: 870 | L1Loss: 0.07642 | RMSE: 0.10423 | PLCC: 0.91684 | SROCC: 0.93219\n",
            "Training batch[0]: last record -> y: 62.27109960327493 | y_pred: 57.693373867376295\n",
            "Training batch[1]: last record -> y: 68.83770070426726 | y_pred: 67.93346060863018\n",
            "Training batch[2]: last record -> y: 45.98249831230828 | y_pred: 46.64726268097854\n",
            "Training batch[3]: last record -> y: 57.104697480745926 | y_pred: 54.65600723545367\n",
            "Training batch[4]: last record -> y: 56.724497179747004 | y_pred: 61.16063156739142\n",
            "Training batch[5]: last record -> y: 29.03989978959936 | y_pred: 28.29996728092084\n",
            "Training batch[6]: last record -> y: 56.15299868224588 | y_pred: 57.28448478692917\n",
            "Training batch[7]: last record -> y: 68.93579896233337 | y_pred: 65.72519648652974\n",
            "Training batch[8]: last record -> y: 38.59769772417644 | y_pred: 38.219623351789096\n",
            "Training batch[9]: last record -> y: 29.49510018254307 | y_pred: 46.77230008889421\n",
            "Training batch[10]: last record -> y: 60.175200939423576 | y_pred: 61.47169576939723\n",
            "Training batch[11]: last record -> y: 39.67380194122427 | y_pred: 56.71816442114459\n",
            "Training batch[12]: last record -> y: 39.45899952945217 | y_pred: 39.98301037801241\n",
            "Training batch[13]: last record -> y: 65.00969768384539 | y_pred: 64.28511946138724\n",
            "Training batch[14]: last record -> y: 65.06269795251433 | y_pred: 65.01124147466771\n",
            "Training batch[15]: last record -> y: 48.79289874727124 | y_pred: 54.59653591025449\n",
            "Training batch[16]: last record -> y: 33.63470129929681 | y_pred: 34.53516875611183\n",
            "Training batch[17]: last record -> y: 68.3981030513794 | y_pred: 64.2086953832204\n",
            "Training batch[18]: last record -> y: 26.387500716897307 | y_pred: 27.30233025270627\n",
            "Training batch[19]: last record -> y: 29.528200022642977 | y_pred: 33.33524849482495\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 43.64103077208824\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 43.25692114245487\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 33.007947955279974\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 37.62849941345917\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.554729539874273\n",
            "[Training] Epoch: 871 | L1Loss: 0.05079 | RMSE: 0.08296 | PLCC: 0.94503 | SROCC: 0.94539\n",
            "[Testing]  Epoch: 871 | L1Loss: 0.08465 | RMSE: 0.10897 | PLCC: 0.91342 | SROCC: 0.92581\n",
            "Training batch[0]: last record -> y: 65.26629501590105 | y_pred: 62.117061441515716\n",
            "Training batch[1]: last record -> y: 38.51679986885574 | y_pred: 37.67521677510433\n",
            "Training batch[2]: last record -> y: 47.28070096087913 | y_pred: 48.79266074618613\n",
            "Training batch[3]: last record -> y: 29.695998828221605 | y_pred: 50.64888910110244\n",
            "Training batch[4]: last record -> y: 64.4571009752251 | y_pred: 68.02142773942478\n",
            "Training batch[5]: last record -> y: 47.25970218946463 | y_pred: 50.627237434819335\n",
            "Training batch[6]: last record -> y: 62.94929977644574 | y_pred: 59.35405860102901\n",
            "Training batch[7]: last record -> y: 56.96869915799175 | y_pred: 54.978280002075735\n",
            "Training batch[8]: last record -> y: 35.46180025113438 | y_pred: 47.89688255399392\n",
            "Training batch[9]: last record -> y: 55.55740096676209 | y_pred: 55.9986517623222\n",
            "Training batch[10]: last record -> y: 42.995300057764894 | y_pred: 49.456529394556355\n",
            "Training batch[11]: last record -> y: 57.606300848766296 | y_pred: 65.43643041321388\n",
            "Training batch[12]: last record -> y: 67.19999594025103 | y_pred: 64.0265087688008\n",
            "Training batch[13]: last record -> y: 68.86179992224993 | y_pred: 68.9950065566004\n",
            "Training batch[14]: last record -> y: 22.98349944379808 | y_pred: 22.32179974112745\n",
            "Training batch[15]: last record -> y: 54.38009965319543 | y_pred: 61.92749679345775\n",
            "Training batch[16]: last record -> y: 41.72359915164395 | y_pred: 43.867041748476595\n",
            "Training batch[17]: last record -> y: 55.65470159956999 | y_pred: 56.655724514843314\n",
            "Training batch[18]: last record -> y: 48.33070063999071 | y_pred: 50.09048066328228\n",
            "Training batch[19]: last record -> y: 57.104697480745926 | y_pred: 55.311944658474204\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.70518684527099\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.498177514774284\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.26486430960688\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.53404851506434\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.354007538718804\n",
            "[Training] Epoch: 872 | L1Loss: 0.05658 | RMSE: 0.08649 | PLCC: 0.94036 | SROCC: 0.94052\n",
            "[Testing]  Epoch: 872 | L1Loss: 0.07790 | RMSE: 0.10598 | PLCC: 0.91587 | SROCC: 0.93071\n",
            "Training batch[0]: last record -> y: 59.352698135366836 | y_pred: 59.944008615087796\n",
            "Training batch[1]: last record -> y: 47.35749812182803 | y_pred: 46.77752485595852\n",
            "Training batch[2]: last record -> y: 48.559300682237335 | y_pred: 48.467766751396994\n",
            "Training batch[3]: last record -> y: 68.87090185563989 | y_pred: 66.68889825867336\n",
            "Training batch[4]: last record -> y: 65.49870307550941 | y_pred: 67.70268961053966\n",
            "Training batch[5]: last record -> y: 63.97720073318192 | y_pred: 62.12766535472656\n",
            "Training batch[6]: last record -> y: 30.471100017513493 | y_pred: 53.23119122583603\n",
            "Training batch[7]: last record -> y: 33.64979989516223 | y_pred: 37.13430945761684\n",
            "Training batch[8]: last record -> y: 67.78130394193568 | y_pred: 66.38961511038042\n",
            "Training batch[9]: last record -> y: 40.48429855540655 | y_pred: 38.960216795913425\n",
            "Training batch[10]: last record -> y: 40.334599089104245 | y_pred: 39.2794164539298\n",
            "Training batch[11]: last record -> y: 55.66210214682451 | y_pred: 56.217358678382425\n",
            "Training batch[12]: last record -> y: 64.08940216365613 | y_pred: 61.24742799014575\n",
            "Training batch[13]: last record -> y: 56.444000036023226 | y_pred: 58.03098805529521\n",
            "Training batch[14]: last record -> y: 29.49510018254307 | y_pred: 47.21482933622906\n",
            "Training batch[15]: last record -> y: 45.92639920518661 | y_pred: 45.66094919763418\n",
            "Training batch[16]: last record -> y: 65.36119955670347 | y_pred: 63.305024560449056\n",
            "Training batch[17]: last record -> y: 33.63470129929681 | y_pred: 34.268168525275314\n",
            "Training batch[18]: last record -> y: 52.21930066641971 | y_pred: 51.146070151663025\n",
            "Training batch[19]: last record -> y: 48.248902240023654 | y_pred: 48.979797139929815\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.05427639111565\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.22611317456392\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.22885362670718\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.1086794675889\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.68494186327561\n",
            "[Training] Epoch: 873 | L1Loss: 0.05287 | RMSE: 0.08548 | PLCC: 0.94229 | SROCC: 0.94302\n",
            "[Testing]  Epoch: 873 | L1Loss: 0.07624 | RMSE: 0.10327 | PLCC: 0.91609 | SROCC: 0.93049\n",
            "Training batch[0]: last record -> y: 29.17069987919399 | y_pred: 42.22971649970418\n",
            "Training batch[1]: last record -> y: 63.56399868712492 | y_pred: 66.15911749191469\n",
            "Training batch[2]: last record -> y: 38.469501977536765 | y_pred: 37.9542312363925\n",
            "Training batch[3]: last record -> y: 55.8553011357767 | y_pred: 50.63959741009057\n",
            "Training batch[4]: last record -> y: 61.766399004917275 | y_pred: 59.66935214664181\n",
            "Training batch[5]: last record -> y: 52.85669973464883 | y_pred: 53.46318117543001\n",
            "Training batch[6]: last record -> y: 71.94249883281668 | y_pred: 70.81031802226335\n",
            "Training batch[7]: last record -> y: 21.73069952250026 | y_pred: 22.378178057292374\n",
            "Training batch[8]: last record -> y: 34.149799972089 | y_pred: 32.95383406666889\n",
            "Training batch[9]: last record -> y: 58.14030131043933 | y_pred: 62.91791901175111\n",
            "Training batch[10]: last record -> y: 57.655602451923414 | y_pred: 57.05391962761496\n",
            "Training batch[11]: last record -> y: 42.00540208510495 | y_pred: 42.64488366282876\n",
            "Training batch[12]: last record -> y: 37.98649968080997 | y_pred: 39.51308527604306\n",
            "Training batch[13]: last record -> y: 21.597000101274666 | y_pred: 15.66567445156582\n",
            "Training batch[14]: last record -> y: 49.50770284408554 | y_pred: 47.2296143495837\n",
            "Training batch[15]: last record -> y: 61.25039978747873 | y_pred: 51.86911101576061\n",
            "Training batch[16]: last record -> y: 29.976800709821248 | y_pred: 30.663624402038636\n",
            "Training batch[17]: last record -> y: 64.53830115624851 | y_pred: 61.25773279388477\n",
            "Training batch[18]: last record -> y: 48.23669664383465 | y_pred: 46.8464583324062\n",
            "Training batch[19]: last record -> y: 59.352698135366836 | y_pred: 58.57810430649624\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.0576648442011\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.02167200442477\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.058187705152704\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.28437266571564\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.80925175280825\n",
            "[Training] Epoch: 874 | L1Loss: 0.04960 | RMSE: 0.08263 | PLCC: 0.94599 | SROCC: 0.94736\n",
            "[Testing]  Epoch: 874 | L1Loss: 0.07606 | RMSE: 0.10379 | PLCC: 0.91713 | SROCC: 0.93189\n",
            "Training batch[0]: last record -> y: 38.86930038140201 | y_pred: 45.303642028037984\n",
            "Training batch[1]: last record -> y: 63.3563009293664 | y_pred: 62.39143488164427\n",
            "Training batch[2]: last record -> y: 39.64730019877436 | y_pred: 40.46060297439931\n",
            "Training batch[3]: last record -> y: 67.37490100795867 | y_pred: 64.30304029984973\n",
            "Training batch[4]: last record -> y: 59.229098382654456 | y_pred: 60.702273639781424\n",
            "Training batch[5]: last record -> y: 68.83770070426726 | y_pred: 67.33253038234761\n",
            "Training batch[6]: last record -> y: 65.59839658409192 | y_pred: 62.39275032007413\n",
            "Training batch[7]: last record -> y: 37.30360059432371 | y_pred: 37.3243935269619\n",
            "Training batch[8]: last record -> y: 51.65370073635222 | y_pred: 51.31944429347186\n",
            "Training batch[9]: last record -> y: 51.062699014795726 | y_pred: 50.93983899518571\n",
            "Training batch[10]: last record -> y: 61.434799168743666 | y_pred: 62.031853836816026\n",
            "Training batch[11]: last record -> y: 66.65419834371073 | y_pred: 64.93111551475818\n",
            "Training batch[12]: last record -> y: 66.99210199240883 | y_pred: 67.09638827868775\n",
            "Training batch[13]: last record -> y: 36.45529879426431 | y_pred: 37.402501301994334\n",
            "Training batch[14]: last record -> y: 58.70890198391771 | y_pred: 57.82240905027561\n",
            "Training batch[15]: last record -> y: 27.821399362519628 | y_pred: 28.98239358146776\n",
            "Training batch[16]: last record -> y: 65.65200150416626 | y_pred: 65.52632728253639\n",
            "Training batch[17]: last record -> y: 25.665500705518212 | y_pred: 26.01496791031593\n",
            "Training batch[18]: last record -> y: 49.315497670475224 | y_pred: 48.47639911507849\n",
            "Training batch[19]: last record -> y: 58.359600012981446 | y_pred: 58.95611113803511\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.75947682252274\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.69347172429116\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.55574153078828\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.16680977836472\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.65366939819785\n",
            "[Training] Epoch: 875 | L1Loss: 0.04959 | RMSE: 0.08295 | PLCC: 0.94535 | SROCC: 0.94658\n",
            "[Testing]  Epoch: 875 | L1Loss: 0.07613 | RMSE: 0.10285 | PLCC: 0.91631 | SROCC: 0.93065\n",
            "Training batch[0]: last record -> y: 38.89670106038284 | y_pred: 38.00504446806315\n",
            "Training batch[1]: last record -> y: 45.65990070636735 | y_pred: 43.53541296622507\n",
            "Training batch[2]: last record -> y: 28.55800066006435 | y_pred: 28.434293163617497\n",
            "Training batch[3]: last record -> y: 63.439498389766186 | y_pred: 67.36615607619638\n",
            "Training batch[4]: last record -> y: 72.54129669802592 | y_pred: 68.24989269997445\n",
            "Training batch[5]: last record -> y: 53.03879951083468 | y_pred: 54.322712445606385\n",
            "Training batch[6]: last record -> y: 51.99429958652763 | y_pred: 53.09877256805112\n",
            "Training batch[7]: last record -> y: 48.95560143502075 | y_pred: 44.66441774878456\n",
            "Training batch[8]: last record -> y: 67.38510289230953 | y_pred: 63.82854654191556\n",
            "Training batch[9]: last record -> y: 43.48930025598531 | y_pred: 42.549747553402995\n",
            "Training batch[10]: last record -> y: 37.4706001665287 | y_pred: 36.592718691067375\n",
            "Training batch[11]: last record -> y: 64.75720104616153 | y_pred: 64.80368201483702\n",
            "Training batch[12]: last record -> y: 48.261201106908175 | y_pred: 49.51213481023797\n",
            "Training batch[13]: last record -> y: 24.490699609431772 | y_pred: 24.584658176326656\n",
            "Training batch[14]: last record -> y: 59.08630094782029 | y_pred: 58.88367518615928\n",
            "Training batch[15]: last record -> y: 61.02579752021575 | y_pred: 58.52298775790837\n",
            "Training batch[16]: last record -> y: 36.83800132288775 | y_pred: 34.82513609166131\n",
            "Training batch[17]: last record -> y: 68.87090185563989 | y_pred: 66.62880298468349\n",
            "Training batch[18]: last record -> y: 25.608800967279734 | y_pred: 25.84496960484435\n",
            "Training batch[19]: last record -> y: 21.597000101274666 | y_pred: 15.147857661651301\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.16045879935177\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.37841150927068\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.15515224183366\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.424626061923846\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.37301144293008\n",
            "[Training] Epoch: 876 | L1Loss: 0.05035 | RMSE: 0.08367 | PLCC: 0.94458 | SROCC: 0.94633\n",
            "[Testing]  Epoch: 876 | L1Loss: 0.07777 | RMSE: 0.10546 | PLCC: 0.91654 | SROCC: 0.93215\n",
            "Training batch[0]: last record -> y: 64.74600213023791 | y_pred: 63.25708663918499\n",
            "Training batch[1]: last record -> y: 55.98460004960816 | y_pred: 58.30470538432428\n",
            "Training batch[2]: last record -> y: 34.048697754381124 | y_pred: 34.73673476969793\n",
            "Training batch[3]: last record -> y: 51.65370073635222 | y_pred: 51.72165969484331\n",
            "Training batch[4]: last record -> y: 25.83860026161568 | y_pred: 25.501788523301286\n",
            "Training batch[5]: last record -> y: 45.05469932547635 | y_pred: 43.0653543906742\n",
            "Training batch[6]: last record -> y: 43.91579820049958 | y_pred: 43.86717361394267\n",
            "Training batch[7]: last record -> y: 33.63470129929681 | y_pred: 33.55809228785438\n",
            "Training batch[8]: last record -> y: 29.695998828221605 | y_pred: 48.283139017739586\n",
            "Training batch[9]: last record -> y: 73.66169645486639 | y_pred: 72.92241040859471\n",
            "Training batch[10]: last record -> y: 59.44990228124834 | y_pred: 60.22583406216495\n",
            "Training batch[11]: last record -> y: 69.7550951842029 | y_pred: 68.0913775448305\n",
            "Training batch[12]: last record -> y: 63.02299970705735 | y_pred: 59.64932789318391\n",
            "Training batch[13]: last record -> y: 65.2375033170647 | y_pred: 65.91045781767048\n",
            "Training batch[14]: last record -> y: 31.658599999200135 | y_pred: 31.968374492623127\n",
            "Training batch[15]: last record -> y: 71.1275027116335 | y_pred: 64.64593232264247\n",
            "Training batch[16]: last record -> y: 63.38790039776086 | y_pred: 63.64241682843726\n",
            "Training batch[17]: last record -> y: 33.64979989516223 | y_pred: 37.03276822439386\n",
            "Training batch[18]: last record -> y: 34.8158990765744 | y_pred: 33.74899167172748\n",
            "Training batch[19]: last record -> y: 68.3981030513794 | y_pred: 64.90531169440919\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.07752989423045\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.51198801017233\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.598783048244854\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.49809120768123\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.92034921203296\n",
            "[Training] Epoch: 877 | L1Loss: 0.04897 | RMSE: 0.08271 | PLCC: 0.94609 | SROCC: 0.94741\n",
            "[Testing]  Epoch: 877 | L1Loss: 0.07756 | RMSE: 0.10526 | PLCC: 0.91652 | SROCC: 0.93222\n",
            "Training batch[0]: last record -> y: 32.75839979725521 | y_pred: 33.021475422360595\n",
            "Training batch[1]: last record -> y: 49.328201782450606 | y_pred: 49.8305352348807\n",
            "Training batch[2]: last record -> y: 48.79289874727124 | y_pred: 52.54533642317119\n",
            "Training batch[3]: last record -> y: 44.28150134080761 | y_pred: 39.39965042102642\n",
            "Training batch[4]: last record -> y: 67.85620031043459 | y_pred: 69.10041209122551\n",
            "Training batch[5]: last record -> y: 36.67800187719274 | y_pred: 38.68168798548811\n",
            "Training batch[6]: last record -> y: 69.51529622603266 | y_pred: 68.10885454343156\n",
            "Training batch[7]: last record -> y: 52.48210210784259 | y_pred: 53.706328229946166\n",
            "Training batch[8]: last record -> y: 33.643200189396794 | y_pred: 35.144240870866724\n",
            "Training batch[9]: last record -> y: 27.770800010202493 | y_pred: 26.8968552013381\n",
            "Training batch[10]: last record -> y: 64.4866034610859 | y_pred: 60.74635530022056\n",
            "Training batch[11]: last record -> y: 43.26480090811049 | y_pred: 49.268852674024856\n",
            "Training batch[12]: last record -> y: 48.559300682237335 | y_pred: 49.22016215473491\n",
            "Training batch[13]: last record -> y: 40.324099703396996 | y_pred: 39.94853720732681\n",
            "Training batch[14]: last record -> y: 62.891102078674976 | y_pred: 62.96643263834267\n",
            "Training batch[15]: last record -> y: 31.38129978897092 | y_pred: 32.4661392890884\n",
            "Training batch[16]: last record -> y: 39.217899605891034 | y_pred: 47.67657073872465\n",
            "Training batch[17]: last record -> y: 64.4571009752251 | y_pred: 65.28890190275547\n",
            "Training batch[18]: last record -> y: 62.07709977283366 | y_pred: 58.27855421104027\n",
            "Training batch[19]: last record -> y: 58.812097967928366 | y_pred: 56.276276811870275\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.718919997274384\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.90691206228564\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 34.888811030620445\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.350845571686705\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.82516912760792\n",
            "[Training] Epoch: 878 | L1Loss: 0.05184 | RMSE: 0.08354 | PLCC: 0.94435 | SROCC: 0.94481\n",
            "[Testing]  Epoch: 878 | L1Loss: 0.07778 | RMSE: 0.10503 | PLCC: 0.91411 | SROCC: 0.92879\n",
            "Training batch[0]: last record -> y: 51.65370073635222 | y_pred: 51.666575308564234\n",
            "Training batch[1]: last record -> y: 48.045601069877875 | y_pred: 44.825862890260055\n",
            "Training batch[2]: last record -> y: 52.85669973464883 | y_pred: 51.99187133221335\n",
            "Training batch[3]: last record -> y: 29.17069987919399 | y_pred: 42.615657772823624\n",
            "Training batch[4]: last record -> y: 55.70909771244078 | y_pred: 57.063262778320905\n",
            "Training batch[5]: last record -> y: 48.061598602274216 | y_pred: 60.98638261078327\n",
            "Training batch[6]: last record -> y: 47.55979904417018 | y_pred: 47.38873415613307\n",
            "Training batch[7]: last record -> y: 31.31949991261473 | y_pred: 32.10346342609603\n",
            "Training batch[8]: last record -> y: 68.93579896233337 | y_pred: 64.35797352327745\n",
            "Training batch[9]: last record -> y: 69.6393977107623 | y_pred: 66.02550239624247\n",
            "Training batch[10]: last record -> y: 35.46180025113438 | y_pred: 50.65975996147631\n",
            "Training batch[11]: last record -> y: 33.94779976733412 | y_pred: 35.19712213899311\n",
            "Training batch[12]: last record -> y: 68.15290082533102 | y_pred: 63.51709639220417\n",
            "Training batch[13]: last record -> y: 72.15129975776699 | y_pred: 68.76130878840922\n",
            "Training batch[14]: last record -> y: 64.41010219337795 | y_pred: 65.3995852722544\n",
            "Training batch[15]: last record -> y: 66.65419834371073 | y_pred: 60.96535489329085\n",
            "Training batch[16]: last record -> y: 42.00540208510495 | y_pred: 40.30763903375396\n",
            "Training batch[17]: last record -> y: 55.6992978569499 | y_pred: 51.700201002412996\n",
            "Training batch[18]: last record -> y: 41.07529866884761 | y_pred: 42.926488798090645\n",
            "Training batch[19]: last record -> y: 44.271098442026755 | y_pred: 43.48934045887131\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 47.41392689261488\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 48.1965741636086\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 40.236870697589325\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 40.409926432541056\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.666613675256173\n",
            "[Training] Epoch: 879 | L1Loss: 0.05324 | RMSE: 0.08523 | PLCC: 0.94189 | SROCC: 0.94134\n",
            "[Testing]  Epoch: 879 | L1Loss: 0.08877 | RMSE: 0.11741 | PLCC: 0.91816 | SROCC: 0.93335\n",
            "Training batch[0]: last record -> y: 52.35879824837116 | y_pred: 54.18912629601209\n",
            "Training batch[1]: last record -> y: 57.16640087017572 | y_pred: 57.41236212671174\n",
            "Training batch[2]: last record -> y: 64.75720104616153 | y_pred: 63.30409828595566\n",
            "Training batch[3]: last record -> y: 72.15129975776699 | y_pred: 66.12845073047538\n",
            "Training batch[4]: last record -> y: 55.057096956017176 | y_pred: 56.26881515622904\n",
            "Training batch[5]: last record -> y: 59.38040274816581 | y_pred: 58.87489165962643\n",
            "Training batch[6]: last record -> y: 49.4674999580875 | y_pred: 59.28404768723658\n",
            "Training batch[7]: last record -> y: 65.49870307550941 | y_pred: 68.58719169726714\n",
            "Training batch[8]: last record -> y: 53.07039897922914 | y_pred: 54.046344942332325\n",
            "Training batch[9]: last record -> y: 53.07020278914547 | y_pred: 55.74417392641635\n",
            "Training batch[10]: last record -> y: 43.31629919695854 | y_pred: 42.37022558626734\n",
            "Training batch[11]: last record -> y: 67.77610008037209 | y_pred: 64.63671138870995\n",
            "Training batch[12]: last record -> y: 63.91980066066935 | y_pred: 64.91303708098258\n",
            "Training batch[13]: last record -> y: 39.32759880874073 | y_pred: 57.03957202165998\n",
            "Training batch[14]: last record -> y: 43.742801965819126 | y_pred: 60.58629313802339\n",
            "Training batch[15]: last record -> y: 28.55800066006435 | y_pred: 27.952504189990123\n",
            "Training batch[16]: last record -> y: 49.9361015810498 | y_pred: 49.42525798171164\n",
            "Training batch[17]: last record -> y: 43.91579820049958 | y_pred: 41.97635147968276\n",
            "Training batch[18]: last record -> y: 34.048697754381124 | y_pred: 33.83129501994267\n",
            "Training batch[19]: last record -> y: 64.42900076602791 | y_pred: 62.794801693670706\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.65312893224984\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.75300752796147\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.88215052417536\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.70265941894013\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.131429486765825\n",
            "[Training] Epoch: 880 | L1Loss: 0.05663 | RMSE: 0.08713 | PLCC: 0.93974 | SROCC: 0.93795\n",
            "[Testing]  Epoch: 880 | L1Loss: 0.07867 | RMSE: 0.10702 | PLCC: 0.91608 | SROCC: 0.93154\n",
            "Training batch[0]: last record -> y: 53.90739733586133 | y_pred: 60.77264798766328\n",
            "Training batch[1]: last record -> y: 26.82169912219564 | y_pred: 31.25239968776856\n",
            "Training batch[2]: last record -> y: 31.007400888323332 | y_pred: 28.01269193061779\n",
            "Training batch[3]: last record -> y: 38.51679986885574 | y_pred: 38.19792183392735\n",
            "Training batch[4]: last record -> y: 62.94929977644574 | y_pred: 61.77291830491072\n",
            "Training batch[5]: last record -> y: 40.03480134387053 | y_pred: 39.740228365701114\n",
            "Training batch[6]: last record -> y: 70.80740411708621 | y_pred: 68.5836570595302\n",
            "Training batch[7]: last record -> y: 54.647500304776486 | y_pred: 52.91336650652079\n",
            "Training batch[8]: last record -> y: 52.48210210784259 | y_pred: 52.97909661701215\n",
            "Training batch[9]: last record -> y: 60.46889749467823 | y_pred: 59.129269792375\n",
            "Training batch[10]: last record -> y: 43.91579820049958 | y_pred: 43.13417369092565\n",
            "Training batch[11]: last record -> y: 27.66959969745278 | y_pred: 28.542389879431312\n",
            "Training batch[12]: last record -> y: 66.95040034282079 | y_pred: 64.40102277360415\n",
            "Training batch[13]: last record -> y: 33.06290047609298 | y_pred: 34.04008951243122\n",
            "Training batch[14]: last record -> y: 44.64339807186934 | y_pred: 42.765535739939764\n",
            "Training batch[15]: last record -> y: 65.49870307550941 | y_pred: 64.97953587065422\n",
            "Training batch[16]: last record -> y: 44.302501720337546 | y_pred: 46.95644538791964\n",
            "Training batch[17]: last record -> y: 50.61160012028154 | y_pred: 59.3977639624552\n",
            "Training batch[18]: last record -> y: 51.65370073635222 | y_pred: 52.90082642232028\n",
            "Training batch[19]: last record -> y: 51.931000946581435 | y_pred: 60.40074877856523\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 43.79631039896708\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 43.780833895973274\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 33.67483664444637\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 37.8232148552712\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.242621268919095\n",
            "[Training] Epoch: 881 | L1Loss: 0.05266 | RMSE: 0.08517 | PLCC: 0.94260 | SROCC: 0.94385\n",
            "[Testing]  Epoch: 881 | L1Loss: 0.08284 | RMSE: 0.10756 | PLCC: 0.91399 | SROCC: 0.92602\n",
            "Training batch[0]: last record -> y: 62.407599658046365 | y_pred: 60.366968705634235\n",
            "Training batch[1]: last record -> y: 59.05309979644767 | y_pred: 54.874373231040636\n",
            "Training batch[2]: last record -> y: 28.55800066006435 | y_pred: 26.34264756307587\n",
            "Training batch[3]: last record -> y: 52.83100204991888 | y_pred: 50.43961860644367\n",
            "Training batch[4]: last record -> y: 60.984999631504934 | y_pred: 63.93009903194661\n",
            "Training batch[5]: last record -> y: 65.49870307550941 | y_pred: 71.0798735485339\n",
            "Training batch[6]: last record -> y: 67.38510289230953 | y_pred: 66.37301614280955\n",
            "Training batch[7]: last record -> y: 42.00540208510495 | y_pred: 42.48181110852806\n",
            "Training batch[8]: last record -> y: 29.17069987919399 | y_pred: 41.293613204740495\n",
            "Training batch[9]: last record -> y: 73.27570372205287 | y_pred: 61.56591846925312\n",
            "Training batch[10]: last record -> y: 61.653901681202115 | y_pred: 60.28862453763213\n",
            "Training batch[11]: last record -> y: 21.73069952250026 | y_pred: 22.621852171643965\n",
            "Training batch[12]: last record -> y: 55.66210214682451 | y_pred: 55.83784665079179\n",
            "Training batch[13]: last record -> y: 22.983800965443066 | y_pred: 25.90927773925796\n",
            "Training batch[14]: last record -> y: 63.66260189343916 | y_pred: 65.38448185204265\n",
            "Training batch[15]: last record -> y: 23.335699037742657 | y_pred: 24.072215708212354\n",
            "Training batch[16]: last record -> y: 53.03879951083468 | y_pred: 52.205072061007286\n",
            "Training batch[17]: last record -> y: 63.25490281841758 | y_pred: 60.66521944381475\n",
            "Training batch[18]: last record -> y: 31.31949991261473 | y_pred: 31.67961002742271\n",
            "Training batch[19]: last record -> y: 41.72359915164395 | y_pred: 44.528541642803816\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 48.29002175182245\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 48.16168770725494\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.501120743316505\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 40.652230834566694\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.279400784852612\n",
            "[Training] Epoch: 882 | L1Loss: 0.06180 | RMSE: 0.08940 | PLCC: 0.93522 | SROCC: 0.93256\n",
            "[Testing]  Epoch: 882 | L1Loss: 0.08688 | RMSE: 0.11687 | PLCC: 0.91827 | SROCC: 0.93223\n",
            "Training batch[0]: last record -> y: 68.13459725539383 | y_pred: 72.93933099925357\n",
            "Training batch[1]: last record -> y: 56.61489768005458 | y_pred: 62.96242843089726\n",
            "Training batch[2]: last record -> y: 52.01419840698122 | y_pred: 56.51191718351288\n",
            "Training batch[3]: last record -> y: 30.71260036181917 | y_pred: 30.580741328207353\n",
            "Training batch[4]: last record -> y: 51.99429958652763 | y_pred: 49.853019904961684\n",
            "Training batch[5]: last record -> y: 25.033300272477504 | y_pred: 24.082873895319295\n",
            "Training batch[6]: last record -> y: 52.05239758114112 | y_pred: 51.51719746534991\n",
            "Training batch[7]: last record -> y: 40.39899768001135 | y_pred: 61.46288008055558\n",
            "Training batch[8]: last record -> y: 63.02299970705735 | y_pred: 62.12462280031423\n",
            "Training batch[9]: last record -> y: 65.86109832235752 | y_pred: 65.74541049760956\n",
            "Training batch[10]: last record -> y: 61.4574993262936 | y_pred: 63.83198147649523\n",
            "Training batch[11]: last record -> y: 37.4706001665287 | y_pred: 37.397574036286414\n",
            "Training batch[12]: last record -> y: 36.78459902535883 | y_pred: 35.50587708722719\n",
            "Training batch[13]: last record -> y: 65.45760286109589 | y_pred: 63.836201171409584\n",
            "Training batch[14]: last record -> y: 55.55740096676209 | y_pred: 58.40707158076793\n",
            "Training batch[15]: last record -> y: 61.97249829592852 | y_pred: 62.71095777086407\n",
            "Training batch[16]: last record -> y: 47.35749812182803 | y_pred: 47.80466678220705\n",
            "Training batch[17]: last record -> y: 26.46649938788346 | y_pred: 33.941100358411404\n",
            "Training batch[18]: last record -> y: 36.69869993101997 | y_pred: 36.75476365960196\n",
            "Training batch[19]: last record -> y: 30.740801078231357 | y_pred: 29.414113880873174\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.79624959828743\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.462358197611934\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.281696300063004\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.42795807711536\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.48278622720545\n",
            "[Training] Epoch: 883 | L1Loss: 0.05449 | RMSE: 0.08813 | PLCC: 0.93908 | SROCC: 0.93865\n",
            "[Testing]  Epoch: 883 | L1Loss: 0.07660 | RMSE: 0.10428 | PLCC: 0.91473 | SROCC: 0.92929\n",
            "Training batch[0]: last record -> y: 61.97249829592852 | y_pred: 61.43735928852402\n",
            "Training batch[1]: last record -> y: 46.11660066695879 | y_pred: 39.50379519314663\n",
            "Training batch[2]: last record -> y: 28.995599425460398 | y_pred: 28.97237823850793\n",
            "Training batch[3]: last record -> y: 49.1077999127167 | y_pred: 49.41949127974408\n",
            "Training batch[4]: last record -> y: 43.92589877357773 | y_pred: 44.68226943828313\n",
            "Training batch[5]: last record -> y: 70.58750076313868 | y_pred: 65.2867116495263\n",
            "Training batch[6]: last record -> y: 57.412301018325024 | y_pred: 57.03728528150441\n",
            "Training batch[7]: last record -> y: 31.24950025563038 | y_pred: 31.096690694067263\n",
            "Training batch[8]: last record -> y: 39.941398782888996 | y_pred: 56.35988594982268\n",
            "Training batch[9]: last record -> y: 29.17069987919399 | y_pred: 42.20884798564032\n",
            "Training batch[10]: last record -> y: 72.54129669802592 | y_pred: 67.96890668915694\n",
            "Training batch[11]: last record -> y: 58.50000135581013 | y_pred: 63.26215541905162\n",
            "Training batch[12]: last record -> y: 53.863000484795975 | y_pred: 56.583854619602334\n",
            "Training batch[13]: last record -> y: 49.19860054291644 | y_pred: 55.1797961640832\n",
            "Training batch[14]: last record -> y: 55.782900562440545 | y_pred: 52.361142880682564\n",
            "Training batch[15]: last record -> y: 40.94380146120977 | y_pred: 42.46317626681025\n",
            "Training batch[16]: last record -> y: 27.960300333642863 | y_pred: 28.239843060853048\n",
            "Training batch[17]: last record -> y: 30.740801078231357 | y_pred: 30.32498102023817\n",
            "Training batch[18]: last record -> y: 30.71260036181917 | y_pred: 31.735791952489535\n",
            "Training batch[19]: last record -> y: 72.168403673586 | y_pred: 66.29205717910213\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.326126851972504\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.09886154554101\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.22634849055885\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.49296131942788\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.708943140069977\n",
            "[Training] Epoch: 884 | L1Loss: 0.05014 | RMSE: 0.08322 | PLCC: 0.94516 | SROCC: 0.94744\n",
            "[Testing]  Epoch: 884 | L1Loss: 0.07711 | RMSE: 0.10437 | PLCC: 0.91615 | SROCC: 0.92799\n",
            "Training batch[0]: last record -> y: 39.45899952945217 | y_pred: 40.05632596903388\n",
            "Training batch[1]: last record -> y: 45.98249831230828 | y_pred: 47.79742704649652\n",
            "Training batch[2]: last record -> y: 26.76479917358489 | y_pred: 26.914109475950738\n",
            "Training batch[3]: last record -> y: 40.94380146120977 | y_pred: 42.729576670587676\n",
            "Training batch[4]: last record -> y: 47.48310158637855 | y_pred: 57.759660385809866\n",
            "Training batch[5]: last record -> y: 56.6087997063064 | y_pred: 56.25795394454781\n",
            "Training batch[6]: last record -> y: 59.325401983889606 | y_pred: 62.71097385201847\n",
            "Training batch[7]: last record -> y: 34.47520052324171 | y_pred: 38.16414658534268\n",
            "Training batch[8]: last record -> y: 61.434799168743666 | y_pred: 62.399169916910296\n",
            "Training batch[9]: last record -> y: 57.412301018325024 | y_pred: 58.62068398711449\n",
            "Training batch[10]: last record -> y: 56.76089848084507 | y_pred: 56.907191958645626\n",
            "Training batch[11]: last record -> y: 52.59410091577138 | y_pred: 52.34828438962495\n",
            "Training batch[12]: last record -> y: 46.12870012752876 | y_pred: 44.19828619113798\n",
            "Training batch[13]: last record -> y: 36.69869993101997 | y_pred: 36.36143470415152\n",
            "Training batch[14]: last record -> y: 34.048697754381124 | y_pred: 34.57341295750405\n",
            "Training batch[15]: last record -> y: 38.522899450719365 | y_pred: 42.480744927991395\n",
            "Training batch[16]: last record -> y: 36.61619878460567 | y_pred: 42.409907442862846\n",
            "Training batch[17]: last record -> y: 39.47439884290486 | y_pred: 38.102388519987926\n",
            "Training batch[18]: last record -> y: 61.27290053871411 | y_pred: 61.53455700194377\n",
            "Training batch[19]: last record -> y: 63.38790039776086 | y_pred: 62.2388890510133\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.30203391259806\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.068363482369364\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.522442284373824\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.2021561557342\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.629527565156025\n",
            "[Training] Epoch: 885 | L1Loss: 0.05108 | RMSE: 0.08411 | PLCC: 0.94423 | SROCC: 0.94515\n",
            "[Testing]  Epoch: 885 | L1Loss: 0.07680 | RMSE: 0.10242 | PLCC: 0.91561 | SROCC: 0.92865\n",
            "Training batch[0]: last record -> y: 53.02539747675837 | y_pred: 50.46573761741888\n",
            "Training batch[1]: last record -> y: 26.46649938788346 | y_pred: 34.5098747083573\n",
            "Training batch[2]: last record -> y: 39.02769814411886 | y_pred: 41.07315665908163\n",
            "Training batch[3]: last record -> y: 42.544398961449815 | y_pred: 57.652691762977156\n",
            "Training batch[4]: last record -> y: 62.07709977283366 | y_pred: 60.104566076840456\n",
            "Training batch[5]: last record -> y: 63.512198072574165 | y_pred: 61.81891040649248\n",
            "Training batch[6]: last record -> y: 43.03680069292295 | y_pred: 42.606004255837775\n",
            "Training batch[7]: last record -> y: 49.146899631522956 | y_pred: 53.62775892578247\n",
            "Training batch[8]: last record -> y: 52.85669973464883 | y_pred: 51.693855378887065\n",
            "Training batch[9]: last record -> y: 38.44319964140141 | y_pred: 36.95595980663688\n",
            "Training batch[10]: last record -> y: 37.457900878899636 | y_pred: 43.26248200564612\n",
            "Training batch[11]: last record -> y: 24.811199026752462 | y_pred: 23.837303410835176\n",
            "Training batch[12]: last record -> y: 38.79510193500403 | y_pred: 42.911115214484994\n",
            "Training batch[13]: last record -> y: 58.383400121492286 | y_pred: 58.906764507645676\n",
            "Training batch[14]: last record -> y: 43.67010067489571 | y_pred: 44.765264276137486\n",
            "Training batch[15]: last record -> y: 55.84130088275674 | y_pred: 55.24187263629506\n",
            "Training batch[16]: last record -> y: 44.75650004698991 | y_pred: 44.466149979965735\n",
            "Training batch[17]: last record -> y: 48.061598602274216 | y_pred: 60.31741945269937\n",
            "Training batch[18]: last record -> y: 38.48249876652221 | y_pred: 39.05585142112557\n",
            "Training batch[19]: last record -> y: 62.796901892435244 | y_pred: 63.22834961627359\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.96541691010543\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.18658585090418\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.8924279899519\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.5265934457384\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.856414160401414\n",
            "[Training] Epoch: 886 | L1Loss: 0.05237 | RMSE: 0.08445 | PLCC: 0.94423 | SROCC: 0.94459\n",
            "[Testing]  Epoch: 886 | L1Loss: 0.07636 | RMSE: 0.10397 | PLCC: 0.91717 | SROCC: 0.93169\n",
            "Training batch[0]: last record -> y: 43.48930025598531 | y_pred: 44.05588113647116\n",
            "Training batch[1]: last record -> y: 73.27570372205287 | y_pred: 65.95417282778931\n",
            "Training batch[2]: last record -> y: 66.92800251097356 | y_pred: 67.60484221848219\n",
            "Training batch[3]: last record -> y: 67.78130394193568 | y_pred: 67.02170739765779\n",
            "Training batch[4]: last record -> y: 55.70909771244078 | y_pred: 56.681132738794076\n",
            "Training batch[5]: last record -> y: 46.50630171397677 | y_pred: 47.84898644373129\n",
            "Training batch[6]: last record -> y: 38.542598864858405 | y_pred: 39.37506876841178\n",
            "Training batch[7]: last record -> y: 38.3894998425161 | y_pred: 38.26900696883365\n",
            "Training batch[8]: last record -> y: 53.079102099989996 | y_pred: 56.1549895291605\n",
            "Training batch[9]: last record -> y: 64.74600213023791 | y_pred: 62.22804392048647\n",
            "Training batch[10]: last record -> y: 60.352402395979425 | y_pred: 54.85081433984578\n",
            "Training batch[11]: last record -> y: 38.469501977536765 | y_pred: 38.96230091352356\n",
            "Training batch[12]: last record -> y: 27.770800010202493 | y_pred: 27.087630760322327\n",
            "Training batch[13]: last record -> y: 64.73490291747157 | y_pred: 62.59351710028659\n",
            "Training batch[14]: last record -> y: 44.95980121713569 | y_pred: 57.3055575316539\n",
            "Training batch[15]: last record -> y: 36.78459902535883 | y_pred: 36.36431644701986\n",
            "Training batch[16]: last record -> y: 26.15419975119093 | y_pred: 45.14090396174879\n",
            "Training batch[17]: last record -> y: 47.787299135455896 | y_pred: 50.23434910299943\n",
            "Training batch[18]: last record -> y: 54.546501006456765 | y_pred: 49.4798985281293\n",
            "Training batch[19]: last record -> y: 68.8820042846371 | y_pred: 65.73074448479747\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.44207664568364\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.240055689281576\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.08361346451977\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.15188646708225\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.91190580191565\n",
            "[Training] Epoch: 887 | L1Loss: 0.04943 | RMSE: 0.08327 | PLCC: 0.94513 | SROCC: 0.94591\n",
            "[Testing]  Epoch: 887 | L1Loss: 0.07680 | RMSE: 0.10515 | PLCC: 0.91665 | SROCC: 0.93124\n",
            "Training batch[0]: last record -> y: 33.64979989516223 | y_pred: 36.44958355199083\n",
            "Training batch[1]: last record -> y: 22.98349944379808 | y_pred: 21.93022142034779\n",
            "Training batch[2]: last record -> y: 73.29170125444921 | y_pred: 71.79130703538613\n",
            "Training batch[3]: last record -> y: 62.94929977644574 | y_pred: 61.43577047046938\n",
            "Training batch[4]: last record -> y: 31.658599999200135 | y_pred: 32.38634540501721\n",
            "Training batch[5]: last record -> y: 26.202400197300562 | y_pred: 28.036203382407166\n",
            "Training batch[6]: last record -> y: 43.501399716555284 | y_pred: 43.80925572825845\n",
            "Training batch[7]: last record -> y: 47.48310158637855 | y_pred: 58.98590630090598\n",
            "Training batch[8]: last record -> y: 60.984999631504934 | y_pred: 61.24576519878087\n",
            "Training batch[9]: last record -> y: 39.49110072986389 | y_pred: 38.91501106278179\n",
            "Training batch[10]: last record -> y: 44.533298448275104 | y_pred: 45.782440711004824\n",
            "Training batch[11]: last record -> y: 41.65330199330322 | y_pred: 41.406769855786024\n",
            "Training batch[12]: last record -> y: 63.33969874556465 | y_pred: 62.5378923872197\n",
            "Training batch[13]: last record -> y: 51.99429958652763 | y_pred: 53.559118126344856\n",
            "Training batch[14]: last record -> y: 33.94779976733412 | y_pred: 34.16170485069074\n",
            "Training batch[15]: last record -> y: 28.777399065763746 | y_pred: 27.08819440478402\n",
            "Training batch[16]: last record -> y: 62.93809764429125 | y_pred: 64.93786959960585\n",
            "Training batch[17]: last record -> y: 70.80740411708621 | y_pred: 68.45757437657858\n",
            "Training batch[18]: last record -> y: 59.38040274816581 | y_pred: 60.27610696704778\n",
            "Training batch[19]: last record -> y: 52.915900896454104 | y_pred: 49.91147168497196\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.63393446635894\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.497968459767094\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.202951865169894\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.87885901957691\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.10175774675497\n",
            "[Training] Epoch: 888 | L1Loss: 0.04913 | RMSE: 0.08357 | PLCC: 0.94512 | SROCC: 0.94637\n",
            "[Testing]  Epoch: 888 | L1Loss: 0.07778 | RMSE: 0.10619 | PLCC: 0.91652 | SROCC: 0.93111\n",
            "Training batch[0]: last record -> y: 58.50000135581013 | y_pred: 64.75144077665573\n",
            "Training batch[1]: last record -> y: 48.33070063999071 | y_pred: 48.987393877268005\n",
            "Training batch[2]: last record -> y: 43.297297704920425 | y_pred: 42.583609640221425\n",
            "Training batch[3]: last record -> y: 61.01490092999484 | y_pred: 50.53727302464836\n",
            "Training batch[4]: last record -> y: 25.665500705518212 | y_pred: 25.32552057775831\n",
            "Training batch[5]: last record -> y: 57.950199551824426 | y_pred: 58.763272366941464\n",
            "Training batch[6]: last record -> y: 49.146899631522956 | y_pred: 55.74670510011879\n",
            "Training batch[7]: last record -> y: 62.891102078674976 | y_pred: 63.50877921914889\n",
            "Training batch[8]: last record -> y: 29.695998828221605 | y_pred: 47.13433029353746\n",
            "Training batch[9]: last record -> y: 40.63280158382156 | y_pred: 36.402018713508824\n",
            "Training batch[10]: last record -> y: 47.6147001052891 | y_pred: 46.367762568827516\n",
            "Training batch[11]: last record -> y: 65.59839658409192 | y_pred: 62.192038215786624\n",
            "Training batch[12]: last record -> y: 67.7326970446486 | y_pred: 69.55875072145022\n",
            "Training batch[13]: last record -> y: 34.47520052324171 | y_pred: 38.787524495051116\n",
            "Training batch[14]: last record -> y: 72.15129975776699 | y_pred: 69.68054295240813\n",
            "Training batch[15]: last record -> y: 60.352402395979425 | y_pred: 53.58692565853198\n",
            "Training batch[16]: last record -> y: 49.328201782450606 | y_pred: 48.38244657861651\n",
            "Training batch[17]: last record -> y: 58.14030131043933 | y_pred: 61.855520762597735\n",
            "Training batch[18]: last record -> y: 59.08630094782029 | y_pred: 56.97238495858005\n",
            "Training batch[19]: last record -> y: 52.59410091577138 | y_pred: 52.63780306096669\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.80184905624924\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.99299930613097\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.804378845269866\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.50845551169152\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.04662567289509\n",
            "[Training] Epoch: 889 | L1Loss: 0.05229 | RMSE: 0.08415 | PLCC: 0.94366 | SROCC: 0.94312\n",
            "[Testing]  Epoch: 889 | L1Loss: 0.07660 | RMSE: 0.10407 | PLCC: 0.91775 | SROCC: 0.93413\n",
            "Training batch[0]: last record -> y: 66.95040034282079 | y_pred: 64.57813417569537\n",
            "Training batch[1]: last record -> y: 54.171601053947825 | y_pred: 54.76282791166591\n",
            "Training batch[2]: last record -> y: 45.45109978141704 | y_pred: 43.69997141919225\n",
            "Training batch[3]: last record -> y: 40.39899768001135 | y_pred: 59.73700556319932\n",
            "Training batch[4]: last record -> y: 41.601001254850644 | y_pred: 42.14925926801402\n",
            "Training batch[5]: last record -> y: 55.6992978569499 | y_pred: 52.67316551949057\n",
            "Training batch[6]: last record -> y: 51.99429958652763 | y_pred: 53.2908876871968\n",
            "Training batch[7]: last record -> y: 44.302501720337546 | y_pred: 45.77300911394968\n",
            "Training batch[8]: last record -> y: 57.104697480745926 | y_pred: 54.35378123590567\n",
            "Training batch[9]: last record -> y: 59.88249819951079 | y_pred: 57.957165907910166\n",
            "Training batch[10]: last record -> y: 39.47439884290486 | y_pred: 37.99528159922738\n",
            "Training batch[11]: last record -> y: 64.41010219337795 | y_pred: 67.17775248748603\n",
            "Training batch[12]: last record -> y: 38.51679986885574 | y_pred: 38.81488818737682\n",
            "Training batch[13]: last record -> y: 51.01290211208311 | y_pred: 53.63750732157928\n",
            "Training batch[14]: last record -> y: 44.16859877200159 | y_pred: 44.477435734123105\n",
            "Training batch[15]: last record -> y: 68.13459725539383 | y_pred: 70.01019696889944\n",
            "Training batch[16]: last record -> y: 59.43330009744659 | y_pred: 61.082898483256486\n",
            "Training batch[17]: last record -> y: 29.276899018788697 | y_pred: 34.21197132311181\n",
            "Training batch[18]: last record -> y: 48.53810250450829 | y_pred: 42.5467387694149\n",
            "Training batch[19]: last record -> y: 67.31629806509704 | y_pred: 68.4000810333705\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.88153133015089\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 47.261914956309965\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.84488758090856\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.87235434647596\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.723994698558784\n",
            "[Training] Epoch: 890 | L1Loss: 0.05004 | RMSE: 0.08272 | PLCC: 0.94592 | SROCC: 0.94746\n",
            "[Testing]  Epoch: 890 | L1Loss: 0.08213 | RMSE: 0.10990 | PLCC: 0.91822 | SROCC: 0.93393\n",
            "Training batch[0]: last record -> y: 30.074199437670984 | y_pred: 30.115831134237396\n",
            "Training batch[1]: last record -> y: 69.64639944538771 | y_pred: 67.28410037775893\n",
            "Training batch[2]: last record -> y: 27.261600708901653 | y_pred: 26.961168562126886\n",
            "Training batch[3]: last record -> y: 36.81190160929782 | y_pred: 33.562902965192905\n",
            "Training batch[4]: last record -> y: 52.83100204991888 | y_pred: 50.48532124724625\n",
            "Training batch[5]: last record -> y: 39.685200263462434 | y_pred: 40.213454928668625\n",
            "Training batch[6]: last record -> y: 32.75839979725521 | y_pred: 33.27123263539255\n",
            "Training batch[7]: last record -> y: 43.92589877357773 | y_pred: 44.085911084196255\n",
            "Training batch[8]: last record -> y: 63.56399868712492 | y_pred: 65.97157585308014\n",
            "Training batch[9]: last record -> y: 43.91579820049958 | y_pred: 42.02550674433485\n",
            "Training batch[10]: last record -> y: 60.54589727817256 | y_pred: 60.52436782866289\n",
            "Training batch[11]: last record -> y: 67.85620031043459 | y_pred: 69.98239586917407\n",
            "Training batch[12]: last record -> y: 55.66210214682451 | y_pred: 57.06153566233843\n",
            "Training batch[13]: last record -> y: 60.984999631504934 | y_pred: 62.88584675741731\n",
            "Training batch[14]: last record -> y: 60.85200205216165 | y_pred: 59.755556782914255\n",
            "Training batch[15]: last record -> y: 43.91579820049958 | y_pred: 42.4777281034261\n",
            "Training batch[16]: last record -> y: 72.15129975776699 | y_pred: 68.29697510382448\n",
            "Training batch[17]: last record -> y: 68.13459725539383 | y_pred: 70.9477089729885\n",
            "Training batch[18]: last record -> y: 41.19950146484996 | y_pred: 42.63755869699992\n",
            "Training batch[19]: last record -> y: 67.08080242384403 | y_pred: 67.14111640153374\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.914240244345365\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.93033587178354\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.92764893431706\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.398987877465174\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.360494274374588\n",
            "[Training] Epoch: 891 | L1Loss: 0.05127 | RMSE: 0.08495 | PLCC: 0.94285 | SROCC: 0.94329\n",
            "[Testing]  Epoch: 891 | L1Loss: 0.07680 | RMSE: 0.10492 | PLCC: 0.91650 | SROCC: 0.93168\n",
            "Training batch[0]: last record -> y: 49.328201782450606 | y_pred: 50.20506853706934\n",
            "Training batch[1]: last record -> y: 43.4957005554362 | y_pred: 45.45663009041493\n",
            "Training batch[2]: last record -> y: 56.49319871979219 | y_pred: 54.843780442911566\n",
            "Training batch[3]: last record -> y: 46.96450043815821 | y_pred: 45.55158287468055\n",
            "Training batch[4]: last record -> y: 51.776001131789144 | y_pred: 49.27265425892483\n",
            "Training batch[5]: last record -> y: 39.04919864755061 | y_pred: 39.18712349260181\n",
            "Training batch[6]: last record -> y: 63.8380022607023 | y_pred: 65.79110027348861\n",
            "Training batch[7]: last record -> y: 48.61320149555263 | y_pred: 47.77709081864327\n",
            "Training batch[8]: last record -> y: 64.93820087138647 | y_pred: 63.14190054645428\n",
            "Training batch[9]: last record -> y: 43.297297704920425 | y_pred: 43.30462106263383\n",
            "Training batch[10]: last record -> y: 48.33070063999071 | y_pred: 48.64210576246978\n",
            "Training batch[11]: last record -> y: 22.98349944379808 | y_pred: 21.806804992795563\n",
            "Training batch[12]: last record -> y: 30.256500228286825 | y_pred: 32.48658888908017\n",
            "Training batch[13]: last record -> y: 29.39489931353927 | y_pred: 29.493497695507187\n",
            "Training batch[14]: last record -> y: 66.50039818303662 | y_pred: 57.13248571554777\n",
            "Training batch[15]: last record -> y: 31.38129978897092 | y_pred: 32.13376192909959\n",
            "Training batch[16]: last record -> y: 63.20160022404593 | y_pred: 68.03311873867301\n",
            "Training batch[17]: last record -> y: 60.28530217113325 | y_pred: 56.70675001775203\n",
            "Training batch[18]: last record -> y: 62.796901892435244 | y_pred: 62.859100581420535\n",
            "Training batch[19]: last record -> y: 64.65950160072452 | y_pred: 65.60392850120525\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.1557470211128\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.69018971453181\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.84149897396958\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.177653300776115\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.346595734656205\n",
            "[Training] Epoch: 892 | L1Loss: 0.04958 | RMSE: 0.08258 | PLCC: 0.94551 | SROCC: 0.94672\n",
            "[Testing]  Epoch: 892 | L1Loss: 0.07861 | RMSE: 0.10711 | PLCC: 0.91735 | SROCC: 0.93269\n",
            "Training batch[0]: last record -> y: 67.7326970446486 | y_pred: 68.21181895881887\n",
            "Training batch[1]: last record -> y: 58.416900382336735 | y_pred: 60.81469699018635\n",
            "Training batch[2]: last record -> y: 65.84580192829299 | y_pred: 68.18282463743708\n",
            "Training batch[3]: last record -> y: 61.12310136925453 | y_pred: 62.213393988828784\n",
            "Training batch[4]: last record -> y: 60.352402395979425 | y_pred: 53.908394367434084\n",
            "Training batch[5]: last record -> y: 57.606300848766296 | y_pred: 64.56450378922659\n",
            "Training batch[6]: last record -> y: 27.770800010202493 | y_pred: 27.132239882625754\n",
            "Training batch[7]: last record -> y: 45.79939989643424 | y_pred: 47.359829889215916\n",
            "Training batch[8]: last record -> y: 47.66609869097988 | y_pred: 48.804451448591635\n",
            "Training batch[9]: last record -> y: 44.28150134080761 | y_pred: 40.046939399211055\n",
            "Training batch[10]: last record -> y: 65.06269795251433 | y_pred: 64.21688712327136\n",
            "Training batch[11]: last record -> y: 54.441201607450694 | y_pred: 58.858939154462405\n",
            "Training batch[12]: last record -> y: 54.171601053947825 | y_pred: 54.88120128919854\n",
            "Training batch[13]: last record -> y: 30.71260036181917 | y_pred: 31.590367661084258\n",
            "Training batch[14]: last record -> y: 56.58480019148101 | y_pred: 55.75181569098686\n",
            "Training batch[15]: last record -> y: 44.271098442026755 | y_pred: 41.25304206030671\n",
            "Training batch[16]: last record -> y: 45.05469932547635 | y_pred: 42.429624546271725\n",
            "Training batch[17]: last record -> y: 43.91579820049958 | y_pred: 43.78339723198451\n",
            "Training batch[18]: last record -> y: 54.96030127145741 | y_pred: 53.74170355339356\n",
            "Training batch[19]: last record -> y: 28.923200460239684 | y_pred: 28.310789897831512\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.416530123805046\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.26952118848726\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.913560081094374\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.96107713767378\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.9176068193255\n",
            "[Training] Epoch: 893 | L1Loss: 0.04988 | RMSE: 0.08296 | PLCC: 0.94604 | SROCC: 0.94682\n",
            "[Testing]  Epoch: 893 | L1Loss: 0.07688 | RMSE: 0.10522 | PLCC: 0.91600 | SROCC: 0.93019\n",
            "Training batch[0]: last record -> y: 37.4706001665287 | y_pred: 36.92581889894643\n",
            "Training batch[1]: last record -> y: 66.65419834371073 | y_pred: 64.63863791100698\n",
            "Training batch[2]: last record -> y: 57.104697480745926 | y_pred: 53.896693719493214\n",
            "Training batch[3]: last record -> y: 38.76359895353596 | y_pred: 55.40427621457275\n",
            "Training batch[4]: last record -> y: 28.995599425460398 | y_pred: 30.003796947768933\n",
            "Training batch[5]: last record -> y: 57.24819927014278 | y_pred: 54.60505248962431\n",
            "Training batch[6]: last record -> y: 28.998800379243562 | y_pred: 28.054350965146682\n",
            "Training batch[7]: last record -> y: 55.84130088275674 | y_pred: 53.97401190984465\n",
            "Training batch[8]: last record -> y: 73.66169645486639 | y_pred: 71.91604855006085\n",
            "Training batch[9]: last record -> y: 53.70280002467098 | y_pred: 54.291727277309974\n",
            "Training batch[10]: last record -> y: 68.4131003359721 | y_pred: 66.15572115210557\n",
            "Training batch[11]: last record -> y: 52.01739855670667 | y_pred: 54.14543058327854\n",
            "Training batch[12]: last record -> y: 53.02539747675837 | y_pred: 52.53926417927005\n",
            "Training batch[13]: last record -> y: 28.55800066006435 | y_pred: 28.208119767569713\n",
            "Training batch[14]: last record -> y: 44.64339807186934 | y_pred: 42.593261549091835\n",
            "Training batch[15]: last record -> y: 49.1077999127167 | y_pred: 48.73572702715114\n",
            "Training batch[16]: last record -> y: 62.20869829174421 | y_pred: 57.76127493371155\n",
            "Training batch[17]: last record -> y: 38.38710053427974 | y_pred: 37.651744722143235\n",
            "Training batch[18]: last record -> y: 29.17069987919399 | y_pred: 43.849149856092026\n",
            "Training batch[19]: last record -> y: 56.32199875005813 | y_pred: 54.48314125812385\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.089476429980095\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.06448325366637\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.00764769925331\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.33777641750646\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.953590814466907\n",
            "[Training] Epoch: 894 | L1Loss: 0.05320 | RMSE: 0.08501 | PLCC: 0.94286 | SROCC: 0.94252\n",
            "[Testing]  Epoch: 894 | L1Loss: 0.07649 | RMSE: 0.10458 | PLCC: 0.91679 | SROCC: 0.93093\n",
            "Training batch[0]: last record -> y: 69.51529622603266 | y_pred: 67.7250391989237\n",
            "Training batch[1]: last record -> y: 70.78559807172087 | y_pred: 67.49311679017819\n",
            "Training batch[2]: last record -> y: 48.6593994359107 | y_pred: 48.247429206280685\n",
            "Training batch[3]: last record -> y: 53.863000484795975 | y_pred: 56.95519742075817\n",
            "Training batch[4]: last record -> y: 41.8042976006501 | y_pred: 36.81113293011754\n",
            "Training batch[5]: last record -> y: 59.05309979644767 | y_pred: 57.31817802162641\n",
            "Training batch[6]: last record -> y: 40.115501400992116 | y_pred: 35.03426346404592\n",
            "Training batch[7]: last record -> y: 56.444000036023226 | y_pred: 56.8199163174902\n",
            "Training batch[8]: last record -> y: 58.169900283226525 | y_pred: 57.17135708196156\n",
            "Training batch[9]: last record -> y: 32.322799919350985 | y_pred: 30.235346273732375\n",
            "Training batch[10]: last record -> y: 55.782900562440545 | y_pred: 53.98143497071533\n",
            "Training batch[11]: last record -> y: 22.98349944379808 | y_pred: 22.522663611309596\n",
            "Training batch[12]: last record -> y: 61.12310136925453 | y_pred: 59.833974924226595\n",
            "Training batch[13]: last record -> y: 51.99429958652763 | y_pred: 50.8816798921855\n",
            "Training batch[14]: last record -> y: 28.55800066006435 | y_pred: 28.401520575009613\n",
            "Training batch[15]: last record -> y: 51.776001131789144 | y_pred: 49.8444968931301\n",
            "Training batch[16]: last record -> y: 43.742801965819126 | y_pred: 60.660781045200565\n",
            "Training batch[17]: last record -> y: 28.923200460239684 | y_pred: 29.05246319147375\n",
            "Training batch[18]: last record -> y: 38.59769772417644 | y_pred: 37.399178935495456\n",
            "Training batch[19]: last record -> y: 33.93240045388143 | y_pred: 30.77193740937912\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.889069867626176\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.78284258601127\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.679058255183236\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.199301596974806\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.564999769131333\n",
            "[Training] Epoch: 895 | L1Loss: 0.05149 | RMSE: 0.08403 | PLCC: 0.94381 | SROCC: 0.94418\n",
            "[Testing]  Epoch: 895 | L1Loss: 0.07737 | RMSE: 0.10361 | PLCC: 0.91517 | SROCC: 0.92883\n",
            "Training batch[0]: last record -> y: 69.90390053832061 | y_pred: 64.45680829821504\n",
            "Training batch[1]: last record -> y: 72.626600789652 | y_pred: 67.85817185996393\n",
            "Training batch[2]: last record -> y: 68.26640161308069 | y_pred: 68.30069628295246\n",
            "Training batch[3]: last record -> y: 61.282700394204994 | y_pred: 62.70343179060524\n",
            "Training batch[4]: last record -> y: 56.49319871979219 | y_pred: 55.68421695035431\n",
            "Training batch[5]: last record -> y: 53.07020278914547 | y_pred: 57.69471503565319\n",
            "Training batch[6]: last record -> y: 43.91579820049958 | y_pred: 42.79717380310478\n",
            "Training batch[7]: last record -> y: 40.68579863625962 | y_pred: 45.3788648439712\n",
            "Training batch[8]: last record -> y: 57.048500278582424 | y_pred: 56.38437754797269\n",
            "Training batch[9]: last record -> y: 28.126199954886943 | y_pred: 28.710099434603023\n",
            "Training batch[10]: last record -> y: 24.490699609431772 | y_pred: 23.95197490662511\n",
            "Training batch[11]: last record -> y: 22.563999213620633 | y_pred: 23.985560799616735\n",
            "Training batch[12]: last record -> y: 65.45760286109589 | y_pred: 66.41344738120006\n",
            "Training batch[13]: last record -> y: 27.66959969745278 | y_pred: 27.408842974811762\n",
            "Training batch[14]: last record -> y: 44.28150134080761 | y_pred: 41.00388708650701\n",
            "Training batch[15]: last record -> y: 67.704902377385 | y_pred: 65.73912598247034\n",
            "Training batch[16]: last record -> y: 52.915900896454104 | y_pred: 49.21927125878119\n",
            "Training batch[17]: last record -> y: 29.981600130351694 | y_pred: 28.536568501538795\n",
            "Training batch[18]: last record -> y: 25.808300150496677 | y_pred: 39.99322191105591\n",
            "Training batch[19]: last record -> y: 65.86109832235752 | y_pred: 65.31109067959551\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.197971308218825\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.59778740135391\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.989829934032514\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.92475302611683\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.347377278760007\n",
            "[Training] Epoch: 896 | L1Loss: 0.04895 | RMSE: 0.08178 | PLCC: 0.94684 | SROCC: 0.94711\n",
            "[Testing]  Epoch: 896 | L1Loss: 0.07770 | RMSE: 0.10574 | PLCC: 0.91705 | SROCC: 0.93152\n",
            "Training batch[0]: last record -> y: 63.999000346085495 | y_pred: 63.25234913109898\n",
            "Training batch[1]: last record -> y: 53.70280002467098 | y_pred: 54.15816685756272\n",
            "Training batch[2]: last record -> y: 43.91579820049958 | y_pred: 42.3599481204908\n",
            "Training batch[3]: last record -> y: 51.931000946581435 | y_pred: 58.2721989388217\n",
            "Training batch[4]: last record -> y: 48.559300682237335 | y_pred: 45.99995601352407\n",
            "Training batch[5]: last record -> y: 64.93820087138647 | y_pred: 61.85150369022881\n",
            "Training batch[6]: last record -> y: 48.25579783903004 | y_pred: 48.67870325365152\n",
            "Training batch[7]: last record -> y: 38.59769772417644 | y_pred: 39.90993117996061\n",
            "Training batch[8]: last record -> y: 58.27209923566443 | y_pred: 61.57307779919165\n",
            "Training batch[9]: last record -> y: 55.86600153591394 | y_pred: 55.52083885412003\n",
            "Training batch[10]: last record -> y: 38.542598864858405 | y_pred: 38.999409785415196\n",
            "Training batch[11]: last record -> y: 47.55770206163652 | y_pred: 37.46494924887281\n",
            "Training batch[12]: last record -> y: 56.92869889453914 | y_pred: 56.88128200267761\n",
            "Training batch[13]: last record -> y: 63.25490281841758 | y_pred: 61.69995167493971\n",
            "Training batch[14]: last record -> y: 39.02769814411886 | y_pred: 40.67804912795464\n",
            "Training batch[15]: last record -> y: 71.94249883281668 | y_pred: 73.3176305078025\n",
            "Training batch[16]: last record -> y: 60.54589727817256 | y_pred: 64.05006444376477\n",
            "Training batch[17]: last record -> y: 22.98349944379808 | y_pred: 23.699752452623756\n",
            "Training batch[18]: last record -> y: 53.07020278914547 | y_pred: 57.571202121174565\n",
            "Training batch[19]: last record -> y: 54.171601053947825 | y_pred: 52.974178999996866\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 43.912218535530315\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 42.70241399269196\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 33.864082885647235\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 37.72618438585721\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.597002874501214\n",
            "[Training] Epoch: 897 | L1Loss: 0.05754 | RMSE: 0.08641 | PLCC: 0.94089 | SROCC: 0.93933\n",
            "[Testing]  Epoch: 897 | L1Loss: 0.08569 | RMSE: 0.10924 | PLCC: 0.91500 | SROCC: 0.92794\n",
            "Training batch[0]: last record -> y: 46.96450043815821 | y_pred: 42.95474177825463\n",
            "Training batch[1]: last record -> y: 47.35749812182803 | y_pred: 46.14702138673579\n",
            "Training batch[2]: last record -> y: 47.797999535593135 | y_pred: 45.39732922545238\n",
            "Training batch[3]: last record -> y: 36.49660163122326 | y_pred: 36.72478677968638\n",
            "Training batch[4]: last record -> y: 40.48300241436198 | y_pred: 41.89716465919014\n",
            "Training batch[5]: last record -> y: 43.67010067489571 | y_pred: 44.939170704156936\n",
            "Training batch[6]: last record -> y: 28.55800066006435 | y_pred: 29.638269896101576\n",
            "Training batch[7]: last record -> y: 58.359600012981446 | y_pred: 59.57355670988568\n",
            "Training batch[8]: last record -> y: 64.53830115624851 | y_pred: 61.237901514279656\n",
            "Training batch[9]: last record -> y: 49.39419884010499 | y_pred: 56.64554192787773\n",
            "Training batch[10]: last record -> y: 25.707799769992192 | y_pred: 22.982410347616394\n",
            "Training batch[11]: last record -> y: 39.941398782888996 | y_pred: 55.86577318352147\n",
            "Training batch[12]: last record -> y: 51.776001131789144 | y_pred: 48.82694576736526\n",
            "Training batch[13]: last record -> y: 63.25490281841758 | y_pred: 63.43100432401252\n",
            "Training batch[14]: last record -> y: 41.07529866884761 | y_pred: 41.672203782184056\n",
            "Training batch[15]: last record -> y: 63.38619901162542 | y_pred: 63.61089454958392\n",
            "Training batch[16]: last record -> y: 64.4571009752251 | y_pred: 66.02041110275968\n",
            "Training batch[17]: last record -> y: 68.3981030513794 | y_pred: 64.05988038041005\n",
            "Training batch[18]: last record -> y: 63.3563009293664 | y_pred: 61.34806707060693\n",
            "Training batch[19]: last record -> y: 72.626600789652 | y_pred: 67.63200007203159\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.31134007664889\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.984365180480495\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.07324433616316\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.82219224770495\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.02167279378915\n",
            "[Training] Epoch: 898 | L1Loss: 0.05246 | RMSE: 0.08332 | PLCC: 0.94405 | SROCC: 0.94374\n",
            "[Testing]  Epoch: 898 | L1Loss: 0.07572 | RMSE: 0.10236 | PLCC: 0.91614 | SROCC: 0.93129\n",
            "Training batch[0]: last record -> y: 69.23919566992163 | y_pred: 67.42851879295654\n",
            "Training batch[1]: last record -> y: 31.31949991261473 | y_pred: 32.05132189107201\n",
            "Training batch[2]: last record -> y: 54.546501006456765 | y_pred: 51.400895340503894\n",
            "Training batch[3]: last record -> y: 36.61619878460567 | y_pred: 42.302557696670874\n",
            "Training batch[4]: last record -> y: 29.01670111626305 | y_pred: 29.476344732167263\n",
            "Training batch[5]: last record -> y: 65.45760286109589 | y_pred: 62.543318168713995\n",
            "Training batch[6]: last record -> y: 45.04560060831727 | y_pred: 41.2237325482987\n",
            "Training batch[7]: last record -> y: 46.75879995977607 | y_pred: 45.73676862439561\n",
            "Training batch[8]: last record -> y: 73.27570372205287 | y_pred: 66.28603639489506\n",
            "Training batch[9]: last record -> y: 57.048500278582424 | y_pred: 58.84717418190394\n",
            "Training batch[10]: last record -> y: 55.44500012997332 | y_pred: 56.90131268859727\n",
            "Training batch[11]: last record -> y: 61.28480059296953 | y_pred: 63.932874639195916\n",
            "Training batch[12]: last record -> y: 56.724497179747004 | y_pred: 61.377534177928055\n",
            "Training batch[13]: last record -> y: 68.8820042846371 | y_pred: 64.19939082728501\n",
            "Training batch[14]: last record -> y: 69.51529622603266 | y_pred: 65.75577319350441\n",
            "Training batch[15]: last record -> y: 26.387500716897307 | y_pred: 26.933094082776734\n",
            "Training batch[16]: last record -> y: 37.43249908741063 | y_pred: 49.60369125469447\n",
            "Training batch[17]: last record -> y: 65.26629501590105 | y_pred: 66.58776066242581\n",
            "Training batch[18]: last record -> y: 41.601001254850644 | y_pred: 43.735864555811304\n",
            "Training batch[19]: last record -> y: 28.55800066006435 | y_pred: 28.922726066184907\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.46546507664186\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.67430298824729\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.67987050733393\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.43022375791668\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.97940669568169\n",
            "[Training] Epoch: 899 | L1Loss: 0.05599 | RMSE: 0.08637 | PLCC: 0.94080 | SROCC: 0.94108\n",
            "[Testing]  Epoch: 899 | L1Loss: 0.07569 | RMSE: 0.10308 | PLCC: 0.91604 | SROCC: 0.92947\n",
            "Training batch[0]: last record -> y: 25.808300150496677 | y_pred: 39.74157596643977\n",
            "Training batch[1]: last record -> y: 70.18830218633252 | y_pred: 64.20083813118094\n",
            "Training batch[2]: last record -> y: 44.64339807186934 | y_pred: 42.62686955367076\n",
            "Training batch[3]: last record -> y: 34.8158990765744 | y_pred: 33.537027583706845\n",
            "Training batch[4]: last record -> y: 39.64730019877436 | y_pred: 41.057532209467354\n",
            "Training batch[5]: last record -> y: 43.03680069292295 | y_pred: 43.76842246100796\n",
            "Training batch[6]: last record -> y: 55.65470159956999 | y_pred: 58.418547092547215\n",
            "Training batch[7]: last record -> y: 25.707799769992192 | y_pred: 23.742312031799003\n",
            "Training batch[8]: last record -> y: 50.848597741355434 | y_pred: 49.97715998446188\n",
            "Training batch[9]: last record -> y: 61.766399004917275 | y_pred: 55.44529923944515\n",
            "Training batch[10]: last record -> y: 54.38159841678544 | y_pred: 49.69404804503279\n",
            "Training batch[11]: last record -> y: 69.51529622603266 | y_pred: 68.07262370257013\n",
            "Training batch[12]: last record -> y: 25.83860026161568 | y_pred: 26.364346266735595\n",
            "Training batch[13]: last record -> y: 56.58480019148101 | y_pred: 57.66043323070494\n",
            "Training batch[14]: last record -> y: 43.26480090811049 | y_pred: 49.990352963530995\n",
            "Training batch[15]: last record -> y: 29.17069987919399 | y_pred: 41.54954960135035\n",
            "Training batch[16]: last record -> y: 34.00809927198486 | y_pred: 37.32052600932889\n",
            "Training batch[17]: last record -> y: 54.38009965319543 | y_pred: 63.75764794840029\n",
            "Training batch[18]: last record -> y: 35.17409875023975 | y_pred: 50.25853837544673\n",
            "Training batch[19]: last record -> y: 58.359600012981446 | y_pred: 60.28008222841527\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.799157224856344\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.62500153894371\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.97336459389669\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.80535060855618\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.058867853710353\n",
            "[Training] Epoch: 900 | L1Loss: 0.05239 | RMSE: 0.08320 | PLCC: 0.94507 | SROCC: 0.94513\n",
            "[Testing]  Epoch: 900 | L1Loss: 0.07978 | RMSE: 0.10812 | PLCC: 0.91751 | SROCC: 0.93395\n",
            "Training batch[0]: last record -> y: 21.73069952250026 | y_pred: 23.072340585334274\n",
            "Training batch[1]: last record -> y: 58.27209923566443 | y_pred: 60.14650251128273\n",
            "Training batch[2]: last record -> y: 26.15419975119093 | y_pred: 43.08954205500606\n",
            "Training batch[3]: last record -> y: 39.217899605891034 | y_pred: 44.91021015319939\n",
            "Training batch[4]: last record -> y: 67.50669893318377 | y_pred: 67.08633755718824\n",
            "Training batch[5]: last record -> y: 47.782002003196794 | y_pred: 46.6131947553838\n",
            "Training batch[6]: last record -> y: 62.891102078674976 | y_pred: 65.4188923062261\n",
            "Training batch[7]: last record -> y: 69.7550951842029 | y_pred: 71.34971853558363\n",
            "Training batch[8]: last record -> y: 65.59839658409192 | y_pred: 64.3832080707607\n",
            "Training batch[9]: last record -> y: 28.55800066006435 | y_pred: 28.657711053858975\n",
            "Training batch[10]: last record -> y: 39.65230143779252 | y_pred: 38.63279484365273\n",
            "Training batch[11]: last record -> y: 60.042196927618534 | y_pred: 57.360532566083066\n",
            "Training batch[12]: last record -> y: 63.33969874556465 | y_pred: 60.36505183202985\n",
            "Training batch[13]: last record -> y: 38.48249876652221 | y_pred: 39.31549452382444\n",
            "Training batch[14]: last record -> y: 63.512198072574165 | y_pred: 60.97567577818427\n",
            "Training batch[15]: last record -> y: 65.29879824517275 | y_pred: 65.70749756799796\n",
            "Training batch[16]: last record -> y: 35.46180025113438 | y_pred: 49.602504465499806\n",
            "Training batch[17]: last record -> y: 68.08779787986123 | y_pred: 63.815064102067254\n",
            "Training batch[18]: last record -> y: 34.8158990765744 | y_pred: 33.18487201192244\n",
            "Training batch[19]: last record -> y: 33.262500568553776 | y_pred: 44.789743009363974\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.52439285882235\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.049017853627106\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.86124808583372\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.023222758849556\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.014964942260377\n",
            "[Training] Epoch: 901 | L1Loss: 0.05330 | RMSE: 0.08467 | PLCC: 0.94295 | SROCC: 0.94295\n",
            "[Testing]  Epoch: 901 | L1Loss: 0.07593 | RMSE: 0.10313 | PLCC: 0.91611 | SROCC: 0.92967\n",
            "Training batch[0]: last record -> y: 42.59540195074237 | y_pred: 41.60438151350536\n",
            "Training batch[1]: last record -> y: 36.83800132288775 | y_pred: 37.28391726133907\n",
            "Training batch[2]: last record -> y: 55.86600153591394 | y_pred: 57.11728580840963\n",
            "Training batch[3]: last record -> y: 52.21930066641971 | y_pred: 50.72626839984059\n",
            "Training batch[4]: last record -> y: 58.45500306957024 | y_pred: 61.49473363118955\n",
            "Training batch[5]: last record -> y: 61.02579752021575 | y_pred: 58.941036663901286\n",
            "Training batch[6]: last record -> y: 41.72359915164395 | y_pred: 41.850030795646035\n",
            "Training batch[7]: last record -> y: 38.469501977536765 | y_pred: 38.20975273921886\n",
            "Training batch[8]: last record -> y: 25.808300150496677 | y_pred: 40.315522015640454\n",
            "Training batch[9]: last record -> y: 65.29879824517275 | y_pred: 66.74892599181476\n",
            "Training batch[10]: last record -> y: 48.22529832159648 | y_pred: 50.724547716319876\n",
            "Training batch[11]: last record -> y: 48.045601069877875 | y_pred: 46.02090493335993\n",
            "Training batch[12]: last record -> y: 27.51029978197414 | y_pred: 27.680849269012754\n",
            "Training batch[13]: last record -> y: 70.23500185870785 | y_pred: 66.8634045137519\n",
            "Training batch[14]: last record -> y: 46.40150083075707 | y_pred: 42.1903900366209\n",
            "Training batch[15]: last record -> y: 57.606300848766296 | y_pred: 64.5967658011823\n",
            "Training batch[16]: last record -> y: 55.057096956017176 | y_pred: 58.76681343714017\n",
            "Training batch[17]: last record -> y: 65.49870307550941 | y_pred: 67.56566209390394\n",
            "Training batch[18]: last record -> y: 60.352402395979425 | y_pred: 54.95985421536511\n",
            "Training batch[19]: last record -> y: 68.4131003359721 | y_pred: 64.92599527519747\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.668484826585654\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.738575992296546\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.01007771553657\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.302242044484046\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.16828628656225\n",
            "[Training] Epoch: 902 | L1Loss: 0.05074 | RMSE: 0.08346 | PLCC: 0.94524 | SROCC: 0.94624\n",
            "[Testing]  Epoch: 902 | L1Loss: 0.07487 | RMSE: 0.10213 | PLCC: 0.91659 | SROCC: 0.93114\n",
            "Training batch[0]: last record -> y: 67.8711011081009 | y_pred: 64.52086596864888\n",
            "Training batch[1]: last record -> y: 29.981600130351694 | y_pred: 29.391883293031697\n",
            "Training batch[2]: last record -> y: 65.29879824517275 | y_pred: 64.4156630565692\n",
            "Training batch[3]: last record -> y: 33.93240045388143 | y_pred: 33.18601618605794\n",
            "Training batch[4]: last record -> y: 54.546501006456765 | y_pred: 50.19559030466644\n",
            "Training batch[5]: last record -> y: 64.73490291747157 | y_pred: 62.235962280912645\n",
            "Training batch[6]: last record -> y: 57.950199551824426 | y_pred: 56.93058682206561\n",
            "Training batch[7]: last record -> y: 48.60699738618541 | y_pred: 49.114695511723085\n",
            "Training batch[8]: last record -> y: 58.812097967928366 | y_pred: 58.30464105970668\n",
            "Training batch[9]: last record -> y: 42.544398961449815 | y_pred: 56.89443960320705\n",
            "Training batch[10]: last record -> y: 61.12310136925453 | y_pred: 62.3498168540591\n",
            "Training batch[11]: last record -> y: 63.20160022404593 | y_pred: 66.63704940065941\n",
            "Training batch[12]: last record -> y: 64.93820087138647 | y_pred: 63.39077249193656\n",
            "Training batch[13]: last record -> y: 56.92869889453914 | y_pred: 61.56280515776143\n",
            "Training batch[14]: last record -> y: 54.76450035172343 | y_pred: 55.842021318473826\n",
            "Training batch[15]: last record -> y: 48.407398097782334 | y_pred: 56.153108034095794\n",
            "Training batch[16]: last record -> y: 64.4571009752251 | y_pred: 65.48830821730576\n",
            "Training batch[17]: last record -> y: 69.68309663972673 | y_pred: 67.7340864563887\n",
            "Training batch[18]: last record -> y: 59.64789988667326 | y_pred: 58.248340938155025\n",
            "Training batch[19]: last record -> y: 22.98349944379808 | y_pred: 24.079833753080123\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.84926916434165\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.69857909892835\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.132704250700726\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.60724204316591\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.83453695592786\n",
            "[Training] Epoch: 903 | L1Loss: 0.04984 | RMSE: 0.08148 | PLCC: 0.94691 | SROCC: 0.94719\n",
            "[Testing]  Epoch: 903 | L1Loss: 0.07640 | RMSE: 0.10450 | PLCC: 0.91604 | SROCC: 0.93115\n",
            "Training batch[0]: last record -> y: 49.50789903416921 | y_pred: 48.61073464646779\n",
            "Training batch[1]: last record -> y: 64.75720104616153 | y_pred: 64.62970643785366\n",
            "Training batch[2]: last record -> y: 29.695998828221605 | y_pred: 48.339815438304186\n",
            "Training batch[3]: last record -> y: 40.48429855540655 | y_pred: 40.20243129732796\n",
            "Training batch[4]: last record -> y: 55.66210214682451 | y_pred: 55.9119293128781\n",
            "Training batch[5]: last record -> y: 40.39899768001135 | y_pred: 59.50061259353083\n",
            "Training batch[6]: last record -> y: 59.352698135366836 | y_pred: 57.88028834118927\n",
            "Training batch[7]: last record -> y: 66.13350021295673 | y_pred: 66.59832919709697\n",
            "Training batch[8]: last record -> y: 69.90390053832061 | y_pred: 66.67745812543376\n",
            "Training batch[9]: last record -> y: 40.69929876537776 | y_pred: 55.607953683731466\n",
            "Training batch[10]: last record -> y: 64.53830115624851 | y_pred: 59.17865823376587\n",
            "Training batch[11]: last record -> y: 30.074199437670984 | y_pred: 28.834483947650085\n",
            "Training batch[12]: last record -> y: 27.831899552284597 | y_pred: 27.076564513922506\n",
            "Training batch[13]: last record -> y: 29.17069987919399 | y_pred: 41.74744107115623\n",
            "Training batch[14]: last record -> y: 25.895799721727116 | y_pred: 26.212382171865016\n",
            "Training batch[15]: last record -> y: 68.08779787986123 | y_pred: 66.74928942590418\n",
            "Training batch[16]: last record -> y: 56.15299868224588 | y_pred: 58.21326472417945\n",
            "Training batch[17]: last record -> y: 51.01290211208311 | y_pred: 52.616546991081805\n",
            "Training batch[18]: last record -> y: 26.304599953796185 | y_pred: 33.80299862066181\n",
            "Training batch[19]: last record -> y: 43.91579820049958 | y_pred: 41.88282509381236\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.76493798255672\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.84131220914492\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.99335959356961\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.841327213324575\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.93997811726797\n",
            "[Training] Epoch: 904 | L1Loss: 0.05176 | RMSE: 0.08305 | PLCC: 0.94506 | SROCC: 0.94557\n",
            "[Testing]  Epoch: 904 | L1Loss: 0.07634 | RMSE: 0.10390 | PLCC: 0.91719 | SROCC: 0.93218\n",
            "Training batch[0]: last record -> y: 69.23919566992163 | y_pred: 67.90812314175878\n",
            "Training batch[1]: last record -> y: 29.03989978959936 | y_pred: 29.75831490963401\n",
            "Training batch[2]: last record -> y: 65.2375033170647 | y_pred: 68.84966829937116\n",
            "Training batch[3]: last record -> y: 48.33070063999071 | y_pred: 50.35338824032419\n",
            "Training batch[4]: last record -> y: 64.97390103415273 | y_pred: 63.74163433484955\n",
            "Training batch[5]: last record -> y: 38.542598864858405 | y_pred: 35.49128504772534\n",
            "Training batch[6]: last record -> y: 53.02539747675837 | y_pred: 49.06037015593165\n",
            "Training batch[7]: last record -> y: 63.459201020136106 | y_pred: 60.815610399756224\n",
            "Training batch[8]: last record -> y: 52.48210210784259 | y_pred: 53.96408018888769\n",
            "Training batch[9]: last record -> y: 58.15560092073474 | y_pred: 57.62092183434606\n",
            "Training batch[10]: last record -> y: 27.960300333642863 | y_pred: 29.14299928668362\n",
            "Training batch[11]: last record -> y: 67.31629806509704 | y_pred: 68.27725960853104\n",
            "Training batch[12]: last record -> y: 67.38690076537137 | y_pred: 68.45233835270619\n",
            "Training batch[13]: last record -> y: 36.61619878460567 | y_pred: 40.423317209809284\n",
            "Training batch[14]: last record -> y: 43.03680069292295 | y_pred: 41.78313801769161\n",
            "Training batch[15]: last record -> y: 33.262500568553776 | y_pred: 46.636552632148664\n",
            "Training batch[16]: last record -> y: 55.43380121404971 | y_pred: 55.75708066093716\n",
            "Training batch[17]: last record -> y: 64.41010219337795 | y_pred: 66.50851273354647\n",
            "Training batch[18]: last record -> y: 28.126199954886943 | y_pred: 26.90973701006959\n",
            "Training batch[19]: last record -> y: 44.533298448275104 | y_pred: 45.56431432461841\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.26821042055019\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.00754471028506\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.028379677358316\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.71316684522458\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.136705559670673\n",
            "[Training] Epoch: 905 | L1Loss: 0.05507 | RMSE: 0.08615 | PLCC: 0.94117 | SROCC: 0.94166\n",
            "[Testing]  Epoch: 905 | L1Loss: 0.07783 | RMSE: 0.10559 | PLCC: 0.91677 | SROCC: 0.93264\n",
            "Training batch[0]: last record -> y: 40.39899768001135 | y_pred: 60.489114721988926\n",
            "Training batch[1]: last record -> y: 65.14040530680222 | y_pred: 65.72748322668531\n",
            "Training batch[2]: last record -> y: 65.84580192829299 | y_pred: 67.43879786684852\n",
            "Training batch[3]: last record -> y: 30.740801078231357 | y_pred: 30.280476425438337\n",
            "Training batch[4]: last record -> y: 54.441201607450694 | y_pred: 59.97799131056422\n",
            "Training batch[5]: last record -> y: 40.42119932177491 | y_pred: 39.62251109926797\n",
            "Training batch[6]: last record -> y: 54.441799826394345 | y_pred: 54.892844044983576\n",
            "Training batch[7]: last record -> y: 48.6593994359107 | y_pred: 46.44610512871418\n",
            "Training batch[8]: last record -> y: 57.950199551824426 | y_pred: 58.43504957319169\n",
            "Training batch[9]: last record -> y: 56.25930154528646 | y_pred: 54.61040108157749\n",
            "Training batch[10]: last record -> y: 54.171601053947825 | y_pred: 55.32878162713018\n",
            "Training batch[11]: last record -> y: 57.16640087017572 | y_pred: 57.19846347581688\n",
            "Training batch[12]: last record -> y: 47.797999535593135 | y_pred: 46.37245344156577\n",
            "Training batch[13]: last record -> y: 61.434799168743666 | y_pred: 59.91269860747252\n",
            "Training batch[14]: last record -> y: 49.39419884010499 | y_pred: 55.84083774551004\n",
            "Training batch[15]: last record -> y: 50.734000218875735 | y_pred: 60.8033758574893\n",
            "Training batch[16]: last record -> y: 51.931000946581435 | y_pred: 61.706239406309805\n",
            "Training batch[17]: last record -> y: 57.949199304020794 | y_pred: 67.96090470672789\n",
            "Training batch[18]: last record -> y: 58.169900283226525 | y_pred: 56.30697573561838\n",
            "Training batch[19]: last record -> y: 30.995199312422926 | y_pred: 29.253358620921404\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.08837004655743\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.04964356438677\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.06725571426489\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.36710683501519\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.915251332234874\n",
            "[Training] Epoch: 906 | L1Loss: 0.05009 | RMSE: 0.08240 | PLCC: 0.94616 | SROCC: 0.94641\n",
            "[Testing]  Epoch: 906 | L1Loss: 0.07718 | RMSE: 0.10519 | PLCC: 0.91665 | SROCC: 0.93247\n",
            "Training batch[0]: last record -> y: 39.65230143779252 | y_pred: 38.31260297840993\n",
            "Training batch[1]: last record -> y: 65.86109832235752 | y_pred: 65.52043836379539\n",
            "Training batch[2]: last record -> y: 61.28480059296953 | y_pred: 62.294478385540515\n",
            "Training batch[3]: last record -> y: 26.98159966879109 | y_pred: 25.408841863046973\n",
            "Training batch[4]: last record -> y: 43.67010067489571 | y_pred: 44.16090233150612\n",
            "Training batch[5]: last record -> y: 57.949199304020794 | y_pred: 67.73445953917076\n",
            "Training batch[6]: last record -> y: 50.21400001022266 | y_pred: 53.1245442260913\n",
            "Training batch[7]: last record -> y: 26.749900788091736 | y_pred: 26.395128008428856\n",
            "Training batch[8]: last record -> y: 59.325401983889606 | y_pred: 63.23880236663308\n",
            "Training batch[9]: last record -> y: 36.45529879426431 | y_pred: 38.05278941547442\n",
            "Training batch[10]: last record -> y: 58.27209923566443 | y_pred: 58.7522246138692\n",
            "Training batch[11]: last record -> y: 47.28070096087913 | y_pred: 47.02449922522169\n",
            "Training batch[12]: last record -> y: 49.50789903416921 | y_pred: 48.876713723999956\n",
            "Training batch[13]: last record -> y: 43.4957005554362 | y_pred: 45.34466183667951\n",
            "Training batch[14]: last record -> y: 51.931000946581435 | y_pred: 62.10122150443249\n",
            "Training batch[15]: last record -> y: 68.24639987323894 | y_pred: 70.2486997860251\n",
            "Training batch[16]: last record -> y: 54.76450035172343 | y_pred: 55.21803393301366\n",
            "Training batch[17]: last record -> y: 57.606300848766296 | y_pred: 62.91192717362196\n",
            "Training batch[18]: last record -> y: 31.0084992311688 | y_pred: 29.21876886186641\n",
            "Training batch[19]: last record -> y: 58.359600012981446 | y_pred: 55.816777122297935\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.11963004874053\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.86783470324201\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 34.959854354525305\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 37.84560625465667\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.172991880514786\n",
            "[Training] Epoch: 907 | L1Loss: 0.05328 | RMSE: 0.08413 | PLCC: 0.94301 | SROCC: 0.94318\n",
            "[Testing]  Epoch: 907 | L1Loss: 0.07772 | RMSE: 0.10425 | PLCC: 0.91630 | SROCC: 0.93114\n",
            "Training batch[0]: last record -> y: 48.23669664383465 | y_pred: 45.089707998603444\n",
            "Training batch[1]: last record -> y: 64.08940216365613 | y_pred: 61.64516639813246\n",
            "Training batch[2]: last record -> y: 64.4866034610859 | y_pred: 64.75743261478488\n",
            "Training batch[3]: last record -> y: 62.23139844929415 | y_pred: 63.72435352633215\n",
            "Training batch[4]: last record -> y: 43.297297704920425 | y_pred: 45.64216158494958\n",
            "Training batch[5]: last record -> y: 26.749900788091736 | y_pred: 24.777100144935503\n",
            "Training batch[6]: last record -> y: 67.31629806509704 | y_pred: 67.10321955307654\n",
            "Training batch[7]: last record -> y: 28.126199954886943 | y_pred: 28.00370980182811\n",
            "Training batch[8]: last record -> y: 25.608800967279734 | y_pred: 26.01461573303459\n",
            "Training batch[9]: last record -> y: 68.15290082533102 | y_pred: 64.09275347623293\n",
            "Training batch[10]: last record -> y: 57.104697480745926 | y_pred: 52.935822230523854\n",
            "Training batch[11]: last record -> y: 67.8711011081009 | y_pred: 64.98665982205307\n",
            "Training batch[12]: last record -> y: 53.90739733586133 | y_pred: 59.34215533054271\n",
            "Training batch[13]: last record -> y: 56.801496963241334 | y_pred: 54.18342713489301\n",
            "Training batch[14]: last record -> y: 60.20750154614984 | y_pred: 57.99090416983972\n",
            "Training batch[15]: last record -> y: 63.20890106814318 | y_pred: 63.50430865822591\n",
            "Training batch[16]: last record -> y: 55.51060159122949 | y_pred: 68.49006474092675\n",
            "Training batch[17]: last record -> y: 54.38009965319543 | y_pred: 63.48523319287756\n",
            "Training batch[18]: last record -> y: 54.171601053947825 | y_pred: 53.30175854757067\n",
            "Training batch[19]: last record -> y: 64.75720104616153 | y_pred: 61.81419219579175\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.10100194719155\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.212490828672344\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.28360176300578\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.7408764362234\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.416777912742987\n",
            "[Training] Epoch: 908 | L1Loss: 0.05291 | RMSE: 0.08477 | PLCC: 0.94337 | SROCC: 0.94477\n",
            "[Testing]  Epoch: 908 | L1Loss: 0.07698 | RMSE: 0.10201 | PLCC: 0.91619 | SROCC: 0.92826\n",
            "Training batch[0]: last record -> y: 38.59769772417644 | y_pred: 37.892982943632205\n",
            "Training batch[1]: last record -> y: 67.50070066259286 | y_pred: 65.82068959758317\n",
            "Training batch[2]: last record -> y: 64.93270111658194 | y_pred: 67.87053826769693\n",
            "Training batch[3]: last record -> y: 48.261201106908175 | y_pred: 48.99809749363612\n",
            "Training batch[4]: last record -> y: 71.94249883281668 | y_pred: 69.69446923211785\n",
            "Training batch[5]: last record -> y: 67.35350020768419 | y_pred: 68.27825342387291\n",
            "Training batch[6]: last record -> y: 68.86179992224993 | y_pred: 69.70136161489336\n",
            "Training batch[7]: last record -> y: 58.416900382336735 | y_pred: 59.06828040620053\n",
            "Training batch[8]: last record -> y: 49.146899631522956 | y_pred: 54.02011336327632\n",
            "Training batch[9]: last record -> y: 47.6147001052891 | y_pred: 47.04178968243173\n",
            "Training batch[10]: last record -> y: 63.25490281841758 | y_pred: 64.35951088163802\n",
            "Training batch[11]: last record -> y: 47.28070096087913 | y_pred: 48.59229277860277\n",
            "Training batch[12]: last record -> y: 67.4182976112204 | y_pred: 68.4373700141914\n",
            "Training batch[13]: last record -> y: 21.597000101274666 | y_pred: 15.799829874025846\n",
            "Training batch[14]: last record -> y: 54.821202502135066 | y_pred: 57.757836782900995\n",
            "Training batch[15]: last record -> y: 48.061598602274216 | y_pred: 59.45586195706869\n",
            "Training batch[16]: last record -> y: 63.3563009293664 | y_pred: 60.26406539863365\n",
            "Training batch[17]: last record -> y: 52.21930066641971 | y_pred: 48.60287739442833\n",
            "Training batch[18]: last record -> y: 40.77389924063573 | y_pred: 41.040482969573304\n",
            "Training batch[19]: last record -> y: 27.51029978197414 | y_pred: 29.4464571026586\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.43506702433456\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.8314303397666\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.97664032504781\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.94044677854856\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.122623600470348\n",
            "[Training] Epoch: 909 | L1Loss: 0.05262 | RMSE: 0.08245 | PLCC: 0.94611 | SROCC: 0.94592\n",
            "[Testing]  Epoch: 909 | L1Loss: 0.07743 | RMSE: 0.10464 | PLCC: 0.91792 | SROCC: 0.93479\n",
            "Training batch[0]: last record -> y: 57.16640087017572 | y_pred: 56.776008333518575\n",
            "Training batch[1]: last record -> y: 28.55800066006435 | y_pred: 29.34111669670881\n",
            "Training batch[2]: last record -> y: 66.99210199240883 | y_pred: 66.15730997016021\n",
            "Training batch[3]: last record -> y: 70.18830218633252 | y_pred: 64.36392033417428\n",
            "Training batch[4]: last record -> y: 59.64789988667326 | y_pred: 62.70706291526858\n",
            "Training batch[5]: last record -> y: 32.75839979725521 | y_pred: 33.326158622300795\n",
            "Training batch[6]: last record -> y: 39.02769814411886 | y_pred: 39.49952082230732\n",
            "Training batch[7]: last record -> y: 64.74600213023791 | y_pred: 61.75423200349883\n",
            "Training batch[8]: last record -> y: 47.01879845598717 | y_pred: 41.89269731449804\n",
            "Training batch[9]: last record -> y: 52.35879824837116 | y_pred: 51.6290515428891\n",
            "Training batch[10]: last record -> y: 42.995300057764894 | y_pred: 48.6511980471671\n",
            "Training batch[11]: last record -> y: 58.383400121492286 | y_pred: 57.29618543487004\n",
            "Training batch[12]: last record -> y: 31.0084992311688 | y_pred: 30.636757617383864\n",
            "Training batch[13]: last record -> y: 47.959997868779965 | y_pred: 45.8999215844683\n",
            "Training batch[14]: last record -> y: 63.97720073318192 | y_pred: 61.180614009847886\n",
            "Training batch[15]: last record -> y: 40.324099703396996 | y_pred: 40.724311392930304\n",
            "Training batch[16]: last record -> y: 38.79510193500403 | y_pred: 41.937722938700404\n",
            "Training batch[17]: last record -> y: 64.29779784351558 | y_pred: 62.517842403914756\n",
            "Training batch[18]: last record -> y: 26.044099725567833 | y_pred: 27.991249319341875\n",
            "Training batch[19]: last record -> y: 41.72359915164395 | y_pred: 43.96563852232907\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.80744866806458\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.80743258691018\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.35181832767432\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.425457457606285\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.845747128659553\n",
            "[Training] Epoch: 910 | L1Loss: 0.04959 | RMSE: 0.08159 | PLCC: 0.94698 | SROCC: 0.94729\n",
            "[Testing]  Epoch: 910 | L1Loss: 0.07880 | RMSE: 0.10690 | PLCC: 0.91807 | SROCC: 0.93318\n",
            "Training batch[0]: last record -> y: 69.13320156504551 | y_pred: 66.37015369732649\n",
            "Training batch[1]: last record -> y: 56.89680031667285 | y_pred: 59.76877227559953\n",
            "Training batch[2]: last record -> y: 65.81910077952853 | y_pred: 66.57693804551513\n",
            "Training batch[3]: last record -> y: 65.86109832235752 | y_pred: 63.90985607478888\n",
            "Training batch[4]: last record -> y: 61.12310136925453 | y_pred: 61.94226250942711\n",
            "Training batch[5]: last record -> y: 66.4921035235975 | y_pred: 64.47741147323131\n",
            "Training batch[6]: last record -> y: 42.00540208510495 | y_pred: 41.49537380029483\n",
            "Training batch[7]: last record -> y: 47.782002003196794 | y_pred: 46.34385310846676\n",
            "Training batch[8]: last record -> y: 71.1275027116335 | y_pred: 66.3222350734477\n",
            "Training batch[9]: last record -> y: 28.923200460239684 | y_pred: 28.52289791218402\n",
            "Training batch[10]: last record -> y: 32.16310119124324 | y_pred: 36.37309354109095\n",
            "Training batch[11]: last record -> y: 29.928600665740476 | y_pred: 29.143243720230487\n",
            "Training batch[12]: last record -> y: 56.15299868224588 | y_pred: 56.29882580656886\n",
            "Training batch[13]: last record -> y: 63.890597284280375 | y_pred: 60.93481356485586\n",
            "Training batch[14]: last record -> y: 66.95040034282079 | y_pred: 65.42828048416436\n",
            "Training batch[15]: last record -> y: 53.863000484795975 | y_pred: 57.33186308402014\n",
            "Training batch[16]: last record -> y: 34.00809927198486 | y_pred: 40.30353833938216\n",
            "Training batch[17]: last record -> y: 63.25490281841758 | y_pred: 64.38747600913825\n",
            "Training batch[18]: last record -> y: 48.53829869459196 | y_pred: 45.88154725745176\n",
            "Training batch[19]: last record -> y: 57.16640087017572 | y_pred: 55.01560436143632\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.764944415018476\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.56567785265793\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.39804973074956\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.480463046228806\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.621762779754363\n",
            "[Training] Epoch: 911 | L1Loss: 0.05177 | RMSE: 0.08490 | PLCC: 0.94348 | SROCC: 0.94304\n",
            "[Testing]  Epoch: 911 | L1Loss: 0.07766 | RMSE: 0.10389 | PLCC: 0.91314 | SROCC: 0.92814\n",
            "Training batch[0]: last record -> y: 28.55800066006435 | y_pred: 29.236259529448716\n",
            "Training batch[1]: last record -> y: 47.55979904417018 | y_pred: 47.568251298922405\n",
            "Training batch[2]: last record -> y: 65.45760286109589 | y_pred: 66.58276263963853\n",
            "Training batch[3]: last record -> y: 30.471100017513493 | y_pred: 53.48202185592413\n",
            "Training batch[4]: last record -> y: 52.99509736563937 | y_pred: 55.03458655608915\n",
            "Training batch[5]: last record -> y: 65.3453982143908 | y_pred: 65.33405456807759\n",
            "Training batch[6]: last record -> y: 37.76599810791879 | y_pred: 35.28173152474915\n",
            "Training batch[7]: last record -> y: 68.08779787986123 | y_pred: 62.64443003511451\n",
            "Training batch[8]: last record -> y: 48.6593994359107 | y_pred: 45.519619972196665\n",
            "Training batch[9]: last record -> y: 37.43249908741063 | y_pred: 49.49343886013344\n",
            "Training batch[10]: last record -> y: 67.8711011081009 | y_pred: 68.87544960610398\n",
            "Training batch[11]: last record -> y: 28.547899282928483 | y_pred: 29.32551556476841\n",
            "Training batch[12]: last record -> y: 29.01670111626305 | y_pred: 28.82309929439316\n",
            "Training batch[13]: last record -> y: 56.92869889453914 | y_pred: 60.233006257027\n",
            "Training batch[14]: last record -> y: 53.082598142956385 | y_pred: 52.25974476973374\n",
            "Training batch[15]: last record -> y: 56.801496963241334 | y_pred: 52.39753131685711\n",
            "Training batch[16]: last record -> y: 60.22169798925347 | y_pred: 54.325806459712794\n",
            "Training batch[17]: last record -> y: 31.007400888323332 | y_pred: 30.88959999988731\n",
            "Training batch[18]: last record -> y: 36.413502265865304 | y_pred: 39.00952322341686\n",
            "Training batch[19]: last record -> y: 56.25930154528646 | y_pred: 54.474248379741084\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.644739547853305\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.39269624486997\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.84778976771702\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.63438059933003\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.195374742972405\n",
            "[Training] Epoch: 912 | L1Loss: 0.05628 | RMSE: 0.08655 | PLCC: 0.94079 | SROCC: 0.94019\n",
            "[Testing]  Epoch: 912 | L1Loss: 0.07585 | RMSE: 0.10220 | PLCC: 0.91614 | SROCC: 0.93066\n",
            "Training batch[0]: last record -> y: 61.766399004917275 | y_pred: 55.69712368487512\n",
            "Training batch[1]: last record -> y: 61.54970223315695 | y_pred: 64.55793946200083\n",
            "Training batch[2]: last record -> y: 55.782900562440545 | y_pred: 51.308872542569816\n",
            "Training batch[3]: last record -> y: 31.31949991261473 | y_pred: 31.947093496948924\n",
            "Training batch[4]: last record -> y: 49.146899631522956 | y_pred: 55.3354327925897\n",
            "Training batch[5]: last record -> y: 54.441201607450694 | y_pred: 61.31579862618946\n",
            "Training batch[6]: last record -> y: 53.191200611076056 | y_pred: 59.63142313587582\n",
            "Training batch[7]: last record -> y: 64.75720104616153 | y_pred: 63.41275221376941\n",
            "Training batch[8]: last record -> y: 63.459201020136106 | y_pred: 61.45713589220418\n",
            "Training batch[9]: last record -> y: 43.31629919695854 | y_pred: 42.47915610993675\n",
            "Training batch[10]: last record -> y: 43.36610092401747 | y_pred: 48.29694951313763\n",
            "Training batch[11]: last record -> y: 33.643200189396794 | y_pred: 33.883602190857005\n",
            "Training batch[12]: last record -> y: 70.78559807172087 | y_pred: 67.62746197026013\n",
            "Training batch[13]: last record -> y: 37.98649968080997 | y_pred: 38.251880539398485\n",
            "Training batch[14]: last record -> y: 48.33070063999071 | y_pred: 48.96587086022009\n",
            "Training batch[15]: last record -> y: 42.995300057764894 | y_pred: 48.320141754012184\n",
            "Training batch[16]: last record -> y: 42.00540208510495 | y_pred: 41.550450145996706\n",
            "Training batch[17]: last record -> y: 48.261201106908175 | y_pred: 48.5185864155294\n",
            "Training batch[18]: last record -> y: 61.02579752021575 | y_pred: 60.7549169068227\n",
            "Training batch[19]: last record -> y: 48.559300682237335 | y_pred: 49.540289695360116\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.80021842719316\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.00420450066281\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.155344907979384\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.73977954763984\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.862935872568016\n",
            "[Training] Epoch: 913 | L1Loss: 0.05171 | RMSE: 0.08238 | PLCC: 0.94589 | SROCC: 0.94648\n",
            "[Testing]  Epoch: 913 | L1Loss: 0.07525 | RMSE: 0.10248 | PLCC: 0.91629 | SROCC: 0.93047\n",
            "Training batch[0]: last record -> y: 54.38009965319543 | y_pred: 62.680660875975946\n",
            "Training batch[1]: last record -> y: 38.3894998425161 | y_pred: 37.885378165716816\n",
            "Training batch[2]: last record -> y: 25.665500705518212 | y_pred: 25.716181267665064\n",
            "Training batch[3]: last record -> y: 52.48210210784259 | y_pred: 54.587372868477814\n",
            "Training batch[4]: last record -> y: 21.597000101274666 | y_pred: 15.555927005252926\n",
            "Training batch[5]: last record -> y: 68.87090185563989 | y_pred: 69.06379851888937\n",
            "Training batch[6]: last record -> y: 52.35879824837116 | y_pred: 52.30624503579452\n",
            "Training batch[7]: last record -> y: 56.25930154528646 | y_pred: 52.09764994962052\n",
            "Training batch[8]: last record -> y: 59.05309979644767 | y_pred: 58.47732049564547\n",
            "Training batch[9]: last record -> y: 65.22029969808841 | y_pred: 62.15739940921071\n",
            "Training batch[10]: last record -> y: 28.547899282928483 | y_pred: 29.712764235740508\n",
            "Training batch[11]: last record -> y: 65.59839658409192 | y_pred: 63.73403438128048\n",
            "Training batch[12]: last record -> y: 58.383400121492286 | y_pred: 57.07112003036036\n",
            "Training batch[13]: last record -> y: 21.73069952250026 | y_pred: 21.310209999805615\n",
            "Training batch[14]: last record -> y: 69.50440285204263 | y_pred: 64.99479045371731\n",
            "Training batch[15]: last record -> y: 33.262500568553776 | y_pred: 45.00536877624131\n",
            "Training batch[16]: last record -> y: 26.635299647352298 | y_pred: 38.91939156924013\n",
            "Training batch[17]: last record -> y: 46.12870012752876 | y_pred: 44.60753709755909\n",
            "Training batch[18]: last record -> y: 58.169900283226525 | y_pred: 58.066617460982116\n",
            "Training batch[19]: last record -> y: 53.90739733586133 | y_pred: 61.300135581804625\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.61693040755074\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.722533740374956\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.06858432532533\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.42224122672644\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.365524211295337\n",
            "[Training] Epoch: 914 | L1Loss: 0.05145 | RMSE: 0.08384 | PLCC: 0.94480 | SROCC: 0.94495\n",
            "[Testing]  Epoch: 914 | L1Loss: 0.07982 | RMSE: 0.10778 | PLCC: 0.91870 | SROCC: 0.93408\n",
            "Training batch[0]: last record -> y: 64.93270111658194 | y_pred: 67.84114513368604\n",
            "Training batch[1]: last record -> y: 30.995199312422926 | y_pred: 30.793552089007107\n",
            "Training batch[2]: last record -> y: 72.54129669802592 | y_pred: 66.11186141159715\n",
            "Training batch[3]: last record -> y: 50.734000218875735 | y_pred: 59.537306571638965\n",
            "Training batch[4]: last record -> y: 59.62590086745513 | y_pred: 58.2500198106743\n",
            "Training batch[5]: last record -> y: 25.608800967279734 | y_pred: 26.652773027693627\n",
            "Training batch[6]: last record -> y: 45.04560060831727 | y_pred: 44.79882082102233\n",
            "Training batch[7]: last record -> y: 34.8158990765744 | y_pred: 34.44316364744759\n",
            "Training batch[8]: last record -> y: 61.716499182816506 | y_pred: 53.147488817188105\n",
            "Training batch[9]: last record -> y: 64.41010219337795 | y_pred: 67.11178437590956\n",
            "Training batch[10]: last record -> y: 47.40309784324245 | y_pred: 46.93018647090116\n",
            "Training batch[11]: last record -> y: 59.64789988667326 | y_pred: 60.83976750989473\n",
            "Training batch[12]: last record -> y: 47.25970218946463 | y_pred: 48.946927260337816\n",
            "Training batch[13]: last record -> y: 60.46889749467823 | y_pred: 60.37939300552307\n",
            "Training batch[14]: last record -> y: 58.15560092073474 | y_pred: 55.9977576501376\n",
            "Training batch[15]: last record -> y: 45.92639920518661 | y_pred: 44.83298523354347\n",
            "Training batch[16]: last record -> y: 48.559300682237335 | y_pred: 48.08815823687996\n",
            "Training batch[17]: last record -> y: 41.65330199330322 | y_pred: 40.93251249082152\n",
            "Training batch[18]: last record -> y: 55.643100654786394 | y_pred: 54.142841517420266\n",
            "Training batch[19]: last record -> y: 66.13350021295673 | y_pred: 66.22080480019008\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.60895737119961\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.51710342538672\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.32813400347516\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.55549128019379\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.976333338900673\n",
            "[Training] Epoch: 915 | L1Loss: 0.05022 | RMSE: 0.08280 | PLCC: 0.94558 | SROCC: 0.94626\n",
            "[Testing]  Epoch: 915 | L1Loss: 0.07775 | RMSE: 0.10555 | PLCC: 0.91847 | SROCC: 0.93387\n",
            "Training batch[0]: last record -> y: 51.931000946581435 | y_pred: 61.71840962395913\n",
            "Training batch[1]: last record -> y: 22.983800965443066 | y_pred: 24.300972541714373\n",
            "Training batch[2]: last record -> y: 53.863000484795975 | y_pred: 55.07448068392273\n",
            "Training batch[3]: last record -> y: 55.44500012997332 | y_pred: 54.37714715324773\n",
            "Training batch[4]: last record -> y: 48.23669664383465 | y_pred: 46.22048331626229\n",
            "Training batch[5]: last record -> y: 47.44130023363323 | y_pred: 47.32991894203337\n",
            "Training batch[6]: last record -> y: 29.276899018788697 | y_pred: 34.00586559963881\n",
            "Training batch[7]: last record -> y: 73.27570372205287 | y_pred: 65.91837939432753\n",
            "Training batch[8]: last record -> y: 46.69590013245897 | y_pred: 54.764571108802784\n",
            "Training batch[9]: last record -> y: 46.50630171397677 | y_pred: 47.289214324018076\n",
            "Training batch[10]: last record -> y: 62.07709977283366 | y_pred: 58.273790973107225\n",
            "Training batch[11]: last record -> y: 61.97249829592852 | y_pred: 62.16992019602594\n",
            "Training batch[12]: last record -> y: 66.92800251097356 | y_pred: 68.28579548528614\n",
            "Training batch[13]: last record -> y: 52.99509736563937 | y_pred: 52.404459078172295\n",
            "Training batch[14]: last record -> y: 53.90739733586133 | y_pred: 60.0466063801548\n",
            "Training batch[15]: last record -> y: 39.45899952945217 | y_pred: 40.37766281246991\n",
            "Training batch[16]: last record -> y: 44.09870203440539 | y_pred: 45.51005168532913\n",
            "Training batch[17]: last record -> y: 55.71429835777349 | y_pred: 50.12819097034844\n",
            "Training batch[18]: last record -> y: 46.75879995977607 | y_pred: 46.19750656285669\n",
            "Training batch[19]: last record -> y: 56.977000249892626 | y_pred: 56.09282621871489\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.03112129074975\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.43977880257353\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.86830143400675\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.16312880212274\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.686782505250136\n",
            "[Training] Epoch: 916 | L1Loss: 0.04845 | RMSE: 0.08125 | PLCC: 0.94769 | SROCC: 0.94878\n",
            "[Testing]  Epoch: 916 | L1Loss: 0.07705 | RMSE: 0.10505 | PLCC: 0.91800 | SROCC: 0.93466\n",
            "Training batch[0]: last record -> y: 54.546501006456765 | y_pred: 50.322322666255786\n",
            "Training batch[1]: last record -> y: 45.65990070636735 | y_pred: 44.26853993036184\n",
            "Training batch[2]: last record -> y: 70.58750076313868 | y_pred: 66.35158961268803\n",
            "Training batch[3]: last record -> y: 65.26629501590105 | y_pred: 64.35572537789244\n",
            "Training batch[4]: last record -> y: 42.59540195074237 | y_pred: 41.653306817649536\n",
            "Training batch[5]: last record -> y: 46.59859789153563 | y_pred: 46.03266347345664\n",
            "Training batch[6]: last record -> y: 64.08940216365613 | y_pred: 61.749571684953935\n",
            "Training batch[7]: last record -> y: 33.64979989516223 | y_pred: 37.52621362278751\n",
            "Training batch[8]: last record -> y: 66.13350021295673 | y_pred: 66.24145943490043\n",
            "Training batch[9]: last record -> y: 63.49129900431694 | y_pred: 59.79422552678261\n",
            "Training batch[10]: last record -> y: 31.0084992311688 | y_pred: 30.992765429704605\n",
            "Training batch[11]: last record -> y: 49.39419884010499 | y_pred: 56.369898076551635\n",
            "Training batch[12]: last record -> y: 58.27209923566443 | y_pred: 60.5282916303363\n",
            "Training batch[13]: last record -> y: 45.44269898635889 | y_pred: 45.925759175241524\n",
            "Training batch[14]: last record -> y: 38.76359895353596 | y_pred: 54.776458298134685\n",
            "Training batch[15]: last record -> y: 68.86179992224993 | y_pred: 68.94183904392571\n",
            "Training batch[16]: last record -> y: 47.66609869097988 | y_pred: 47.6007866905029\n",
            "Training batch[17]: last record -> y: 40.69929876537776 | y_pred: 54.7055146773871\n",
            "Training batch[18]: last record -> y: 50.734000218875735 | y_pred: 59.14510651322735\n",
            "Training batch[19]: last record -> y: 41.07529866884761 | y_pred: 41.21842415923152\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.61431721996087\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.52302933078283\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.64509531865269\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.585008239093554\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.047760600366814\n",
            "[Training] Epoch: 917 | L1Loss: 0.04965 | RMSE: 0.08181 | PLCC: 0.94683 | SROCC: 0.94805\n",
            "[Testing]  Epoch: 917 | L1Loss: 0.07792 | RMSE: 0.10563 | PLCC: 0.91833 | SROCC: 0.93415\n",
            "Training batch[0]: last record -> y: 59.08630094782029 | y_pred: 58.84257497174576\n",
            "Training batch[1]: last record -> y: 35.46180025113438 | y_pred: 51.06261539279285\n",
            "Training batch[2]: last record -> y: 43.48559998235805 | y_pred: 41.21977175997017\n",
            "Training batch[3]: last record -> y: 55.71429835777349 | y_pred: 49.46530327239657\n",
            "Training batch[4]: last record -> y: 64.42900076602791 | y_pred: 59.1707720356485\n",
            "Training batch[5]: last record -> y: 54.38009965319543 | y_pred: 63.606224582346385\n",
            "Training batch[6]: last record -> y: 27.66959969745278 | y_pred: 28.663399762227698\n",
            "Training batch[7]: last record -> y: 67.19999594025103 | y_pred: 64.20623496659732\n",
            "Training batch[8]: last record -> y: 49.50770284408554 | y_pred: 49.77121507253287\n",
            "Training batch[9]: last record -> y: 31.31949991261473 | y_pred: 32.393096273634\n",
            "Training batch[10]: last record -> y: 51.01290211208311 | y_pred: 51.58883900819842\n",
            "Training batch[11]: last record -> y: 70.18830218633252 | y_pred: 64.510905301614\n",
            "Training batch[12]: last record -> y: 53.63130321221206 | y_pred: 54.032412230160844\n",
            "Training batch[13]: last record -> y: 64.93820087138647 | y_pred: 64.9247924048484\n",
            "Training batch[14]: last record -> y: 62.94929977644574 | y_pred: 62.72937069265117\n",
            "Training batch[15]: last record -> y: 44.533298448275104 | y_pred: 45.201980186157016\n",
            "Training batch[16]: last record -> y: 27.261600708901653 | y_pred: 26.298890741955603\n",
            "Training batch[17]: last record -> y: 68.06819816887946 | y_pred: 67.41353758951823\n",
            "Training batch[18]: last record -> y: 63.8380022607023 | y_pred: 67.0120201102477\n",
            "Training batch[19]: last record -> y: 61.352299630444804 | y_pred: 63.65987774588393\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.28165602759691\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.990494016129105\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.48949391483234\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.21607921921304\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.103333853739628\n",
            "[Training] Epoch: 918 | L1Loss: 0.05157 | RMSE: 0.08462 | PLCC: 0.94376 | SROCC: 0.94441\n",
            "[Testing]  Epoch: 918 | L1Loss: 0.07870 | RMSE: 0.10760 | PLCC: 0.91751 | SROCC: 0.93261\n",
            "Training batch[0]: last record -> y: 27.261600708901653 | y_pred: 27.449660964965574\n",
            "Training batch[1]: last record -> y: 68.93579896233337 | y_pred: 65.71799212935889\n",
            "Training batch[2]: last record -> y: 52.48210210784259 | y_pred: 53.91335701168168\n",
            "Training batch[3]: last record -> y: 53.70280002467098 | y_pred: 53.67787745158307\n",
            "Training batch[4]: last record -> y: 22.563999213620633 | y_pred: 24.501578108353982\n",
            "Training batch[5]: last record -> y: 39.47439884290486 | y_pred: 39.109268191693445\n",
            "Training batch[6]: last record -> y: 68.96460030986236 | y_pred: 67.25911026382255\n",
            "Training batch[7]: last record -> y: 63.439498389766186 | y_pred: 67.03292882719757\n",
            "Training batch[8]: last record -> y: 58.27209923566443 | y_pred: 58.52503971320971\n",
            "Training batch[9]: last record -> y: 54.18170323514141 | y_pred: 52.55641955478313\n",
            "Training batch[10]: last record -> y: 50.734000218875735 | y_pred: 59.005731148049335\n",
            "Training batch[11]: last record -> y: 65.29879824517275 | y_pred: 66.22816353644316\n",
            "Training batch[12]: last record -> y: 48.79289874727124 | y_pred: 54.511215737474004\n",
            "Training batch[13]: last record -> y: 73.29170125444921 | y_pred: 70.44924143007597\n",
            "Training batch[14]: last record -> y: 65.2375033170647 | y_pred: 65.31207163001386\n",
            "Training batch[15]: last record -> y: 25.810800367976896 | y_pred: 19.125222946669176\n",
            "Training batch[16]: last record -> y: 52.90630044727777 | y_pred: 54.1699929385079\n",
            "Training batch[17]: last record -> y: 61.02579752021575 | y_pred: 61.00780270844302\n",
            "Training batch[18]: last record -> y: 61.716499182816506 | y_pred: 53.521977092144425\n",
            "Training batch[19]: last record -> y: 57.24819927014278 | y_pred: 53.453188346086336\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.42693141447046\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.193600296599584\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.20716481991542\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.1199894973314\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.086698951746683\n",
            "[Training] Epoch: 919 | L1Loss: 0.04885 | RMSE: 0.08219 | PLCC: 0.94672 | SROCC: 0.94860\n",
            "[Testing]  Epoch: 919 | L1Loss: 0.07601 | RMSE: 0.10246 | PLCC: 0.91673 | SROCC: 0.93317\n",
            "Training batch[0]: last record -> y: 65.22029969808841 | y_pred: 60.18400376334171\n",
            "Training batch[1]: last record -> y: 45.44269898635889 | y_pred: 44.62890251929389\n",
            "Training batch[2]: last record -> y: 53.03879951083468 | y_pred: 53.45353569902136\n",
            "Training batch[3]: last record -> y: 38.522899450719365 | y_pred: 41.75220430908928\n",
            "Training batch[4]: last record -> y: 49.146899631522956 | y_pred: 54.9161038267066\n",
            "Training batch[5]: last record -> y: 56.724497179747004 | y_pred: 60.82500501015625\n",
            "Training batch[6]: last record -> y: 42.995300057764894 | y_pred: 48.70689994977511\n",
            "Training batch[7]: last record -> y: 28.777399065763746 | y_pred: 27.35181839725442\n",
            "Training batch[8]: last record -> y: 50.21070015733994 | y_pred: 49.371736683640165\n",
            "Training batch[9]: last record -> y: 33.94779976733412 | y_pred: 34.38461377239548\n",
            "Training batch[10]: last record -> y: 61.97249829592852 | y_pred: 63.957909780364616\n",
            "Training batch[11]: last record -> y: 68.15290082533102 | y_pred: 63.610733738039926\n",
            "Training batch[12]: last record -> y: 65.45760286109589 | y_pred: 64.73698703508171\n",
            "Training batch[13]: last record -> y: 41.601200661165194 | y_pred: 42.33198620922144\n",
            "Training batch[14]: last record -> y: 69.85279784587078 | y_pred: 58.97769526346974\n",
            "Training batch[15]: last record -> y: 40.39899768001135 | y_pred: 60.599412143782274\n",
            "Training batch[16]: last record -> y: 48.25579783903004 | y_pred: 48.97757472439184\n",
            "Training batch[17]: last record -> y: 26.82169912219564 | y_pred: 32.47539881779147\n",
            "Training batch[18]: last record -> y: 26.635299647352298 | y_pred: 39.49439254216941\n",
            "Training batch[19]: last record -> y: 35.46180025113438 | y_pred: 50.28872270225406\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.25223218553913\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.020314601139944\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.63768144491405\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.27565989626214\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.23380895016186\n",
            "[Training] Epoch: 920 | L1Loss: 0.04962 | RMSE: 0.08257 | PLCC: 0.94629 | SROCC: 0.94705\n",
            "[Testing]  Epoch: 920 | L1Loss: 0.07647 | RMSE: 0.10260 | PLCC: 0.91576 | SROCC: 0.93048\n",
            "Training batch[0]: last record -> y: 38.542598864858405 | y_pred: 38.297671626550255\n",
            "Training batch[1]: last record -> y: 69.50440285204263 | y_pred: 63.68558507930652\n",
            "Training batch[2]: last record -> y: 54.171601053947825 | y_pred: 54.463126653358586\n",
            "Training batch[3]: last record -> y: 60.22169798925347 | y_pred: 54.83955753176633\n",
            "Training batch[4]: last record -> y: 55.70909771244078 | y_pred: 56.77268918325058\n",
            "Training batch[5]: last record -> y: 25.27400017732296 | y_pred: 46.71871768243602\n",
            "Training batch[6]: last record -> y: 42.59540195074237 | y_pred: 41.64291035133044\n",
            "Training batch[7]: last record -> y: 63.25490281841758 | y_pred: 62.74689915094632\n",
            "Training batch[8]: last record -> y: 61.18830080165071 | y_pred: 62.11405426564306\n",
            "Training batch[9]: last record -> y: 53.03879951083468 | y_pred: 52.98430047857573\n",
            "Training batch[10]: last record -> y: 65.45760286109589 | y_pred: 65.29831581054077\n",
            "Training batch[11]: last record -> y: 36.61619878460567 | y_pred: 42.08630798100285\n",
            "Training batch[12]: last record -> y: 43.297297704920425 | y_pred: 44.58309695910316\n",
            "Training batch[13]: last record -> y: 56.49319871979219 | y_pred: 55.36149069517819\n",
            "Training batch[14]: last record -> y: 40.69929876537776 | y_pred: 54.61493596711807\n",
            "Training batch[15]: last record -> y: 67.31629806509704 | y_pred: 67.80479529228205\n",
            "Training batch[16]: last record -> y: 36.413502265865304 | y_pred: 36.53352396172386\n",
            "Training batch[17]: last record -> y: 57.950199551824426 | y_pred: 58.11288615841954\n",
            "Training batch[18]: last record -> y: 48.407398097782334 | y_pred: 56.59212998165617\n",
            "Training batch[19]: last record -> y: 49.146899631522956 | y_pred: 54.85994521931366\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.24668257915596\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.69327392609205\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.849061787029996\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.81624382869268\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.413095730414426\n",
            "[Training] Epoch: 921 | L1Loss: 0.04899 | RMSE: 0.08164 | PLCC: 0.94715 | SROCC: 0.94791\n",
            "[Testing]  Epoch: 921 | L1Loss: 0.07530 | RMSE: 0.10301 | PLCC: 0.91770 | SROCC: 0.93312\n",
            "Training batch[0]: last record -> y: 44.76750116471442 | y_pred: 43.029787701489454\n",
            "Training batch[1]: last record -> y: 63.3563009293664 | y_pred: 62.44195865253573\n",
            "Training batch[2]: last record -> y: 64.53830115624851 | y_pred: 61.96133797477546\n",
            "Training batch[3]: last record -> y: 65.14040530680222 | y_pred: 65.61696066873037\n",
            "Training batch[4]: last record -> y: 28.923200460239684 | y_pred: 27.786808799406913\n",
            "Training batch[5]: last record -> y: 67.37490100795867 | y_pred: 64.62602385349624\n",
            "Training batch[6]: last record -> y: 51.776001131789144 | y_pred: 49.32792197036406\n",
            "Training batch[7]: last record -> y: 62.20869829174421 | y_pred: 59.09767997267318\n",
            "Training batch[8]: last record -> y: 62.07709977283366 | y_pred: 59.90201428848968\n",
            "Training batch[9]: last record -> y: 57.950199551824426 | y_pred: 57.398300765305066\n",
            "Training batch[10]: last record -> y: 38.89670106038284 | y_pred: 37.64718571487106\n",
            "Training batch[11]: last record -> y: 38.542598864858405 | y_pred: 38.55703813339346\n",
            "Training batch[12]: last record -> y: 55.8553011357767 | y_pred: 48.95465586314208\n",
            "Training batch[13]: last record -> y: 63.999000346085495 | y_pred: 61.62117331576883\n",
            "Training batch[14]: last record -> y: 64.8636003961285 | y_pred: 63.926506502053826\n",
            "Training batch[15]: last record -> y: 32.16310119124324 | y_pred: 38.32102789519968\n",
            "Training batch[16]: last record -> y: 36.49660163122326 | y_pred: 37.16544739688004\n",
            "Training batch[17]: last record -> y: 70.7566005341082 | y_pred: 65.5777676792286\n",
            "Training batch[18]: last record -> y: 34.048697754381124 | y_pred: 35.17467767179812\n",
            "Training batch[19]: last record -> y: 48.045601069877875 | y_pred: 45.58333833027268\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.26149508432661\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.553987161116765\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.95614971811233\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.70557669420168\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.476413265717042\n",
            "[Training] Epoch: 922 | L1Loss: 0.05035 | RMSE: 0.08257 | PLCC: 0.94579 | SROCC: 0.94587\n",
            "[Testing]  Epoch: 922 | L1Loss: 0.07799 | RMSE: 0.10547 | PLCC: 0.91770 | SROCC: 0.93462\n",
            "Training batch[0]: last record -> y: 40.324099703396996 | y_pred: 41.40661226047291\n",
            "Training batch[1]: last record -> y: 63.49129900431694 | y_pred: 61.59254886093822\n",
            "Training batch[2]: last record -> y: 39.32759880874073 | y_pred: 55.54565850781978\n",
            "Training batch[3]: last record -> y: 28.114999430847888 | y_pred: 27.07749802493538\n",
            "Training batch[4]: last record -> y: 53.63130321221206 | y_pred: 53.73599152735096\n",
            "Training batch[5]: last record -> y: 54.38159841678544 | y_pred: 50.796848586498754\n",
            "Training batch[6]: last record -> y: 42.74340003090924 | y_pred: 43.092388419334725\n",
            "Training batch[7]: last record -> y: 54.23470028757947 | y_pred: 55.07388568120996\n",
            "Training batch[8]: last record -> y: 48.53829869459196 | y_pred: 47.294566132202135\n",
            "Training batch[9]: last record -> y: 60.41220177672835 | y_pred: 59.038601027641334\n",
            "Training batch[10]: last record -> y: 69.6393977107623 | y_pred: 65.88854885291698\n",
            "Training batch[11]: last record -> y: 69.7550951842029 | y_pred: 66.62419734206355\n",
            "Training batch[12]: last record -> y: 38.76359895353596 | y_pred: 54.647722224707195\n",
            "Training batch[13]: last record -> y: 43.48930025598531 | y_pred: 43.98174540657533\n",
            "Training batch[14]: last record -> y: 30.471100017513493 | y_pred: 52.1928793297418\n",
            "Training batch[15]: last record -> y: 33.93240045388143 | y_pred: 31.521081203297513\n",
            "Training batch[16]: last record -> y: 53.082598142956385 | y_pred: 53.074123374588\n",
            "Training batch[17]: last record -> y: 50.21070015733994 | y_pred: 48.76782501133198\n",
            "Training batch[18]: last record -> y: 28.118200384631052 | y_pred: 29.73224896646832\n",
            "Training batch[19]: last record -> y: 57.412301018325024 | y_pred: 60.327547363739995\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 47.909670287972176\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 48.16189997849301\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.79694483529818\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 40.133200319523894\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.00596568008325\n",
            "[Training] Epoch: 923 | L1Loss: 0.05358 | RMSE: 0.08551 | PLCC: 0.94172 | SROCC: 0.94129\n",
            "[Testing]  Epoch: 923 | L1Loss: 0.08663 | RMSE: 0.11596 | PLCC: 0.91901 | SROCC: 0.93444\n",
            "Training batch[0]: last record -> y: 47.55770206163652 | y_pred: 40.73218311800872\n",
            "Training batch[1]: last record -> y: 64.8636003961285 | y_pred: 64.37426051645298\n",
            "Training batch[2]: last record -> y: 29.17069987919399 | y_pred: 41.75264654083526\n",
            "Training batch[3]: last record -> y: 55.84130088275674 | y_pred: 54.100364756190174\n",
            "Training batch[4]: last record -> y: 25.520900573275895 | y_pred: 23.97213102554909\n",
            "Training batch[5]: last record -> y: 48.6593994359107 | y_pred: 46.67006414980119\n",
            "Training batch[6]: last record -> y: 33.94779976733412 | y_pred: 34.063889620942064\n",
            "Training batch[7]: last record -> y: 61.653901681202115 | y_pred: 66.43255500885721\n",
            "Training batch[8]: last record -> y: 55.057096956017176 | y_pred: 60.452793826662855\n",
            "Training batch[9]: last record -> y: 26.044099725567833 | y_pred: 27.33559572869649\n",
            "Training batch[10]: last record -> y: 56.38050198976248 | y_pred: 59.787986038875715\n",
            "Training batch[11]: last record -> y: 37.4706001665287 | y_pred: 36.78274808448748\n",
            "Training batch[12]: last record -> y: 24.91699975574693 | y_pred: 25.033909748229235\n",
            "Training batch[13]: last record -> y: 57.949199304020794 | y_pred: 66.92407549306927\n",
            "Training batch[14]: last record -> y: 34.00809927198486 | y_pred: 39.1357072176412\n",
            "Training batch[15]: last record -> y: 58.416900382336735 | y_pred: 61.07692594251262\n",
            "Training batch[16]: last record -> y: 64.07659834852348 | y_pred: 63.920160878527895\n",
            "Training batch[17]: last record -> y: 64.74639772663613 | y_pred: 62.393847054804155\n",
            "Training batch[18]: last record -> y: 28.55800066006435 | y_pred: 27.71006390220981\n",
            "Training batch[19]: last record -> y: 26.202400197300562 | y_pred: 27.532405740869024\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.13834882016829\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.6125272336227\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.035588858875485\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.13889932678944\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.167408007356727\n",
            "[Training] Epoch: 924 | L1Loss: 0.05400 | RMSE: 0.08473 | PLCC: 0.94292 | SROCC: 0.94162\n",
            "[Testing]  Epoch: 924 | L1Loss: 0.07538 | RMSE: 0.10292 | PLCC: 0.91678 | SROCC: 0.93080\n",
            "Training batch[0]: last record -> y: 68.25509656153804 | y_pred: 69.29946140414813\n",
            "Training batch[1]: last record -> y: 32.369998911570406 | y_pred: 33.768218299927184\n",
            "Training batch[2]: last record -> y: 26.76479917358489 | y_pred: 27.335073895236235\n",
            "Training batch[3]: last record -> y: 43.48930025598531 | y_pred: 43.94295444593354\n",
            "Training batch[4]: last record -> y: 43.48930025598531 | y_pred: 43.571593955507865\n",
            "Training batch[5]: last record -> y: 40.03480134387053 | y_pred: 39.0022014738189\n",
            "Training batch[6]: last record -> y: 54.18170323514141 | y_pred: 53.847810226350475\n",
            "Training batch[7]: last record -> y: 63.38790039776086 | y_pred: 63.810117538974055\n",
            "Training batch[8]: last record -> y: 38.76359895353596 | y_pred: 54.588572522595996\n",
            "Training batch[9]: last record -> y: 66.95040034282079 | y_pred: 64.89546037922423\n",
            "Training batch[10]: last record -> y: 38.469501977536765 | y_pred: 38.07439283829433\n",
            "Training batch[11]: last record -> y: 48.22529832159648 | y_pred: 49.321241858826625\n",
            "Training batch[12]: last record -> y: 28.55800066006435 | y_pred: 28.786809757318167\n",
            "Training batch[13]: last record -> y: 49.09339763083676 | y_pred: 46.273752140209695\n",
            "Training batch[14]: last record -> y: 60.352402395979425 | y_pred: 54.288244099267104\n",
            "Training batch[15]: last record -> y: 60.22169798925347 | y_pred: 55.58659469445843\n",
            "Training batch[16]: last record -> y: 61.434799168743666 | y_pred: 60.14908514467925\n",
            "Training batch[17]: last record -> y: 40.63280158382156 | y_pred: 36.57418837685316\n",
            "Training batch[18]: last record -> y: 56.90710190418099 | y_pred: 57.5302852319212\n",
            "Training batch[19]: last record -> y: 26.46649938788346 | y_pred: 34.733368984082176\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.336594075370954\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.60797626692772\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.05079022027553\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.584998590400915\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.39319505367007\n",
            "[Training] Epoch: 925 | L1Loss: 0.04863 | RMSE: 0.08154 | PLCC: 0.94753 | SROCC: 0.94837\n",
            "[Testing]  Epoch: 925 | L1Loss: 0.07529 | RMSE: 0.10228 | PLCC: 0.91704 | SROCC: 0.93230\n",
            "Training batch[0]: last record -> y: 55.6992978569499 | y_pred: 52.632563820863425\n",
            "Training batch[1]: last record -> y: 62.41920060282996 | y_pred: 59.013971131563494\n",
            "Training batch[2]: last record -> y: 66.22540401034826 | y_pred: 64.9980002521354\n",
            "Training batch[3]: last record -> y: 50.61160012028154 | y_pred: 56.94032878540065\n",
            "Training batch[4]: last record -> y: 63.52500188770682 | y_pred: 60.608407941553196\n",
            "Training batch[5]: last record -> y: 64.93270111658194 | y_pred: 67.19120598125642\n",
            "Training batch[6]: last record -> y: 43.36610092401747 | y_pred: 49.00324667927475\n",
            "Training batch[7]: last record -> y: 67.50669893318377 | y_pred: 67.28325450903753\n",
            "Training batch[8]: last record -> y: 39.99449875471521 | y_pred: 46.88052143365462\n",
            "Training batch[9]: last record -> y: 52.83100204991888 | y_pred: 51.28081414437406\n",
            "Training batch[10]: last record -> y: 48.407398097782334 | y_pred: 56.0117353895414\n",
            "Training batch[11]: last record -> y: 61.25039978747873 | y_pred: 52.99362754812728\n",
            "Training batch[12]: last record -> y: 40.77389924063573 | y_pred: 40.75846776487424\n",
            "Training batch[13]: last record -> y: 65.00969768384539 | y_pred: 63.94750527346832\n",
            "Training batch[14]: last record -> y: 45.98249831230828 | y_pred: 45.42768401249634\n",
            "Training batch[15]: last record -> y: 48.045601069877875 | y_pred: 45.26158498493771\n",
            "Training batch[16]: last record -> y: 46.11660066695879 | y_pred: 39.36214273650569\n",
            "Training batch[17]: last record -> y: 64.4866034610859 | y_pred: 62.90768174886057\n",
            "Training batch[18]: last record -> y: 63.36859979625092 | y_pred: 59.624997106577894\n",
            "Training batch[19]: last record -> y: 57.949199304020794 | y_pred: 67.3766667187117\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.88703897167909\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.34462163379351\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.36878846220475\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.31159484388263\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.285753644898023\n",
            "[Training] Epoch: 926 | L1Loss: 0.04904 | RMSE: 0.08169 | PLCC: 0.94765 | SROCC: 0.94916\n",
            "[Testing]  Epoch: 926 | L1Loss: 0.07536 | RMSE: 0.10142 | PLCC: 0.91818 | SROCC: 0.93330\n",
            "Training batch[0]: last record -> y: 32.369998911570406 | y_pred: 33.397831523400384\n",
            "Training batch[1]: last record -> y: 70.23500185870785 | y_pred: 68.39869483786129\n",
            "Training batch[2]: last record -> y: 60.20750154614984 | y_pred: 57.18735139812702\n",
            "Training batch[3]: last record -> y: 60.46889749467823 | y_pred: 60.09069125682481\n",
            "Training batch[4]: last record -> y: 69.23919566992163 | y_pred: 68.55285200016306\n",
            "Training batch[5]: last record -> y: 34.47520052324171 | y_pred: 36.9466311289699\n",
            "Training batch[6]: last record -> y: 58.169900283226525 | y_pred: 56.52486251280425\n",
            "Training batch[7]: last record -> y: 53.64059811945481 | y_pred: 55.783289726377006\n",
            "Training batch[8]: last record -> y: 71.1275027116335 | y_pred: 66.1756296212518\n",
            "Training batch[9]: last record -> y: 58.14030131043933 | y_pred: 63.30065370268335\n",
            "Training batch[10]: last record -> y: 73.29170125444921 | y_pred: 71.14652993351865\n",
            "Training batch[11]: last record -> y: 43.297297704920425 | y_pred: 43.585750195725495\n",
            "Training batch[12]: last record -> y: 53.03879951083468 | y_pred: 53.12483690310137\n",
            "Training batch[13]: last record -> y: 58.50000135581013 | y_pred: 63.79910838067235\n",
            "Training batch[14]: last record -> y: 33.63470129929681 | y_pred: 33.71060756429199\n",
            "Training batch[15]: last record -> y: 59.08630094782029 | y_pred: 57.311899938948955\n",
            "Training batch[16]: last record -> y: 67.08080242384403 | y_pred: 65.29092169574801\n",
            "Training batch[17]: last record -> y: 46.50630171397677 | y_pred: 47.165029217285564\n",
            "Training batch[18]: last record -> y: 30.96270010343983 | y_pred: 28.741725436902243\n",
            "Training batch[19]: last record -> y: 48.53829869459196 | y_pred: 48.397328078897544\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.7333079599689\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.71041804479705\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.07287462346551\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.80947044645973\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.401644340366744\n",
            "[Training] Epoch: 927 | L1Loss: 0.04886 | RMSE: 0.08161 | PLCC: 0.94713 | SROCC: 0.94773\n",
            "[Testing]  Epoch: 927 | L1Loss: 0.07513 | RMSE: 0.10260 | PLCC: 0.91754 | SROCC: 0.93276\n",
            "Training batch[0]: last record -> y: 59.44990228124834 | y_pred: 60.23550526842064\n",
            "Training batch[1]: last record -> y: 58.21779960972003 | y_pred: 59.04363764519917\n",
            "Training batch[2]: last record -> y: 63.66260189343916 | y_pred: 65.01879318477359\n",
            "Training batch[3]: last record -> y: 28.118200384631052 | y_pred: 28.079596769438012\n",
            "Training batch[4]: last record -> y: 37.76599810791879 | y_pred: 36.79861535953319\n",
            "Training batch[5]: last record -> y: 60.352402395979425 | y_pred: 54.52545077534819\n",
            "Training batch[6]: last record -> y: 53.30369793479122 | y_pred: 60.42093062733625\n",
            "Training batch[7]: last record -> y: 30.71260036181917 | y_pred: 31.095149315418098\n",
            "Training batch[8]: last record -> y: 71.94249883281668 | y_pred: 70.5305027194861\n",
            "Training batch[9]: last record -> y: 39.65230143779252 | y_pred: 39.72879305680783\n",
            "Training batch[10]: last record -> y: 62.23139844929415 | y_pred: 59.866825506433315\n",
            "Training batch[11]: last record -> y: 54.171601053947825 | y_pred: 53.634722065637334\n",
            "Training batch[12]: last record -> y: 48.061598602274216 | y_pred: 59.23497765270281\n",
            "Training batch[13]: last record -> y: 57.412301018325024 | y_pred: 56.00024379660772\n",
            "Training batch[14]: last record -> y: 60.22169798925347 | y_pred: 56.06720572352606\n",
            "Training batch[15]: last record -> y: 53.07020278914547 | y_pred: 59.677965212938034\n",
            "Training batch[16]: last record -> y: 63.38790039776086 | y_pred: 64.82901626547755\n",
            "Training batch[17]: last record -> y: 54.76450035172343 | y_pred: 55.16828205753336\n",
            "Training batch[18]: last record -> y: 33.643200189396794 | y_pred: 34.04357751482041\n",
            "Training batch[19]: last record -> y: 42.59540195074237 | y_pred: 40.76231276889109\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.730260581210246\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.21941858998753\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.37881843205707\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.21473965905159\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.62693849929775\n",
            "[Training] Epoch: 928 | L1Loss: 0.05163 | RMSE: 0.08304 | PLCC: 0.94504 | SROCC: 0.94508\n",
            "[Testing]  Epoch: 928 | L1Loss: 0.07548 | RMSE: 0.10209 | PLCC: 0.91676 | SROCC: 0.93257\n",
            "Training batch[0]: last record -> y: 34.048697754381124 | y_pred: 34.55504023860294\n",
            "Training batch[1]: last record -> y: 28.55800066006435 | y_pred: 29.409730962241667\n",
            "Training batch[2]: last record -> y: 63.49129900431694 | y_pred: 62.217706954438654\n",
            "Training batch[3]: last record -> y: 69.85279784587078 | y_pred: 61.646697324031265\n",
            "Training batch[4]: last record -> y: 40.324099703396996 | y_pred: 40.67617245723625\n",
            "Training batch[5]: last record -> y: 39.04919864755061 | y_pred: 38.25589439553653\n",
            "Training batch[6]: last record -> y: 39.67210055508883 | y_pred: 39.16377848076047\n",
            "Training batch[7]: last record -> y: 29.01670111626305 | y_pred: 28.960763624743095\n",
            "Training batch[8]: last record -> y: 43.297297704920425 | y_pred: 43.51478888570807\n",
            "Training batch[9]: last record -> y: 36.69869993101997 | y_pred: 36.39887645593922\n",
            "Training batch[10]: last record -> y: 37.98649968080997 | y_pred: 38.951584432231925\n",
            "Training batch[11]: last record -> y: 27.831899552284597 | y_pred: 28.046464767029306\n",
            "Training batch[12]: last record -> y: 62.94929977644574 | y_pred: 60.988254457155335\n",
            "Training batch[13]: last record -> y: 39.941398782888996 | y_pred: 56.900614766496346\n",
            "Training batch[14]: last record -> y: 63.459201020136106 | y_pred: 62.39011622698354\n",
            "Training batch[15]: last record -> y: 25.033300272477504 | y_pred: 24.10539474799768\n",
            "Training batch[16]: last record -> y: 25.810800367976896 | y_pred: 18.409169444665267\n",
            "Training batch[17]: last record -> y: 28.55800066006435 | y_pred: 28.418959782897844\n",
            "Training batch[18]: last record -> y: 36.81190160929782 | y_pred: 34.29864231286183\n",
            "Training batch[19]: last record -> y: 57.104697480745926 | y_pred: 54.32417583065671\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.08991881557961\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.13672140734309\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.15698694769708\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.33091795900873\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.554435006573442\n",
            "[Training] Epoch: 929 | L1Loss: 0.05019 | RMSE: 0.08139 | PLCC: 0.94776 | SROCC: 0.94756\n",
            "[Testing]  Epoch: 929 | L1Loss: 0.07622 | RMSE: 0.10433 | PLCC: 0.91752 | SROCC: 0.93278\n",
            "Training batch[0]: last record -> y: 28.114999430847888 | y_pred: 28.144050036270073\n",
            "Training batch[1]: last record -> y: 63.439498389766186 | y_pred: 66.41564406689099\n",
            "Training batch[2]: last record -> y: 33.643200189396794 | y_pred: 33.6878205685083\n",
            "Training batch[3]: last record -> y: 28.55800066006435 | y_pred: 28.022478117127434\n",
            "Training batch[4]: last record -> y: 67.37490100795867 | y_pred: 64.47785531309273\n",
            "Training batch[5]: last record -> y: 30.420999180982733 | y_pred: 27.532556903720376\n",
            "Training batch[6]: last record -> y: 70.18830218633252 | y_pred: 65.76341495807492\n",
            "Training batch[7]: last record -> y: 51.931000946581435 | y_pred: 60.4551320265125\n",
            "Training batch[8]: last record -> y: 36.81190160929782 | y_pred: 33.57162618739676\n",
            "Training batch[9]: last record -> y: 45.79939989643424 | y_pred: 45.79261043304689\n",
            "Training batch[10]: last record -> y: 55.77679937246148 | y_pred: 57.31996302976472\n",
            "Training batch[11]: last record -> y: 60.00719790318408 | y_pred: 62.19431209101867\n",
            "Training batch[12]: last record -> y: 45.92639920518661 | y_pred: 46.424635179475786\n",
            "Training batch[13]: last record -> y: 54.96030127145741 | y_pred: 52.95808498067413\n",
            "Training batch[14]: last record -> y: 65.22029969808841 | y_pred: 60.23592337843502\n",
            "Training batch[15]: last record -> y: 69.90390053832061 | y_pred: 64.66429056850461\n",
            "Training batch[16]: last record -> y: 64.8636003961285 | y_pred: 61.40529668288286\n",
            "Training batch[17]: last record -> y: 25.033300272477504 | y_pred: 24.71125786839221\n",
            "Training batch[18]: last record -> y: 68.25509656153804 | y_pred: 70.70704484871158\n",
            "Training batch[19]: last record -> y: 67.4182976112204 | y_pred: 68.43039400941302\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.01495490623222\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.46153017201391\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.91161456911914\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.1508411920463\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.275811719366033\n",
            "[Training] Epoch: 930 | L1Loss: 0.05132 | RMSE: 0.08192 | PLCC: 0.94680 | SROCC: 0.94661\n",
            "[Testing]  Epoch: 930 | L1Loss: 0.07839 | RMSE: 0.10646 | PLCC: 0.91721 | SROCC: 0.93278\n",
            "Training batch[0]: last record -> y: 39.02769814411886 | y_pred: 40.04976164180812\n",
            "Training batch[1]: last record -> y: 67.38510289230953 | y_pred: 63.31540976996007\n",
            "Training batch[2]: last record -> y: 68.4131003359721 | y_pred: 63.8457244310448\n",
            "Training batch[3]: last record -> y: 63.459201020136106 | y_pred: 62.481537589743084\n",
            "Training batch[4]: last record -> y: 56.96869915799175 | y_pred: 59.08012256830011\n",
            "Training batch[5]: last record -> y: 50.734000218875735 | y_pred: 62.50588124127262\n",
            "Training batch[6]: last record -> y: 69.84349972239715 | y_pred: 71.17295448642744\n",
            "Training batch[7]: last record -> y: 48.65330146216252 | y_pred: 46.52788101506508\n",
            "Training batch[8]: last record -> y: 38.51679986885574 | y_pred: 36.83414184583194\n",
            "Training batch[9]: last record -> y: 49.19730118564098 | y_pred: 55.1949542602199\n",
            "Training batch[10]: last record -> y: 28.118200384631052 | y_pred: 26.575187086121446\n",
            "Training batch[11]: last record -> y: 34.048697754381124 | y_pred: 33.62506386346536\n",
            "Training batch[12]: last record -> y: 65.45760286109589 | y_pred: 66.73390619360589\n",
            "Training batch[13]: last record -> y: 67.19999594025103 | y_pred: 65.26286973001402\n",
            "Training batch[14]: last record -> y: 29.01670111626305 | y_pred: 31.13187465177731\n",
            "Training batch[15]: last record -> y: 73.27570372205287 | y_pred: 68.00342327895942\n",
            "Training batch[16]: last record -> y: 63.25490281841758 | y_pred: 63.11422487973323\n",
            "Training batch[17]: last record -> y: 63.49129900431694 | y_pred: 58.489805903921024\n",
            "Training batch[18]: last record -> y: 28.777399065763746 | y_pred: 26.614165392211987\n",
            "Training batch[19]: last record -> y: 56.444000036023226 | y_pred: 55.24016803392874\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.89465033590977\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.223049868504404\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.48794353769472\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 40.476870670189555\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.972332347686148\n",
            "[Training] Epoch: 931 | L1Loss: 0.05936 | RMSE: 0.08590 | PLCC: 0.94018 | SROCC: 0.93753\n",
            "[Testing]  Epoch: 931 | L1Loss: 0.07659 | RMSE: 0.10444 | PLCC: 0.91583 | SROCC: 0.93015\n",
            "Training batch[0]: last record -> y: 30.71260036181917 | y_pred: 32.22223963654636\n",
            "Training batch[1]: last record -> y: 43.31629919695854 | y_pred: 45.82313085598116\n",
            "Training batch[2]: last record -> y: 48.261201106908175 | y_pred: 50.730922285923725\n",
            "Training batch[3]: last record -> y: 69.65740217122766 | y_pred: 69.36969102164039\n",
            "Training batch[4]: last record -> y: 66.13350021295673 | y_pred: 65.10751291359406\n",
            "Training batch[5]: last record -> y: 26.646600678606347 | y_pred: 44.14492409649506\n",
            "Training batch[6]: last record -> y: 30.995199312422926 | y_pred: 29.738653286207807\n",
            "Training batch[7]: last record -> y: 34.10250047265458 | y_pred: 33.33592631548288\n",
            "Training batch[8]: last record -> y: 45.587398821758484 | y_pred: 46.63461002869724\n",
            "Training batch[9]: last record -> y: 56.15299868224588 | y_pred: 58.35005423973007\n",
            "Training batch[10]: last record -> y: 63.25490281841758 | y_pred: 65.51139432256127\n",
            "Training batch[11]: last record -> y: 72.626600789652 | y_pred: 69.8089959975182\n",
            "Training batch[12]: last record -> y: 37.43249908741063 | y_pred: 48.523163112071416\n",
            "Training batch[13]: last record -> y: 27.831899552284597 | y_pred: 26.91120200323536\n",
            "Training batch[14]: last record -> y: 29.17069987919399 | y_pred: 40.9695264839022\n",
            "Training batch[15]: last record -> y: 69.85279784587078 | y_pred: 59.9139143427451\n",
            "Training batch[16]: last record -> y: 25.895799721727116 | y_pred: 25.880597804444676\n",
            "Training batch[17]: last record -> y: 60.984999631504934 | y_pred: 62.432850286684015\n",
            "Training batch[18]: last record -> y: 60.85200205216165 | y_pred: 59.668110681522194\n",
            "Training batch[19]: last record -> y: 43.297297704920425 | y_pred: 42.58658304566984\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.88840586980302\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.4277628101529\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.03708601435005\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.07882817453117\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.64616658175936\n",
            "[Training] Epoch: 932 | L1Loss: 0.05346 | RMSE: 0.08437 | PLCC: 0.94423 | SROCC: 0.94357\n",
            "[Testing]  Epoch: 932 | L1Loss: 0.07526 | RMSE: 0.10242 | PLCC: 0.91871 | SROCC: 0.93423\n",
            "Training batch[0]: last record -> y: 51.99429958652763 | y_pred: 54.01540158503735\n",
            "Training batch[1]: last record -> y: 54.546501006456765 | y_pred: 49.97466097306824\n",
            "Training batch[2]: last record -> y: 55.86600153591394 | y_pred: 57.057081182569846\n",
            "Training batch[3]: last record -> y: 25.520900573275895 | y_pred: 25.371881339804673\n",
            "Training batch[4]: last record -> y: 49.09339763083676 | y_pred: 45.573609231861155\n",
            "Training batch[5]: last record -> y: 62.979599887564746 | y_pred: 63.54556968418342\n",
            "Training batch[6]: last record -> y: 29.01670111626305 | y_pred: 29.778935773920125\n",
            "Training batch[7]: last record -> y: 53.63130321221206 | y_pred: 54.71991374303616\n",
            "Training batch[8]: last record -> y: 70.58750076313868 | y_pred: 66.4417309155574\n",
            "Training batch[9]: last record -> y: 44.533298448275104 | y_pred: 45.772965694832806\n",
            "Training batch[10]: last record -> y: 57.24819927014278 | y_pred: 53.25354403045094\n",
            "Training batch[11]: last record -> y: 47.25970218946463 | y_pred: 47.94287465557568\n",
            "Training batch[12]: last record -> y: 54.18170323514141 | y_pred: 53.60934278776449\n",
            "Training batch[13]: last record -> y: 61.716499182816506 | y_pred: 52.60182308611388\n",
            "Training batch[14]: last record -> y: 61.01490092999484 | y_pred: 51.78167777929207\n",
            "Training batch[15]: last record -> y: 28.55800066006435 | y_pred: 29.392714688714136\n",
            "Training batch[16]: last record -> y: 46.75879995977607 | y_pred: 47.65004969888946\n",
            "Training batch[17]: last record -> y: 35.84120131875966 | y_pred: 48.84148313094215\n",
            "Training batch[18]: last record -> y: 40.39899768001135 | y_pred: 60.7557627755441\n",
            "Training batch[19]: last record -> y: 50.21070015733994 | y_pred: 47.9005554896587\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.20415918257811\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.951739734435364\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.38377288187394\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.05701408858863\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.928739400487217\n",
            "[Training] Epoch: 933 | L1Loss: 0.04892 | RMSE: 0.08134 | PLCC: 0.94792 | SROCC: 0.94954\n",
            "[Testing]  Epoch: 933 | L1Loss: 0.07610 | RMSE: 0.10198 | PLCC: 0.91751 | SROCC: 0.93131\n",
            "Training batch[0]: last record -> y: 63.25490281841758 | y_pred: 62.4791736600464\n",
            "Training batch[1]: last record -> y: 54.821202502135066 | y_pred: 57.097531718345635\n",
            "Training batch[2]: last record -> y: 43.03680069292295 | y_pred: 42.16012047969525\n",
            "Training batch[3]: last record -> y: 30.71260036181917 | y_pred: 31.60760022613846\n",
            "Training batch[4]: last record -> y: 66.95040034282079 | y_pred: 65.01381445937159\n",
            "Training batch[5]: last record -> y: 28.118200384631052 | y_pred: 28.25561786926096\n",
            "Training batch[6]: last record -> y: 63.36859979625092 | y_pred: 58.547810627839\n",
            "Training batch[7]: last record -> y: 64.97390103415273 | y_pred: 63.13305591153471\n",
            "Training batch[8]: last record -> y: 46.75879995977607 | y_pred: 46.31797209857666\n",
            "Training batch[9]: last record -> y: 59.08630094782029 | y_pred: 58.9497301359695\n",
            "Training batch[10]: last record -> y: 31.24950025563038 | y_pred: 32.5191138279103\n",
            "Training batch[11]: last record -> y: 41.601200661165194 | y_pred: 45.377132903642405\n",
            "Training batch[12]: last record -> y: 49.19860054291644 | y_pred: 55.99484374496046\n",
            "Training batch[13]: last record -> y: 65.00969768384539 | y_pred: 64.24099598994667\n",
            "Training batch[14]: last record -> y: 55.86600153591394 | y_pred: 52.613240705737326\n",
            "Training batch[15]: last record -> y: 28.114999430847888 | y_pred: 27.482320181434943\n",
            "Training batch[16]: last record -> y: 58.14030131043933 | y_pred: 63.06266548249846\n",
            "Training batch[17]: last record -> y: 56.89680031667285 | y_pred: 58.953821181648664\n",
            "Training batch[18]: last record -> y: 32.25839972032844 | y_pred: 33.36898916892767\n",
            "Training batch[19]: last record -> y: 60.984999631504934 | y_pred: 60.712700660293876\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.9927709537385\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.69367113060571\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.99003883518617\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.87114825989602\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.35124479639302\n",
            "[Training] Epoch: 934 | L1Loss: 0.05103 | RMSE: 0.08289 | PLCC: 0.94541 | SROCC: 0.94542\n",
            "[Testing]  Epoch: 934 | L1Loss: 0.07634 | RMSE: 0.10366 | PLCC: 0.91477 | SROCC: 0.93008\n",
            "Training batch[0]: last record -> y: 36.413502265865304 | y_pred: 37.15089234403331\n",
            "Training batch[1]: last record -> y: 47.6147001052891 | y_pred: 46.00404866731867\n",
            "Training batch[2]: last record -> y: 59.229098382654456 | y_pred: 61.238309975601396\n",
            "Training batch[3]: last record -> y: 29.39489931353927 | y_pred: 29.139618223971183\n",
            "Training batch[4]: last record -> y: 40.68579863625962 | y_pred: 47.972946414302214\n",
            "Training batch[5]: last record -> y: 25.033300272477504 | y_pred: 24.037468755873107\n",
            "Training batch[6]: last record -> y: 54.38159841678544 | y_pred: 50.20132162809432\n",
            "Training batch[7]: last record -> y: 47.28070096087913 | y_pred: 47.761048259014615\n",
            "Training batch[8]: last record -> y: 59.38040274816581 | y_pred: 60.19315715642574\n",
            "Training batch[9]: last record -> y: 50.21070015733994 | y_pred: 49.47278744165396\n",
            "Training batch[10]: last record -> y: 61.766399004917275 | y_pred: 57.8028479340644\n",
            "Training batch[11]: last record -> y: 52.35879824837116 | y_pred: 53.05336903672037\n",
            "Training batch[12]: last record -> y: 59.44990228124834 | y_pred: 61.15638292639915\n",
            "Training batch[13]: last record -> y: 55.66210214682451 | y_pred: 55.49727996292518\n",
            "Training batch[14]: last record -> y: 25.707799769992192 | y_pred: 22.690650968423554\n",
            "Training batch[15]: last record -> y: 38.56610147201286 | y_pred: 37.672696858209974\n",
            "Training batch[16]: last record -> y: 46.69590013245897 | y_pred: 54.35917807132205\n",
            "Training batch[17]: last record -> y: 29.528200022642977 | y_pred: 35.67021362026367\n",
            "Training batch[18]: last record -> y: 48.045601069877875 | y_pred: 47.06917106402727\n",
            "Training batch[19]: last record -> y: 65.1140965382051 | y_pred: 67.13454242561534\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.832091275212406\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.85849009827416\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.45817586663986\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.159166405678775\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.376129176739227\n",
            "[Training] Epoch: 935 | L1Loss: 0.04945 | RMSE: 0.08145 | PLCC: 0.94774 | SROCC: 0.94868\n",
            "[Testing]  Epoch: 935 | L1Loss: 0.07589 | RMSE: 0.10323 | PLCC: 0.91727 | SROCC: 0.93352\n",
            "Training batch[0]: last record -> y: 65.29879824517275 | y_pred: 65.49640025419944\n",
            "Training batch[1]: last record -> y: 64.73490291747157 | y_pred: 61.193359932824706\n",
            "Training batch[2]: last record -> y: 67.75769680727763 | y_pred: 66.14669640825673\n",
            "Training batch[3]: last record -> y: 44.64339807186934 | y_pred: 43.21530633110058\n",
            "Training batch[4]: last record -> y: 53.63130321221206 | y_pred: 55.51587942610331\n",
            "Training batch[5]: last record -> y: 69.13320156504551 | y_pred: 68.11643519961535\n",
            "Training batch[6]: last record -> y: 68.06819816887946 | y_pred: 69.31125210655364\n",
            "Training batch[7]: last record -> y: 28.547899282928483 | y_pred: 28.346227937781066\n",
            "Training batch[8]: last record -> y: 63.38790039776086 | y_pred: 62.46110487496344\n",
            "Training batch[9]: last record -> y: 39.49110072986389 | y_pred: 39.249899495030036\n",
            "Training batch[10]: last record -> y: 43.91579820049958 | y_pred: 44.161064751165554\n",
            "Training batch[11]: last record -> y: 55.55740096676209 | y_pred: 61.176712721790636\n",
            "Training batch[12]: last record -> y: 54.441799826394345 | y_pred: 57.709397129619674\n",
            "Training batch[13]: last record -> y: 72.54129669802592 | y_pred: 68.81145304405686\n",
            "Training batch[14]: last record -> y: 29.01670111626305 | y_pred: 29.503960094559318\n",
            "Training batch[15]: last record -> y: 54.18170323514141 | y_pred: 52.168519597057866\n",
            "Training batch[16]: last record -> y: 26.044099725567833 | y_pred: 25.85625937929032\n",
            "Training batch[17]: last record -> y: 59.88249819951079 | y_pred: 56.557179200684914\n",
            "Training batch[18]: last record -> y: 46.40150083075707 | y_pred: 41.654356917031805\n",
            "Training batch[19]: last record -> y: 66.50039818303662 | y_pred: 58.10429560573948\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 47.42609389403333\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 48.341259525969235\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.01581080093342\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 40.41085431514989\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.703867679566258\n",
            "[Training] Epoch: 936 | L1Loss: 0.05346 | RMSE: 0.08389 | PLCC: 0.94403 | SROCC: 0.94401\n",
            "[Testing]  Epoch: 936 | L1Loss: 0.08794 | RMSE: 0.11764 | PLCC: 0.91851 | SROCC: 0.93539\n",
            "Training batch[0]: last record -> y: 26.749900788091736 | y_pred: 26.89313000192152\n",
            "Training batch[1]: last record -> y: 38.542598864858405 | y_pred: 36.99466553716036\n",
            "Training batch[2]: last record -> y: 31.31949991261473 | y_pred: 32.35682764205973\n",
            "Training batch[3]: last record -> y: 30.074199437670984 | y_pred: 28.968003360453622\n",
            "Training batch[4]: last record -> y: 62.27109960327493 | y_pred: 53.80772634089499\n",
            "Training batch[5]: last record -> y: 64.93820087138647 | y_pred: 61.25770706403773\n",
            "Training batch[6]: last record -> y: 56.32199875005813 | y_pred: 54.34174931618418\n",
            "Training batch[7]: last record -> y: 62.93809764429125 | y_pred: 67.78014609881893\n",
            "Training batch[8]: last record -> y: 51.931000946581435 | y_pred: 64.83916990636521\n",
            "Training batch[9]: last record -> y: 38.69430204299886 | y_pred: 47.81187757183966\n",
            "Training batch[10]: last record -> y: 27.821399362519628 | y_pred: 28.525330990844623\n",
            "Training batch[11]: last record -> y: 41.601200661165194 | y_pred: 41.11979683118568\n",
            "Training batch[12]: last record -> y: 44.533298448275104 | y_pred: 42.21274927369757\n",
            "Training batch[13]: last record -> y: 38.79510193500403 | y_pred: 39.44014437591909\n",
            "Training batch[14]: last record -> y: 38.44319964140141 | y_pred: 35.93740200060665\n",
            "Training batch[15]: last record -> y: 62.921900705580356 | y_pred: 60.70087457934869\n",
            "Training batch[16]: last record -> y: 61.97249829592852 | y_pred: 65.29996573698213\n",
            "Training batch[17]: last record -> y: 26.15419975119093 | y_pred: 47.6894227973205\n",
            "Training batch[18]: last record -> y: 61.21029982086884 | y_pred: 61.26044729274736\n",
            "Training batch[19]: last record -> y: 66.61560035692173 | y_pred: 68.50773149714973\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.87690929866949\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.69337186728035\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 34.7784653653639\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.55567605961585\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.170809821716347\n",
            "[Training] Epoch: 937 | L1Loss: 0.06368 | RMSE: 0.09410 | PLCC: 0.93077 | SROCC: 0.92637\n",
            "[Testing]  Epoch: 937 | L1Loss: 0.07683 | RMSE: 0.10207 | PLCC: 0.91707 | SROCC: 0.93145\n",
            "Training batch[0]: last record -> y: 26.46649938788346 | y_pred: 33.918085010235245\n",
            "Training batch[1]: last record -> y: 38.67589876990439 | y_pred: 50.418455807254304\n",
            "Training batch[2]: last record -> y: 47.25970218946463 | y_pred: 48.27858805104461\n",
            "Training batch[3]: last record -> y: 52.99509736563937 | y_pred: 55.61849970478647\n",
            "Training batch[4]: last record -> y: 43.907299310399594 | y_pred: 51.746421456387225\n",
            "Training batch[5]: last record -> y: 68.26640161308069 | y_pred: 67.86485197150137\n",
            "Training batch[6]: last record -> y: 65.36119955670347 | y_pred: 63.5767188802547\n",
            "Training batch[7]: last record -> y: 43.26480090811049 | y_pred: 48.51157824844222\n",
            "Training batch[8]: last record -> y: 54.97959865673647 | y_pred: 62.07309878161914\n",
            "Training batch[9]: last record -> y: 40.115501400992116 | y_pred: 35.211588745490644\n",
            "Training batch[10]: last record -> y: 26.387500716897307 | y_pred: 27.6991713322775\n",
            "Training batch[11]: last record -> y: 61.01490092999484 | y_pred: 51.355147672469\n",
            "Training batch[12]: last record -> y: 63.33969874556465 | y_pred: 61.385963919064125\n",
            "Training batch[13]: last record -> y: 30.96270010343983 | y_pred: 27.41093674111454\n",
            "Training batch[14]: last record -> y: 37.30360059432371 | y_pred: 36.97443866115702\n",
            "Training batch[15]: last record -> y: 57.606300848766296 | y_pred: 64.13821168348863\n",
            "Training batch[16]: last record -> y: 38.69430204299886 | y_pred: 44.147717393014204\n",
            "Training batch[17]: last record -> y: 68.51670156507362 | y_pred: 67.47408313583128\n",
            "Training batch[18]: last record -> y: 26.000999417575912 | y_pred: 29.3059785702888\n",
            "Training batch[19]: last record -> y: 28.777399065763746 | y_pred: 27.588214583153785\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.125597918991616\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.14980488070876\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.28233778427\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.07713965331925\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.62739279190953\n",
            "[Training] Epoch: 938 | L1Loss: 0.05098 | RMSE: 0.08290 | PLCC: 0.94519 | SROCC: 0.94560\n",
            "[Testing]  Epoch: 938 | L1Loss: 0.07574 | RMSE: 0.10181 | PLCC: 0.91734 | SROCC: 0.93389\n",
            "Training batch[0]: last record -> y: 28.118200384631052 | y_pred: 28.26484443159751\n",
            "Training batch[1]: last record -> y: 53.70280002467098 | y_pred: 52.661529196167294\n",
            "Training batch[2]: last record -> y: 48.61320149555263 | y_pred: 47.98760599465254\n",
            "Training batch[3]: last record -> y: 68.87090185563989 | y_pred: 67.24399719491817\n",
            "Training batch[4]: last record -> y: 64.73490291747157 | y_pred: 63.843788260055135\n",
            "Training batch[5]: last record -> y: 38.44319964140141 | y_pred: 38.516931734321815\n",
            "Training batch[6]: last record -> y: 67.50070066259286 | y_pred: 65.37830990498423\n",
            "Training batch[7]: last record -> y: 67.78130394193568 | y_pred: 66.09760707633768\n",
            "Training batch[8]: last record -> y: 69.68309663972673 | y_pred: 67.37562465990663\n",
            "Training batch[9]: last record -> y: 53.082598142956385 | y_pred: 52.92379352703324\n",
            "Training batch[10]: last record -> y: 59.88249819951079 | y_pred: 59.524798649747254\n",
            "Training batch[11]: last record -> y: 29.528200022642977 | y_pred: 37.172560091470814\n",
            "Training batch[12]: last record -> y: 48.99319917400612 | y_pred: 58.42050577715304\n",
            "Training batch[13]: last record -> y: 58.383400121492286 | y_pred: 56.44675312965637\n",
            "Training batch[14]: last record -> y: 65.80919800464949 | y_pred: 58.83467912493575\n",
            "Training batch[15]: last record -> y: 62.93809764429125 | y_pred: 64.53294291560269\n",
            "Training batch[16]: last record -> y: 26.98159966879109 | y_pred: 25.60624245561482\n",
            "Training batch[17]: last record -> y: 56.977000249892626 | y_pred: 56.56340903989917\n",
            "Training batch[18]: last record -> y: 64.07659834852348 | y_pred: 62.45060709737163\n",
            "Training batch[19]: last record -> y: 64.97390103415273 | y_pred: 63.57838810408134\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.48977495774716\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.7806523327821\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.418225300912354\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.13798270098869\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.508134548856077\n",
            "[Training] Epoch: 939 | L1Loss: 0.05176 | RMSE: 0.08379 | PLCC: 0.94445 | SROCC: 0.94419\n",
            "[Testing]  Epoch: 939 | L1Loss: 0.07501 | RMSE: 0.10105 | PLCC: 0.91780 | SROCC: 0.93389\n",
            "Training batch[0]: last record -> y: 56.73359911313696 | y_pred: 53.20236093222911\n",
            "Training batch[1]: last record -> y: 63.36859979625092 | y_pred: 58.68877159484077\n",
            "Training batch[2]: last record -> y: 57.950199551824426 | y_pred: 57.53659869313833\n",
            "Training batch[3]: last record -> y: 40.97670028687969 | y_pred: 41.828686279411954\n",
            "Training batch[4]: last record -> y: 52.59410091577138 | y_pred: 53.25038569172693\n",
            "Training batch[5]: last record -> y: 59.08630094782029 | y_pred: 57.41400562069134\n",
            "Training batch[6]: last record -> y: 72.15129975776699 | y_pred: 67.10558026654235\n",
            "Training batch[7]: last record -> y: 55.84130088275674 | y_pred: 53.27361974360292\n",
            "Training batch[8]: last record -> y: 56.61489768005458 | y_pred: 59.18955804021766\n",
            "Training batch[9]: last record -> y: 44.76750116471442 | y_pred: 43.540369178010906\n",
            "Training batch[10]: last record -> y: 24.811199026752462 | y_pred: 24.34124055638773\n",
            "Training batch[11]: last record -> y: 53.863000484795975 | y_pred: 57.612678634601025\n",
            "Training batch[12]: last record -> y: 30.995199312422926 | y_pred: 30.77746450214613\n",
            "Training batch[13]: last record -> y: 29.39489931353927 | y_pred: 28.76317046035132\n",
            "Training batch[14]: last record -> y: 62.921900705580356 | y_pred: 58.22071351489717\n",
            "Training batch[15]: last record -> y: 59.78609811134925 | y_pred: 56.37842108838322\n",
            "Training batch[16]: last record -> y: 64.75720104616153 | y_pred: 62.970604089793824\n",
            "Training batch[17]: last record -> y: 59.24809987469257 | y_pred: 60.33447834128606\n",
            "Training batch[18]: last record -> y: 26.46649938788346 | y_pred: 36.7843240376186\n",
            "Training batch[19]: last record -> y: 42.32070206317951 | y_pred: 41.470007387345504\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 47.48912237058562\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 48.25117289902482\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.61920933056274\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 41.17445345875774\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.838227734716014\n",
            "[Training] Epoch: 940 | L1Loss: 0.05582 | RMSE: 0.08565 | PLCC: 0.94126 | SROCC: 0.94121\n",
            "[Testing]  Epoch: 940 | L1Loss: 0.08717 | RMSE: 0.11665 | PLCC: 0.91855 | SROCC: 0.93547\n",
            "Training batch[0]: last record -> y: 54.38009965319543 | y_pred: 67.4365818837723\n",
            "Training batch[1]: last record -> y: 70.7566005341082 | y_pred: 67.34780104656511\n",
            "Training batch[2]: last record -> y: 58.23469768676273 | y_pred: 57.703617562728596\n",
            "Training batch[3]: last record -> y: 69.45530065520006 | y_pred: 63.432512736295166\n",
            "Training batch[4]: last record -> y: 28.995599425460398 | y_pred: 27.223949902106767\n",
            "Training batch[5]: last record -> y: 28.016500752037246 | y_pred: 27.200153009826806\n",
            "Training batch[6]: last record -> y: 53.191200611076056 | y_pred: 59.49464005278696\n",
            "Training batch[7]: last record -> y: 55.782900562440545 | y_pred: 54.53815810355445\n",
            "Training batch[8]: last record -> y: 38.79510193500403 | y_pred: 43.958197772188555\n",
            "Training batch[9]: last record -> y: 69.6393977107623 | y_pred: 69.2455348609858\n",
            "Training batch[10]: last record -> y: 24.811199026752462 | y_pred: 24.48478295069944\n",
            "Training batch[11]: last record -> y: 52.48210210784259 | y_pred: 51.41696041374871\n",
            "Training batch[12]: last record -> y: 37.30360059432371 | y_pred: 35.98391513159095\n",
            "Training batch[13]: last record -> y: 43.48930025598531 | y_pred: 42.22999952802161\n",
            "Training batch[14]: last record -> y: 37.457900878899636 | y_pred: 44.6545503524452\n",
            "Training batch[15]: last record -> y: 35.17409875023975 | y_pred: 51.42362122790087\n",
            "Training batch[16]: last record -> y: 68.15290082533102 | y_pred: 64.92447078176042\n",
            "Training batch[17]: last record -> y: 47.44130023363323 | y_pred: 48.806107807494755\n",
            "Training batch[18]: last record -> y: 69.42099633663565 | y_pred: 70.86662457627676\n",
            "Training batch[19]: last record -> y: 35.50039984603882 | y_pred: 39.78912954811369\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.015097720799304\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.26087082782885\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 34.50532052543144\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.04315519587385\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 23.87769886865715\n",
            "[Training] Epoch: 941 | L1Loss: 0.05789 | RMSE: 0.08631 | PLCC: 0.94029 | SROCC: 0.93798\n",
            "[Testing]  Epoch: 941 | L1Loss: 0.07937 | RMSE: 0.10471 | PLCC: 0.91657 | SROCC: 0.93198\n",
            "Training batch[0]: last record -> y: 40.94380146120977 | y_pred: 41.334460945029946\n",
            "Training batch[1]: last record -> y: 64.07659834852348 | y_pred: 60.05712023890101\n",
            "Training batch[2]: last record -> y: 46.59859789153563 | y_pred: 45.578549362492595\n",
            "Training batch[3]: last record -> y: 48.061598602274216 | y_pred: 60.63912937891746\n",
            "Training batch[4]: last record -> y: 27.821399362519628 | y_pred: 28.1471649558772\n",
            "Training batch[5]: last record -> y: 31.80629896989518 | y_pred: 32.24975288360798\n",
            "Training batch[6]: last record -> y: 61.352299630444804 | y_pred: 61.567053798753705\n",
            "Training batch[7]: last record -> y: 69.45530065520006 | y_pred: 65.98065848908482\n",
            "Training batch[8]: last record -> y: 38.76359895353596 | y_pred: 53.21758013675253\n",
            "Training batch[9]: last record -> y: 48.60699738618541 | y_pred: 49.090268238190674\n",
            "Training batch[10]: last record -> y: 32.16310119124324 | y_pred: 37.89573282103447\n",
            "Training batch[11]: last record -> y: 47.48310158637855 | y_pred: 61.48765792325389\n",
            "Training batch[12]: last record -> y: 55.70909771244078 | y_pred: 57.007818174183285\n",
            "Training batch[13]: last record -> y: 47.55979904417018 | y_pred: 46.93765938335048\n",
            "Training batch[14]: last record -> y: 65.26629501590105 | y_pred: 64.11603255534123\n",
            "Training batch[15]: last record -> y: 47.797999535593135 | y_pred: 46.67702568154061\n",
            "Training batch[16]: last record -> y: 40.324099703396996 | y_pred: 40.164162974204146\n",
            "Training batch[17]: last record -> y: 53.07020278914547 | y_pred: 57.92978935066094\n",
            "Training batch[18]: last record -> y: 60.41220177672835 | y_pred: 62.60378974171681\n",
            "Training batch[19]: last record -> y: 53.079102099989996 | y_pred: 56.55042511583724\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.382663212640296\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.46606329558551\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.1229510305576\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.19192210907454\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.331351202314607\n",
            "[Training] Epoch: 942 | L1Loss: 0.05089 | RMSE: 0.08322 | PLCC: 0.94536 | SROCC: 0.94588\n",
            "[Testing]  Epoch: 942 | L1Loss: 0.07580 | RMSE: 0.10192 | PLCC: 0.91791 | SROCC: 0.93316\n",
            "Training batch[0]: last record -> y: 70.78559807172087 | y_pred: 65.96030939630805\n",
            "Training batch[1]: last record -> y: 47.55770206163652 | y_pred: 37.781302542560525\n",
            "Training batch[2]: last record -> y: 46.59859789153563 | y_pred: 44.10962435447334\n",
            "Training batch[3]: last record -> y: 27.960300333642863 | y_pred: 28.103168525556384\n",
            "Training batch[4]: last record -> y: 54.821202502135066 | y_pred: 57.43205189215814\n",
            "Training batch[5]: last record -> y: 48.559300682237335 | y_pred: 49.26615425631667\n",
            "Training batch[6]: last record -> y: 46.11660066695879 | y_pred: 38.955879708571956\n",
            "Training batch[7]: last record -> y: 35.84120131875966 | y_pred: 48.88180501748275\n",
            "Training batch[8]: last record -> y: 58.07179880892954 | y_pred: 57.39021837710402\n",
            "Training batch[9]: last record -> y: 25.707799769992192 | y_pred: 22.85517484690878\n",
            "Training batch[10]: last record -> y: 64.97390103415273 | y_pred: 63.698626895524285\n",
            "Training batch[11]: last record -> y: 65.86109832235752 | y_pred: 65.66001635151883\n",
            "Training batch[12]: last record -> y: 61.766399004917275 | y_pred: 59.83577922975019\n",
            "Training batch[13]: last record -> y: 51.776001131789144 | y_pred: 50.159105381565496\n",
            "Training batch[14]: last record -> y: 61.18830080165071 | y_pred: 62.5337916928479\n",
            "Training batch[15]: last record -> y: 44.16859877200159 | y_pred: 43.03608186532131\n",
            "Training batch[16]: last record -> y: 69.6393977107623 | y_pred: 66.23194582395786\n",
            "Training batch[17]: last record -> y: 32.25839972032844 | y_pred: 31.750305998392548\n",
            "Training batch[18]: last record -> y: 68.3981030513794 | y_pred: 65.58155961543594\n",
            "Training batch[19]: last record -> y: 29.17069987919399 | y_pred: 44.037067793939514\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.88368298860951\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 47.425328431083926\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.53809116342677\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 40.170939572667976\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.8809531979569\n",
            "[Training] Epoch: 943 | L1Loss: 0.05082 | RMSE: 0.08268 | PLCC: 0.94579 | SROCC: 0.94531\n",
            "[Testing]  Epoch: 943 | L1Loss: 0.08140 | RMSE: 0.10951 | PLCC: 0.91869 | SROCC: 0.93563\n",
            "Training batch[0]: last record -> y: 63.37459806684183 | y_pred: 63.01254374046698\n",
            "Training batch[1]: last record -> y: 60.20750154614984 | y_pred: 57.45526664664885\n",
            "Training batch[2]: last record -> y: 40.42119932177491 | y_pred: 39.647424023663234\n",
            "Training batch[3]: last record -> y: 55.65470159956999 | y_pred: 52.88619900427875\n",
            "Training batch[4]: last record -> y: 43.92589877357773 | y_pred: 42.986249584069014\n",
            "Training batch[5]: last record -> y: 54.23470028757947 | y_pred: 54.56793075280916\n",
            "Training batch[6]: last record -> y: 63.20890106814318 | y_pred: 62.68092460690809\n",
            "Training batch[7]: last record -> y: 26.044099725567833 | y_pred: 29.28728664047287\n",
            "Training batch[8]: last record -> y: 52.83100204991888 | y_pred: 52.41723394722703\n",
            "Training batch[9]: last record -> y: 58.27209923566443 | y_pred: 58.581343050992245\n",
            "Training batch[10]: last record -> y: 62.23139844929415 | y_pred: 58.15032308586092\n",
            "Training batch[11]: last record -> y: 53.70280002467098 | y_pred: 52.85005500165107\n",
            "Training batch[12]: last record -> y: 65.84580192829299 | y_pred: 68.17746318056038\n",
            "Training batch[13]: last record -> y: 25.665500705518212 | y_pred: 25.54116403390549\n",
            "Training batch[14]: last record -> y: 47.01879845598717 | y_pred: 41.9287882493162\n",
            "Training batch[15]: last record -> y: 43.91579820049958 | y_pred: 42.65948213479237\n",
            "Training batch[16]: last record -> y: 29.01670111626305 | y_pred: 29.812342764069058\n",
            "Training batch[17]: last record -> y: 21.597000101274666 | y_pred: 15.687277874385728\n",
            "Training batch[18]: last record -> y: 46.75879995977607 | y_pred: 47.19287212801237\n",
            "Training batch[19]: last record -> y: 60.54589727817256 | y_pred: 61.07192470349446\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.499315906652214\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.455977195546325\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.8835188765612\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.25414331167599\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.346425676448433\n",
            "[Training] Epoch: 944 | L1Loss: 0.05224 | RMSE: 0.08322 | PLCC: 0.94465 | SROCC: 0.94463\n",
            "[Testing]  Epoch: 944 | L1Loss: 0.07574 | RMSE: 0.10219 | PLCC: 0.91756 | SROCC: 0.93335\n",
            "Training batch[0]: last record -> y: 34.149799972089 | y_pred: 33.24166260868327\n",
            "Training batch[1]: last record -> y: 53.34080037422109 | y_pred: 53.635944233371674\n",
            "Training batch[2]: last record -> y: 70.11219651502279 | y_pred: 65.71333824327576\n",
            "Training batch[3]: last record -> y: 43.48930025598531 | y_pred: 43.847347158683874\n",
            "Training batch[4]: last record -> y: 52.87200256117512 | y_pred: 54.227048874316324\n",
            "Training batch[5]: last record -> y: 61.716499182816506 | y_pred: 53.10350364367537\n",
            "Training batch[6]: last record -> y: 49.39419884010499 | y_pred: 56.032393240482634\n",
            "Training batch[7]: last record -> y: 45.30530160317198 | y_pred: 45.05195909676672\n",
            "Training batch[8]: last record -> y: 66.61560035692173 | y_pred: 69.6393977107623\n",
            "Training batch[9]: last record -> y: 62.27109960327493 | y_pred: 59.55953715948044\n",
            "Training batch[10]: last record -> y: 39.941398782888996 | y_pred: 57.88516093097223\n",
            "Training batch[11]: last record -> y: 26.646600678606347 | y_pred: 45.03749088215375\n",
            "Training batch[12]: last record -> y: 59.43330009744659 | y_pred: 60.73683204058534\n",
            "Training batch[13]: last record -> y: 55.55740096676209 | y_pred: 56.359628651352295\n",
            "Training batch[14]: last record -> y: 33.85829849440984 | y_pred: 31.522501973288684\n",
            "Training batch[15]: last record -> y: 53.02539747675837 | y_pred: 53.86431913945671\n",
            "Training batch[16]: last record -> y: 65.00969768384539 | y_pred: 64.61784819459967\n",
            "Training batch[17]: last record -> y: 54.171601053947825 | y_pred: 54.811573106880815\n",
            "Training batch[18]: last record -> y: 48.6593994359107 | y_pred: 46.39256292514199\n",
            "Training batch[19]: last record -> y: 54.441799826394345 | y_pred: 53.897732562067404\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.81665336698916\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.15841633888954\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.38680417947819\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.693832627143934\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.431371158331416\n",
            "[Training] Epoch: 945 | L1Loss: 0.05037 | RMSE: 0.08291 | PLCC: 0.94482 | SROCC: 0.94535\n",
            "[Testing]  Epoch: 945 | L1Loss: 0.07528 | RMSE: 0.10121 | PLCC: 0.91839 | SROCC: 0.93405\n",
            "Training batch[0]: last record -> y: 48.33070063999071 | y_pred: 48.17194748376164\n",
            "Training batch[1]: last record -> y: 34.274600987035 | y_pred: 35.31373141388815\n",
            "Training batch[2]: last record -> y: 69.51529622603266 | y_pred: 68.67911157581307\n",
            "Training batch[3]: last record -> y: 62.41920060282996 | y_pred: 60.245851883161095\n",
            "Training batch[4]: last record -> y: 68.26640161308069 | y_pred: 66.01521367365785\n",
            "Training batch[5]: last record -> y: 51.062699014795726 | y_pred: 48.51520937310556\n",
            "Training batch[6]: last record -> y: 54.96030127145741 | y_pred: 51.07322573846545\n",
            "Training batch[7]: last record -> y: 65.14040530680222 | y_pred: 63.59236584348514\n",
            "Training batch[8]: last record -> y: 65.36119955670347 | y_pred: 64.65355800605857\n",
            "Training batch[9]: last record -> y: 40.94380146120977 | y_pred: 43.64085548750529\n",
            "Training batch[10]: last record -> y: 31.31949991261473 | y_pred: 32.344774012779794\n",
            "Training batch[11]: last record -> y: 52.48210210784259 | y_pred: 54.48321844766497\n",
            "Training batch[12]: last record -> y: 62.23139844929415 | y_pred: 59.134444707860666\n",
            "Training batch[13]: last record -> y: 36.61619878460567 | y_pred: 39.52959901349561\n",
            "Training batch[14]: last record -> y: 26.15419975119093 | y_pred: 42.636775544780676\n",
            "Training batch[15]: last record -> y: 63.33969874556465 | y_pred: 62.39409792081278\n",
            "Training batch[16]: last record -> y: 39.217899605891034 | y_pred: 46.76294085703387\n",
            "Training batch[17]: last record -> y: 59.88249819951079 | y_pred: 59.377180084824204\n",
            "Training batch[18]: last record -> y: 52.915900896454104 | y_pred: 49.87443678639056\n",
            "Training batch[19]: last record -> y: 69.90390053832061 | y_pred: 65.9386963247955\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.67343284394076\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.18459308816705\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.25020602966492\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.57884900310512\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.216936449112666\n",
            "[Training] Epoch: 946 | L1Loss: 0.05132 | RMSE: 0.08189 | PLCC: 0.94755 | SROCC: 0.94729\n",
            "[Testing]  Epoch: 946 | L1Loss: 0.07814 | RMSE: 0.10321 | PLCC: 0.91614 | SROCC: 0.93110\n",
            "Training batch[0]: last record -> y: 44.16859877200159 | y_pred: 43.44293667973693\n",
            "Training batch[1]: last record -> y: 30.471100017513493 | y_pred: 50.39781082123659\n",
            "Training batch[2]: last record -> y: 55.86600153591394 | y_pred: 54.35242720270526\n",
            "Training batch[3]: last record -> y: 63.20890106814318 | y_pred: 61.30027387973246\n",
            "Training batch[4]: last record -> y: 53.02539747675837 | y_pred: 53.373345414494224\n",
            "Training batch[5]: last record -> y: 56.38050198976248 | y_pred: 59.96451530317768\n",
            "Training batch[6]: last record -> y: 39.49110072986389 | y_pred: 39.613742045774075\n",
            "Training batch[7]: last record -> y: 66.95040034282079 | y_pred: 63.66316473384313\n",
            "Training batch[8]: last record -> y: 65.49870307550941 | y_pred: 64.39384736251122\n",
            "Training batch[9]: last record -> y: 46.50630171397677 | y_pred: 46.69454127491224\n",
            "Training batch[10]: last record -> y: 36.45529879426431 | y_pred: 37.63908402928473\n",
            "Training batch[11]: last record -> y: 22.98349944379808 | y_pred: 25.747620326544393\n",
            "Training batch[12]: last record -> y: 33.643200189396794 | y_pred: 35.43696934062655\n",
            "Training batch[13]: last record -> y: 26.387500716897307 | y_pred: 27.66411361162949\n",
            "Training batch[14]: last record -> y: 65.29879824517275 | y_pred: 64.23224140949173\n",
            "Training batch[15]: last record -> y: 65.2375033170647 | y_pred: 65.95800979122896\n",
            "Training batch[16]: last record -> y: 67.8711011081009 | y_pred: 66.60946700463387\n",
            "Training batch[17]: last record -> y: 47.40309784324245 | y_pred: 49.05018435273519\n",
            "Training batch[18]: last record -> y: 36.67800187719274 | y_pred: 36.95703081151987\n",
            "Training batch[19]: last record -> y: 66.13350021295673 | y_pred: 65.49747125908243\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.90353004166195\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.92267626408966\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.815841338272094\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.53939389078664\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.52934222142423\n",
            "[Training] Epoch: 947 | L1Loss: 0.05232 | RMSE: 0.08270 | PLCC: 0.94525 | SROCC: 0.94386\n",
            "[Testing]  Epoch: 947 | L1Loss: 0.07566 | RMSE: 0.10286 | PLCC: 0.91775 | SROCC: 0.93260\n",
            "Training batch[0]: last record -> y: 36.413502265865304 | y_pred: 36.39924793060584\n",
            "Training batch[1]: last record -> y: 63.56399868712492 | y_pred: 66.57011641981899\n",
            "Training batch[2]: last record -> y: 63.459201020136106 | y_pred: 62.21425593870458\n",
            "Training batch[3]: last record -> y: 26.15419975119093 | y_pred: 44.5456809371625\n",
            "Training batch[4]: last record -> y: 66.4921035235975 | y_pred: 65.66189141412178\n",
            "Training batch[5]: last record -> y: 40.03480134387053 | y_pred: 40.041203251436855\n",
            "Training batch[6]: last record -> y: 29.928600665740476 | y_pred: 29.890382194195297\n",
            "Training batch[7]: last record -> y: 57.606300848766296 | y_pred: 63.245286288086845\n",
            "Training batch[8]: last record -> y: 33.93240045388143 | y_pred: 30.97809378848848\n",
            "Training batch[9]: last record -> y: 51.13240117042369 | y_pred: 54.02450673465819\n",
            "Training batch[10]: last record -> y: 65.59839658409192 | y_pred: 62.74837861715105\n",
            "Training batch[11]: last record -> y: 49.50770284408554 | y_pred: 50.151299589220116\n",
            "Training batch[12]: last record -> y: 62.407599658046365 | y_pred: 64.34854031810687\n",
            "Training batch[13]: last record -> y: 35.17409875023975 | y_pred: 50.30990479882871\n",
            "Training batch[14]: last record -> y: 64.8636003961285 | y_pred: 61.45603272501239\n",
            "Training batch[15]: last record -> y: 48.60699738618541 | y_pred: 48.07860603116683\n",
            "Training batch[16]: last record -> y: 58.812097967928366 | y_pred: 55.88972767111454\n",
            "Training batch[17]: last record -> y: 42.74340003090924 | y_pred: 42.32891470873119\n",
            "Training batch[18]: last record -> y: 59.38040274816581 | y_pred: 61.005480589747776\n",
            "Training batch[19]: last record -> y: 50.734000218875735 | y_pred: 60.75928133212665\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.88771759639474\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.23058067310956\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.78986866580192\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.82569311501766\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.421083239804517\n",
            "[Training] Epoch: 948 | L1Loss: 0.05142 | RMSE: 0.08201 | PLCC: 0.94660 | SROCC: 0.94680\n",
            "[Testing]  Epoch: 948 | L1Loss: 0.07620 | RMSE: 0.10405 | PLCC: 0.91772 | SROCC: 0.93083\n",
            "Training batch[0]: last record -> y: 66.22540401034826 | y_pred: 65.3955649836546\n",
            "Training batch[1]: last record -> y: 31.165001025781976 | y_pred: 32.172921952234844\n",
            "Training batch[2]: last record -> y: 26.76479917358489 | y_pred: 25.859689489523674\n",
            "Training batch[3]: last record -> y: 48.61320149555263 | y_pred: 46.88040082499663\n",
            "Training batch[4]: last record -> y: 28.126199954886943 | y_pred: 28.702121573905572\n",
            "Training batch[5]: last record -> y: 48.559300682237335 | y_pred: 52.291315292050285\n",
            "Training batch[6]: last record -> y: 62.891102078674976 | y_pred: 65.82305352727985\n",
            "Training batch[7]: last record -> y: 59.05309979644767 | y_pred: 59.646340014696534\n",
            "Training batch[8]: last record -> y: 67.704902377385 | y_pred: 64.86528248487866\n",
            "Training batch[9]: last record -> y: 40.48429855540655 | y_pred: 38.93976960809482\n",
            "Training batch[10]: last record -> y: 67.35739827951056 | y_pred: 66.38994959839192\n",
            "Training batch[11]: last record -> y: 71.1275027116335 | y_pred: 65.29952832958247\n",
            "Training batch[12]: last record -> y: 53.07039897922914 | y_pred: 54.879744336609974\n",
            "Training batch[13]: last record -> y: 49.19730118564098 | y_pred: 63.266619547512846\n",
            "Training batch[14]: last record -> y: 60.175200939423576 | y_pred: 62.00444190102712\n",
            "Training batch[15]: last record -> y: 61.97249829592852 | y_pred: 60.67073527977368\n",
            "Training batch[16]: last record -> y: 26.15419975119093 | y_pred: 42.042805242122085\n",
            "Training batch[17]: last record -> y: 40.39899768001135 | y_pred: 57.14543426107002\n",
            "Training batch[18]: last record -> y: 54.441799826394345 | y_pred: 53.15647496626639\n",
            "Training batch[19]: last record -> y: 38.3894998425161 | y_pred: 38.14072438396022\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 47.23763884562891\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.956017629212624\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.37308420010538\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 40.40161730006298\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.38561077922654\n",
            "[Training] Epoch: 949 | L1Loss: 0.05365 | RMSE: 0.08378 | PLCC: 0.94358 | SROCC: 0.94318\n",
            "[Testing]  Epoch: 949 | L1Loss: 0.08110 | RMSE: 0.10848 | PLCC: 0.91955 | SROCC: 0.93539\n",
            "Training batch[0]: last record -> y: 44.95980121713569 | y_pred: 58.76191511751017\n",
            "Training batch[1]: last record -> y: 73.27570372205287 | y_pred: 69.61156766495901\n",
            "Training batch[2]: last record -> y: 60.984999631504934 | y_pred: 65.18314579896446\n",
            "Training batch[3]: last record -> y: 43.48930025598531 | y_pred: 45.85085798239629\n",
            "Training batch[4]: last record -> y: 57.24819927014278 | y_pred: 52.90467625068345\n",
            "Training batch[5]: last record -> y: 55.84130088275674 | y_pred: 51.8253927894109\n",
            "Training batch[6]: last record -> y: 64.74639772663613 | y_pred: 58.974553005900134\n",
            "Training batch[7]: last record -> y: 32.25839972032844 | y_pred: 30.68651351315276\n",
            "Training batch[8]: last record -> y: 55.98460004960816 | y_pred: 55.50113622375011\n",
            "Training batch[9]: last record -> y: 44.53200069911509 | y_pred: 43.944429087791946\n",
            "Training batch[10]: last record -> y: 69.42099633663565 | y_pred: 71.91434394769453\n",
            "Training batch[11]: last record -> y: 42.59540195074237 | y_pred: 44.22704733578098\n",
            "Training batch[12]: last record -> y: 28.118200384631052 | y_pred: 31.120899263899844\n",
            "Training batch[13]: last record -> y: 48.79289874727124 | y_pred: 53.78176814146377\n",
            "Training batch[14]: last record -> y: 43.48930025598531 | y_pred: 43.819412585376995\n",
            "Training batch[15]: last record -> y: 48.53829869459196 | y_pred: 46.24667790866317\n",
            "Training batch[16]: last record -> y: 27.66959969745278 | y_pred: 27.497926137721663\n",
            "Training batch[17]: last record -> y: 43.91579820049958 | y_pred: 44.69739537211103\n",
            "Training batch[18]: last record -> y: 31.24950025563038 | y_pred: 32.3315737971912\n",
            "Training batch[19]: last record -> y: 61.352299630444804 | y_pred: 64.22826936435513\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.20912680502556\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.370239066604995\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.009299387663646\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.10657459833158\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.96915495975219\n",
            "[Training] Epoch: 950 | L1Loss: 0.06018 | RMSE: 0.08962 | PLCC: 0.93549 | SROCC: 0.93043\n",
            "[Testing]  Epoch: 950 | L1Loss: 0.07839 | RMSE: 0.10568 | PLCC: 0.91886 | SROCC: 0.93453\n",
            "Training batch[0]: last record -> y: 54.96030127145741 | y_pred: 53.86315164764733\n",
            "Training batch[1]: last record -> y: 56.25930154528646 | y_pred: 53.142957147878406\n",
            "Training batch[2]: last record -> y: 53.082598142956385 | y_pred: 51.54310420508705\n",
            "Training batch[3]: last record -> y: 34.00809927198486 | y_pred: 37.420148760832035\n",
            "Training batch[4]: last record -> y: 26.46649938788346 | y_pred: 33.48974657759999\n",
            "Training batch[5]: last record -> y: 37.4706001665287 | y_pred: 37.46506503318449\n",
            "Training batch[6]: last record -> y: 53.64059811945481 | y_pred: 53.49585164870746\n",
            "Training batch[7]: last record -> y: 63.25490281841758 | y_pred: 63.72854105893771\n",
            "Training batch[8]: last record -> y: 41.8042976006501 | y_pred: 37.41375167761203\n",
            "Training batch[9]: last record -> y: 53.863000484795975 | y_pred: 55.05680106277623\n",
            "Training batch[10]: last record -> y: 65.84580192829299 | y_pred: 66.75538739965236\n",
            "Training batch[11]: last record -> y: 64.8636003961285 | y_pred: 62.912068687780675\n",
            "Training batch[12]: last record -> y: 44.09870203440539 | y_pred: 44.58857580840697\n",
            "Training batch[13]: last record -> y: 37.43249908741063 | y_pred: 49.503569987404944\n",
            "Training batch[14]: last record -> y: 28.917199777475616 | y_pred: 32.63068809336295\n",
            "Training batch[15]: last record -> y: 56.977000249892626 | y_pred: 55.475943487268296\n",
            "Training batch[16]: last record -> y: 67.50669893318377 | y_pred: 67.63866410241462\n",
            "Training batch[17]: last record -> y: 55.71429835777349 | y_pred: 49.74489343901223\n",
            "Training batch[18]: last record -> y: 56.90710190418099 | y_pred: 58.358767009183566\n",
            "Training batch[19]: last record -> y: 47.25970218946463 | y_pred: 49.44004621129716\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.0552558872721\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.70618885504359\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.24702548503183\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.39815954416008\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.45430127638926\n",
            "[Training] Epoch: 951 | L1Loss: 0.04878 | RMSE: 0.08066 | PLCC: 0.94797 | SROCC: 0.94904\n",
            "[Testing]  Epoch: 951 | L1Loss: 0.07925 | RMSE: 0.10728 | PLCC: 0.91903 | SROCC: 0.93380\n",
            "Training batch[0]: last record -> y: 31.80629896989518 | y_pred: 31.648539629007985\n",
            "Training batch[1]: last record -> y: 63.3563009293664 | y_pred: 64.16170946629677\n",
            "Training batch[2]: last record -> y: 48.53810250450829 | y_pred: 42.643987942528724\n",
            "Training batch[3]: last record -> y: 54.23470028757947 | y_pred: 52.93219432209139\n",
            "Training batch[4]: last record -> y: 62.41920060282996 | y_pred: 57.57067465931027\n",
            "Training batch[5]: last record -> y: 49.9361015810498 | y_pred: 48.16117311031417\n",
            "Training batch[6]: last record -> y: 25.27400017732296 | y_pred: 45.06823483313417\n",
            "Training batch[7]: last record -> y: 30.71260036181917 | y_pred: 32.04937446327426\n",
            "Training batch[8]: last record -> y: 57.104697480745926 | y_pred: 54.03228679715653\n",
            "Training batch[9]: last record -> y: 49.19860054291644 | y_pred: 55.34780241655358\n",
            "Training batch[10]: last record -> y: 59.05309979644767 | y_pred: 58.07165407853995\n",
            "Training batch[11]: last record -> y: 36.78459902535883 | y_pred: 36.63431742126727\n",
            "Training batch[12]: last record -> y: 56.977000249892626 | y_pred: 56.301726846822476\n",
            "Training batch[13]: last record -> y: 52.90630044727777 | y_pred: 54.60039217107942\n",
            "Training batch[14]: last record -> y: 38.79510193500403 | y_pred: 41.758459878150575\n",
            "Training batch[15]: last record -> y: 51.062699014795726 | y_pred: 49.5746776359274\n",
            "Training batch[16]: last record -> y: 53.07020278914547 | y_pred: 56.08543210392213\n",
            "Training batch[17]: last record -> y: 29.976800709821248 | y_pred: 30.496712472125125\n",
            "Training batch[18]: last record -> y: 57.950199551824426 | y_pred: 57.03997083428908\n",
            "Training batch[19]: last record -> y: 30.080500033964597 | y_pred: 30.291808814943465\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.65095008968228\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.8399165187966\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.860440965736416\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.138206229034836\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.521472660343647\n",
            "[Training] Epoch: 952 | L1Loss: 0.04902 | RMSE: 0.07994 | PLCC: 0.94924 | SROCC: 0.94950\n",
            "[Testing]  Epoch: 952 | L1Loss: 0.07849 | RMSE: 0.10682 | PLCC: 0.91910 | SROCC: 0.93341\n",
            "Training batch[0]: last record -> y: 60.22169798925347 | y_pred: 56.146932870806495\n",
            "Training batch[1]: last record -> y: 38.3894998425161 | y_pred: 38.382638013933956\n",
            "Training batch[2]: last record -> y: 47.40309784324245 | y_pred: 47.88285013866516\n",
            "Training batch[3]: last record -> y: 28.923200460239684 | y_pred: 27.10642319735325\n",
            "Training batch[4]: last record -> y: 69.90390053832061 | y_pred: 66.08867238695348\n",
            "Training batch[5]: last record -> y: 44.302501720337546 | y_pred: 44.876973623287086\n",
            "Training batch[6]: last record -> y: 48.559300682237335 | y_pred: 49.87628933537735\n",
            "Training batch[7]: last record -> y: 59.64789988667326 | y_pred: 61.45848349294283\n",
            "Training batch[8]: last record -> y: 58.68100118103507 | y_pred: 58.30366975798097\n",
            "Training batch[9]: last record -> y: 29.528200022642977 | y_pred: 35.52005101671466\n",
            "Training batch[10]: last record -> y: 48.25579783903004 | y_pred: 48.49713093932996\n",
            "Training batch[11]: last record -> y: 52.886501329981456 | y_pred: 52.748544322621456\n",
            "Training batch[12]: last record -> y: 68.26640161308069 | y_pred: 66.53553872162979\n",
            "Training batch[13]: last record -> y: 67.43609623290945 | y_pred: 67.74624059288362\n",
            "Training batch[14]: last record -> y: 27.51029978197414 | y_pred: 27.822923051898954\n",
            "Training batch[15]: last record -> y: 33.06290047609298 | y_pred: 33.75946050324137\n",
            "Training batch[16]: last record -> y: 51.99429958652763 | y_pred: 53.45023584613864\n",
            "Training batch[17]: last record -> y: 69.84349972239715 | y_pred: 68.84167918186563\n",
            "Training batch[18]: last record -> y: 44.64339807186934 | y_pred: 44.11574484183768\n",
            "Training batch[19]: last record -> y: 63.999000346085495 | y_pred: 62.423812677911656\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.77252185497139\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.20926173286898\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.25531195004021\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.08242552877027\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.481891310963135\n",
            "[Training] Epoch: 953 | L1Loss: 0.04861 | RMSE: 0.08151 | PLCC: 0.94783 | SROCC: 0.94917\n",
            "[Testing]  Epoch: 953 | L1Loss: 0.07445 | RMSE: 0.10110 | PLCC: 0.91873 | SROCC: 0.93301\n",
            "Training batch[0]: last record -> y: 56.76089848084507 | y_pred: 57.64474124024218\n",
            "Training batch[1]: last record -> y: 55.55740096676209 | y_pred: 56.999410946663374\n",
            "Training batch[2]: last record -> y: 62.23139844929415 | y_pred: 61.298260519201676\n",
            "Training batch[3]: last record -> y: 32.369998911570406 | y_pred: 34.617785686837806\n",
            "Training batch[4]: last record -> y: 31.658599999200135 | y_pred: 32.28290659557513\n",
            "Training batch[5]: last record -> y: 26.46649938788346 | y_pred: 34.102674149122095\n",
            "Training batch[6]: last record -> y: 55.44500012997332 | y_pred: 53.51896026757913\n",
            "Training batch[7]: last record -> y: 44.16859877200159 | y_pred: 42.72520581282197\n",
            "Training batch[8]: last record -> y: 60.175200939423576 | y_pred: 58.88401932286342\n",
            "Training batch[9]: last record -> y: 26.304599953796185 | y_pred: 35.30978831482946\n",
            "Training batch[10]: last record -> y: 36.67800187719274 | y_pred: 37.84458510135232\n",
            "Training batch[11]: last record -> y: 52.48210210784259 | y_pred: 53.71306944987032\n",
            "Training batch[12]: last record -> y: 37.98649968080997 | y_pred: 37.8269488993227\n",
            "Training batch[13]: last record -> y: 50.61160012028154 | y_pred: 55.36556244347207\n",
            "Training batch[14]: last record -> y: 63.02299970705735 | y_pred: 59.13254391541068\n",
            "Training batch[15]: last record -> y: 54.546501006456765 | y_pred: 50.51959661973274\n",
            "Training batch[16]: last record -> y: 69.68309663972673 | y_pred: 68.21870812536349\n",
            "Training batch[17]: last record -> y: 54.76450035172343 | y_pred: 55.406968199819175\n",
            "Training batch[18]: last record -> y: 48.6593994359107 | y_pred: 46.92654087319886\n",
            "Training batch[19]: last record -> y: 65.65200150416626 | y_pred: 65.79022867492017\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.106606229499675\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.47375345747275\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.59683079610079\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.24124300961694\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.26619036468898\n",
            "[Training] Epoch: 954 | L1Loss: 0.05023 | RMSE: 0.08226 | PLCC: 0.94639 | SROCC: 0.94698\n",
            "[Testing]  Epoch: 954 | L1Loss: 0.07848 | RMSE: 0.10608 | PLCC: 0.91911 | SROCC: 0.93473\n",
            "Training batch[0]: last record -> y: 50.21070015733994 | y_pred: 50.121372560883174\n",
            "Training batch[1]: last record -> y: 43.26480090811049 | y_pred: 49.343231229352114\n",
            "Training batch[2]: last record -> y: 34.8158990765744 | y_pred: 33.02650641151439\n",
            "Training batch[3]: last record -> y: 61.12310136925453 | y_pred: 60.84589121348995\n",
            "Training batch[4]: last record -> y: 40.48429855540655 | y_pred: 39.03448278315989\n",
            "Training batch[5]: last record -> y: 41.07529866884761 | y_pred: 41.02481670895759\n",
            "Training batch[6]: last record -> y: 29.49510018254307 | y_pred: 46.7795784193753\n",
            "Training batch[7]: last record -> y: 56.73359911313696 | y_pred: 55.75813880089663\n",
            "Training batch[8]: last record -> y: 67.50669893318377 | y_pred: 68.25567226686553\n",
            "Training batch[9]: last record -> y: 68.08779787986123 | y_pred: 64.7843942782506\n",
            "Training batch[10]: last record -> y: 26.635299647352298 | y_pred: 37.45429870031421\n",
            "Training batch[11]: last record -> y: 51.65370073635222 | y_pred: 50.82326027448403\n",
            "Training batch[12]: last record -> y: 60.54589727817256 | y_pred: 61.35136692348965\n",
            "Training batch[13]: last record -> y: 28.917199777475616 | y_pred: 33.8102287076797\n",
            "Training batch[14]: last record -> y: 55.39879897338437 | y_pred: 54.88844745737083\n",
            "Training batch[15]: last record -> y: 51.062699014795726 | y_pred: 50.48860823520545\n",
            "Training batch[16]: last record -> y: 39.74829788897864 | y_pred: 31.98989670561332\n",
            "Training batch[17]: last record -> y: 37.30360059432371 | y_pred: 37.76675070594467\n",
            "Training batch[18]: last record -> y: 69.23919566992163 | y_pred: 68.45713053671716\n",
            "Training batch[19]: last record -> y: 46.69590013245897 | y_pred: 55.217673715155115\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.469340634852074\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.923472435085955\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.35357086579677\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.89228678350025\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.471625101994675\n",
            "[Training] Epoch: 955 | L1Loss: 0.05008 | RMSE: 0.08164 | PLCC: 0.94753 | SROCC: 0.94821\n",
            "[Testing]  Epoch: 955 | L1Loss: 0.07613 | RMSE: 0.10417 | PLCC: 0.91747 | SROCC: 0.93228\n",
            "Training batch[0]: last record -> y: 62.891102078674976 | y_pred: 63.89028209365415\n",
            "Training batch[1]: last record -> y: 28.998800379243562 | y_pred: 28.572246954746618\n",
            "Training batch[2]: last record -> y: 58.68100118103507 | y_pred: 57.848971901112236\n",
            "Training batch[3]: last record -> y: 55.44500012997332 | y_pred: 55.43414213452297\n",
            "Training batch[4]: last record -> y: 58.23469768676273 | y_pred: 60.49765059874403\n",
            "Training batch[5]: last record -> y: 56.96869915799175 | y_pred: 56.75063227187661\n",
            "Training batch[6]: last record -> y: 26.82169912219564 | y_pred: 29.785474371298847\n",
            "Training batch[7]: last record -> y: 53.07039897922914 | y_pred: 52.36700606957652\n",
            "Training batch[8]: last record -> y: 25.27400017732296 | y_pred: 45.875639041325485\n",
            "Training batch[9]: last record -> y: 29.39489931353927 | y_pred: 30.27095879420716\n",
            "Training batch[10]: last record -> y: 56.977000249892626 | y_pred: 59.063829142662826\n",
            "Training batch[11]: last record -> y: 40.94380146120977 | y_pred: 43.719429616015304\n",
            "Training batch[12]: last record -> y: 29.928600665740476 | y_pred: 30.328357258604285\n",
            "Training batch[13]: last record -> y: 21.73069952250026 | y_pred: 22.27907110881658\n",
            "Training batch[14]: last record -> y: 69.7550951842029 | y_pred: 64.33313457219242\n",
            "Training batch[15]: last record -> y: 31.658599999200135 | y_pred: 31.82893882311612\n",
            "Training batch[16]: last record -> y: 38.56610147201286 | y_pred: 37.749447383811116\n",
            "Training batch[17]: last record -> y: 31.007400888323332 | y_pred: 30.31718809281631\n",
            "Training batch[18]: last record -> y: 68.24639987323894 | y_pred: 69.38954481486167\n",
            "Training batch[19]: last record -> y: 65.06269795251433 | y_pred: 65.74450352050144\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.95851166240641\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.51787677505979\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.889115425999194\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.18575498636244\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.96334524069661\n",
            "[Training] Epoch: 956 | L1Loss: 0.04990 | RMSE: 0.08057 | PLCC: 0.94868 | SROCC: 0.94842\n",
            "[Testing]  Epoch: 956 | L1Loss: 0.07575 | RMSE: 0.10295 | PLCC: 0.91773 | SROCC: 0.93365\n",
            "Training batch[0]: last record -> y: 34.00809927198486 | y_pred: 38.12054896765096\n",
            "Training batch[1]: last record -> y: 47.35749812182803 | y_pred: 46.25736222764601\n",
            "Training batch[2]: last record -> y: 48.53810250450829 | y_pred: 41.35652589698111\n",
            "Training batch[3]: last record -> y: 48.61320149555263 | y_pred: 46.970151355814096\n",
            "Training batch[4]: last record -> y: 54.38159841678544 | y_pred: 49.98633267493119\n",
            "Training batch[5]: last record -> y: 21.73069952250026 | y_pred: 22.464813266473854\n",
            "Training batch[6]: last record -> y: 70.23500185870785 | y_pred: 67.94421890092326\n",
            "Training batch[7]: last record -> y: 66.95040034282079 | y_pred: 65.98958352977638\n",
            "Training batch[8]: last record -> y: 38.469501977536765 | y_pred: 38.271567088614006\n",
            "Training batch[9]: last record -> y: 64.08940216365613 | y_pred: 61.001804437852115\n",
            "Training batch[10]: last record -> y: 64.74639772663613 | y_pred: 62.68563316891618\n",
            "Training batch[11]: last record -> y: 60.85200205216165 | y_pred: 59.17136382213039\n",
            "Training batch[12]: last record -> y: 38.69430204299886 | y_pred: 43.15221513804613\n",
            "Training batch[13]: last record -> y: 67.85620031043459 | y_pred: 70.18340386670252\n",
            "Training batch[14]: last record -> y: 72.168403673586 | y_pred: 67.11754786164624\n",
            "Training batch[15]: last record -> y: 31.38129978897092 | y_pred: 31.846060428204964\n",
            "Training batch[16]: last record -> y: 59.229098382654456 | y_pred: 61.10785000242231\n",
            "Training batch[17]: last record -> y: 53.03879951083468 | y_pred: 52.42385938283951\n",
            "Training batch[18]: last record -> y: 42.74340003090924 | y_pred: 42.87540540302609\n",
            "Training batch[19]: last record -> y: 49.315497670475224 | y_pred: 49.08670787060669\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.16646671863532\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.806847079036515\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.40241446376058\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.317869710329205\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.337202330342762\n",
            "[Training] Epoch: 957 | L1Loss: 0.04990 | RMSE: 0.08132 | PLCC: 0.94725 | SROCC: 0.94766\n",
            "[Testing]  Epoch: 957 | L1Loss: 0.07546 | RMSE: 0.10265 | PLCC: 0.91860 | SROCC: 0.93395\n",
            "Training batch[0]: last record -> y: 43.779598863315414 | y_pred: 43.2978123018612\n",
            "Training batch[1]: last record -> y: 70.58750076313868 | y_pred: 65.58541587626087\n",
            "Training batch[2]: last record -> y: 59.38040274816581 | y_pred: 59.3045318617103\n",
            "Training batch[3]: last record -> y: 46.635102112021855 | y_pred: 46.540442004766305\n",
            "Training batch[4]: last record -> y: 46.40150083075707 | y_pred: 42.1906923623236\n",
            "Training batch[5]: last record -> y: 37.4706001665287 | y_pred: 36.88784646906356\n",
            "Training batch[6]: last record -> y: 53.30369793479122 | y_pred: 60.57202915407129\n",
            "Training batch[7]: last record -> y: 71.94249883281668 | y_pred: 69.4117175105473\n",
            "Training batch[8]: last record -> y: 62.23139844929415 | y_pred: 60.27506490824271\n",
            "Training batch[9]: last record -> y: 54.171601053947825 | y_pred: 54.71470666524169\n",
            "Training batch[10]: last record -> y: 27.770800010202493 | y_pred: 28.16237853199658\n",
            "Training batch[11]: last record -> y: 29.03989978959936 | y_pred: 29.032643168676714\n",
            "Training batch[12]: last record -> y: 59.08630094782029 | y_pred: 58.29505990791563\n",
            "Training batch[13]: last record -> y: 35.17409875023975 | y_pred: 49.92213349033864\n",
            "Training batch[14]: last record -> y: 40.68579863625962 | y_pred: 47.22316259043873\n",
            "Training batch[15]: last record -> y: 64.4866034610859 | y_pred: 61.2920660585271\n",
            "Training batch[16]: last record -> y: 59.62590086745513 | y_pred: 58.24653341640055\n",
            "Training batch[17]: last record -> y: 38.522899450719365 | y_pred: 40.265828032315994\n",
            "Training batch[18]: last record -> y: 44.302501720337546 | y_pred: 44.53297039272536\n",
            "Training batch[19]: last record -> y: 44.28150134080761 | y_pred: 42.180591789245454\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.182787482235085\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 47.354108214480675\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.95010481217366\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.144355662477096\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.325202170901207\n",
            "[Training] Epoch: 958 | L1Loss: 0.04780 | RMSE: 0.08039 | PLCC: 0.94891 | SROCC: 0.94945\n",
            "[Testing]  Epoch: 958 | L1Loss: 0.08085 | RMSE: 0.10930 | PLCC: 0.91889 | SROCC: 0.93556\n",
            "Training batch[0]: last record -> y: 57.24819927014278 | y_pred: 54.84253576156107\n",
            "Training batch[1]: last record -> y: 60.20750154614984 | y_pred: 59.41263259781272\n",
            "Training batch[2]: last record -> y: 61.01490092999484 | y_pred: 53.03445438291601\n",
            "Training batch[3]: last record -> y: 43.742801965819126 | y_pred: 59.55056387532568\n",
            "Training batch[4]: last record -> y: 67.4182976112204 | y_pred: 64.4032805676818\n",
            "Training batch[5]: last record -> y: 45.30530160317198 | y_pred: 44.142579464183655\n",
            "Training batch[6]: last record -> y: 27.66959969745278 | y_pred: 27.243710424632525\n",
            "Training batch[7]: last record -> y: 39.65230143779252 | y_pred: 40.42911929031652\n",
            "Training batch[8]: last record -> y: 47.787299135455896 | y_pred: 50.90714922452298\n",
            "Training batch[9]: last record -> y: 24.490699609431772 | y_pred: 24.883382906533086\n",
            "Training batch[10]: last record -> y: 60.042196927618534 | y_pred: 58.18086441429591\n",
            "Training batch[11]: last record -> y: 67.7326970446486 | y_pred: 66.0718354182975\n",
            "Training batch[12]: last record -> y: 48.248902240023654 | y_pred: 47.794329816159234\n",
            "Training batch[13]: last record -> y: 55.70909771244078 | y_pred: 57.62733499872047\n",
            "Training batch[14]: last record -> y: 31.165001025781976 | y_pred: 33.26912278793537\n",
            "Training batch[15]: last record -> y: 46.635102112021855 | y_pred: 47.62643934800053\n",
            "Training batch[16]: last record -> y: 62.891102078674976 | y_pred: 60.98461368379935\n",
            "Training batch[17]: last record -> y: 69.68309663972673 | y_pred: 65.59988891522016\n",
            "Training batch[18]: last record -> y: 65.49870307550941 | y_pred: 63.40925617080302\n",
            "Training batch[19]: last record -> y: 48.22529832159648 | y_pred: 48.30589706744536\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.57567742217043\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.276567950345\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.46525961515272\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 40.33904392018019\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.664637147526975\n",
            "[Training] Epoch: 959 | L1Loss: 0.05486 | RMSE: 0.08406 | PLCC: 0.94325 | SROCC: 0.94252\n",
            "[Testing]  Epoch: 959 | L1Loss: 0.07685 | RMSE: 0.10409 | PLCC: 0.91838 | SROCC: 0.93421\n",
            "Training batch[0]: last record -> y: 46.50630171397677 | y_pred: 47.83158985090222\n",
            "Training batch[1]: last record -> y: 47.66609869097988 | y_pred: 51.01964976446902\n",
            "Training batch[2]: last record -> y: 66.4921035235975 | y_pred: 66.99683306803308\n",
            "Training batch[3]: last record -> y: 55.39879897338437 | y_pred: 55.449264852119995\n",
            "Training batch[4]: last record -> y: 58.416900382336735 | y_pred: 58.37452010803304\n",
            "Training batch[5]: last record -> y: 54.97959865673647 | y_pred: 61.292352303075404\n",
            "Training batch[6]: last record -> y: 26.646600678606347 | y_pred: 44.821112517250526\n",
            "Training batch[7]: last record -> y: 38.56610147201286 | y_pred: 37.905286634863046\n",
            "Training batch[8]: last record -> y: 38.86930038140201 | y_pred: 44.584806385815796\n",
            "Training batch[9]: last record -> y: 29.928600665740476 | y_pred: 30.194078011255385\n",
            "Training batch[10]: last record -> y: 67.75769680727763 | y_pred: 68.92963023150583\n",
            "Training batch[11]: last record -> y: 56.90710190418099 | y_pred: 57.49681713338555\n",
            "Training batch[12]: last record -> y: 28.998800379243562 | y_pred: 27.763002258434312\n",
            "Training batch[13]: last record -> y: 58.68100118103507 | y_pred: 56.02957260600101\n",
            "Training batch[14]: last record -> y: 54.96030127145741 | y_pred: 54.295991999456646\n",
            "Training batch[15]: last record -> y: 40.69929876537776 | y_pred: 56.42929864467146\n",
            "Training batch[16]: last record -> y: 48.53829869459196 | y_pred: 50.177579411739316\n",
            "Training batch[17]: last record -> y: 64.4866034610859 | y_pred: 61.147602616097174\n",
            "Training batch[18]: last record -> y: 39.941398782888996 | y_pred: 55.90233207993265\n",
            "Training batch[19]: last record -> y: 28.55800066006435 | y_pred: 28.319158530580864\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.5710410715036\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.750537154938684\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.39222368236426\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.25582218419527\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.460490108659798\n",
            "[Training] Epoch: 960 | L1Loss: 0.05121 | RMSE: 0.08243 | PLCC: 0.94699 | SROCC: 0.94740\n",
            "[Testing]  Epoch: 960 | L1Loss: 0.07443 | RMSE: 0.10055 | PLCC: 0.91894 | SROCC: 0.93347\n",
            "Training batch[0]: last record -> y: 51.931000946581435 | y_pred: 59.55410172929351\n",
            "Training batch[1]: last record -> y: 70.23500185870785 | y_pred: 67.24891159570257\n",
            "Training batch[2]: last record -> y: 65.86109832235752 | y_pred: 64.09707609053544\n",
            "Training batch[3]: last record -> y: 30.71260036181917 | y_pred: 32.34175236386818\n",
            "Training batch[4]: last record -> y: 66.92800251097356 | y_pred: 68.97507557383801\n",
            "Training batch[5]: last record -> y: 47.35749812182803 | y_pred: 47.92104609659418\n",
            "Training batch[6]: last record -> y: 54.441201607450694 | y_pred: 59.9500390479875\n",
            "Training batch[7]: last record -> y: 25.895799721727116 | y_pred: 26.376187624777458\n",
            "Training batch[8]: last record -> y: 43.31629919695854 | y_pred: 42.88693880696121\n",
            "Training batch[9]: last record -> y: 47.797999535593135 | y_pred: 46.5549472060344\n",
            "Training batch[10]: last record -> y: 63.459201020136106 | y_pred: 63.01147273558399\n",
            "Training batch[11]: last record -> y: 67.43609623290945 | y_pred: 69.55628708859626\n",
            "Training batch[12]: last record -> y: 28.998800379243562 | y_pred: 28.700847946477154\n",
            "Training batch[13]: last record -> y: 38.86930038140201 | y_pred: 44.636362566819685\n",
            "Training batch[14]: last record -> y: 35.84120131875966 | y_pred: 48.23989036109833\n",
            "Training batch[15]: last record -> y: 61.02579752021575 | y_pred: 58.7305729475861\n",
            "Training batch[16]: last record -> y: 42.74340003090924 | y_pred: 41.58892591601227\n",
            "Training batch[17]: last record -> y: 46.12870012752876 | y_pred: 43.816969858023754\n",
            "Training batch[18]: last record -> y: 32.322799919350985 | y_pred: 30.375192816734284\n",
            "Training batch[19]: last record -> y: 48.95560143502075 | y_pred: 46.46110723765321\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.49830133966316\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.01703098326516\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.374774175579205\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.7754380532582\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.910846455869603\n",
            "[Training] Epoch: 961 | L1Loss: 0.04835 | RMSE: 0.08120 | PLCC: 0.94818 | SROCC: 0.94934\n",
            "[Testing]  Epoch: 961 | L1Loss: 0.07639 | RMSE: 0.10410 | PLCC: 0.91830 | SROCC: 0.93380\n",
            "Training batch[0]: last record -> y: 68.8820042846371 | y_pred: 66.16356553922151\n",
            "Training batch[1]: last record -> y: 54.96030127145741 | y_pred: 52.84903867269304\n",
            "Training batch[2]: last record -> y: 60.20750154614984 | y_pred: 56.19131685694833\n",
            "Training batch[3]: last record -> y: 34.8158990765744 | y_pred: 33.37855906391064\n",
            "Training batch[4]: last record -> y: 54.46919889725973 | y_pred: 54.68030585975089\n",
            "Training batch[5]: last record -> y: 63.38790039776086 | y_pred: 65.06752873129585\n",
            "Training batch[6]: last record -> y: 60.984999631504934 | y_pred: 62.913072151815186\n",
            "Training batch[7]: last record -> y: 67.50669893318377 | y_pred: 67.7717099252211\n",
            "Training batch[8]: last record -> y: 38.469501977536765 | y_pred: 37.88729343120576\n",
            "Training batch[9]: last record -> y: 40.42119932177491 | y_pred: 39.398378401713444\n",
            "Training batch[10]: last record -> y: 30.080500033964597 | y_pred: 29.90901623185539\n",
            "Training batch[11]: last record -> y: 53.90739733586133 | y_pred: 60.48988661740009\n",
            "Training batch[12]: last record -> y: 38.67589876990439 | y_pred: 52.53776541568004\n",
            "Training batch[13]: last record -> y: 51.224099129038905 | y_pred: 60.96111590099122\n",
            "Training batch[14]: last record -> y: 58.383400121492286 | y_pred: 56.19485149468528\n",
            "Training batch[15]: last record -> y: 61.4574993262936 | y_pred: 61.63198628398686\n",
            "Training batch[16]: last record -> y: 59.409998504722125 | y_pred: 59.23174212443769\n",
            "Training batch[17]: last record -> y: 67.77610008037209 | y_pred: 64.50037214548252\n",
            "Training batch[18]: last record -> y: 61.653901681202115 | y_pred: 65.00092380600518\n",
            "Training batch[19]: last record -> y: 59.05309979644767 | y_pred: 60.03213977365726\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.25085257634521\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.131692830362454\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.122132961359284\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.09872377875388\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.03298025750496\n",
            "[Training] Epoch: 962 | L1Loss: 0.05186 | RMSE: 0.08236 | PLCC: 0.94611 | SROCC: 0.94453\n",
            "[Testing]  Epoch: 962 | L1Loss: 0.07704 | RMSE: 0.10448 | PLCC: 0.91970 | SROCC: 0.93407\n",
            "Training batch[0]: last record -> y: 67.38690076537137 | y_pred: 69.3425492492454\n",
            "Training batch[1]: last record -> y: 67.77610008037209 | y_pred: 64.39719224262626\n",
            "Training batch[2]: last record -> y: 24.91699975574693 | y_pred: 24.970930721226722\n",
            "Training batch[3]: last record -> y: 36.45529879426431 | y_pred: 36.422523793483265\n",
            "Training batch[4]: last record -> y: 48.33070063999071 | y_pred: 47.90329250213745\n",
            "Training batch[5]: last record -> y: 67.7326970446486 | y_pred: 66.90317964104293\n",
            "Training batch[6]: last record -> y: 56.977000249892626 | y_pred: 58.15459424046935\n",
            "Training batch[7]: last record -> y: 54.76450035172343 | y_pred: 56.788947230348185\n",
            "Training batch[8]: last record -> y: 36.61619878460567 | y_pred: 40.84470777968636\n",
            "Training batch[9]: last record -> y: 63.91980066066935 | y_pred: 62.00612720600816\n",
            "Training batch[10]: last record -> y: 65.36119955670347 | y_pred: 62.21058621927068\n",
            "Training batch[11]: last record -> y: 64.08940216365613 | y_pred: 58.71391287162851\n",
            "Training batch[12]: last record -> y: 26.46649938788346 | y_pred: 34.66430846651474\n",
            "Training batch[13]: last record -> y: 35.84120131875966 | y_pred: 49.15871284754462\n",
            "Training batch[14]: last record -> y: 70.7566005341082 | y_pred: 68.54605288808307\n",
            "Training batch[15]: last record -> y: 62.23139844929415 | y_pred: 61.73842744495528\n",
            "Training batch[16]: last record -> y: 34.048697754381124 | y_pred: 34.30133429810826\n",
            "Training batch[17]: last record -> y: 35.17409875023975 | y_pred: 47.224963679731445\n",
            "Training batch[18]: last record -> y: 69.43119822098652 | y_pred: 67.74859487388767\n",
            "Training batch[19]: last record -> y: 47.40309784324245 | y_pred: 46.6107375549916\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.4309774329173\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.24985072042614\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.806586633915344\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.847306186530204\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.13233204155648\n",
            "[Training] Epoch: 963 | L1Loss: 0.05193 | RMSE: 0.08183 | PLCC: 0.94624 | SROCC: 0.94651\n",
            "[Testing]  Epoch: 963 | L1Loss: 0.07704 | RMSE: 0.10536 | PLCC: 0.91767 | SROCC: 0.93418\n",
            "Training batch[0]: last record -> y: 62.23139844929415 | y_pred: 60.677621230087425\n",
            "Training batch[1]: last record -> y: 56.89680031667285 | y_pred: 59.762317300223685\n",
            "Training batch[2]: last record -> y: 32.369998911570406 | y_pred: 32.92611739300412\n",
            "Training batch[3]: last record -> y: 53.63130321221206 | y_pred: 54.64847803896396\n",
            "Training batch[4]: last record -> y: 49.19730118564098 | y_pred: 58.3937531686945\n",
            "Training batch[5]: last record -> y: 56.92869889453914 | y_pred: 59.60639442716888\n",
            "Training batch[6]: last record -> y: 46.11660066695879 | y_pred: 38.173681101785974\n",
            "Training batch[7]: last record -> y: 56.76089848084507 | y_pred: 59.27831314757782\n",
            "Training batch[8]: last record -> y: 22.983800965443066 | y_pred: 23.64055330096278\n",
            "Training batch[9]: last record -> y: 25.219101126348335 | y_pred: 25.13461596751003\n",
            "Training batch[10]: last record -> y: 25.112100743235686 | y_pred: 26.075087708066263\n",
            "Training batch[11]: last record -> y: 55.39879897338437 | y_pred: 53.64959391722573\n",
            "Training batch[12]: last record -> y: 67.4182976112204 | y_pred: 64.34565857523853\n",
            "Training batch[13]: last record -> y: 29.276899018788697 | y_pred: 33.64205199497269\n",
            "Training batch[14]: last record -> y: 43.4957005554362 | y_pred: 45.33848184904389\n",
            "Training batch[15]: last record -> y: 65.45760286109589 | y_pred: 65.78221704379848\n",
            "Training batch[16]: last record -> y: 61.12310136925453 | y_pred: 61.94885578273079\n",
            "Training batch[17]: last record -> y: 63.49129900431694 | y_pred: 60.69248343298318\n",
            "Training batch[18]: last record -> y: 38.79510193500403 | y_pred: 40.60340523357979\n",
            "Training batch[19]: last record -> y: 41.601200661165194 | y_pred: 43.229100745344226\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.597936648382756\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.12148275158086\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.09965924037931\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 37.437595205239745\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 23.145452347723733\n",
            "[Training] Epoch: 964 | L1Loss: 0.04985 | RMSE: 0.08163 | PLCC: 0.94740 | SROCC: 0.94777\n",
            "[Testing]  Epoch: 964 | L1Loss: 0.07682 | RMSE: 0.10390 | PLCC: 0.91756 | SROCC: 0.93306\n",
            "Training batch[0]: last record -> y: 26.000999417575912 | y_pred: 27.663313574198128\n",
            "Training batch[1]: last record -> y: 54.96030127145741 | y_pred: 53.60313867839727\n",
            "Training batch[2]: last record -> y: 30.420999180982733 | y_pred: 27.00605588840142\n",
            "Training batch[3]: last record -> y: 62.407599658046365 | y_pred: 64.05187196551924\n",
            "Training batch[4]: last record -> y: 49.19860054291644 | y_pred: 56.35896289156017\n",
            "Training batch[5]: last record -> y: 52.01419840698122 | y_pred: 55.019357702873094\n",
            "Training batch[6]: last record -> y: 30.074199437670984 | y_pred: 28.494020179171628\n",
            "Training batch[7]: last record -> y: 26.749900788091736 | y_pred: 26.291224855653496\n",
            "Training batch[8]: last record -> y: 48.6593994359107 | y_pred: 47.711331762073996\n",
            "Training batch[9]: last record -> y: 48.53829869459196 | y_pred: 49.960795801745235\n",
            "Training batch[10]: last record -> y: 65.80919800464949 | y_pred: 62.925798777406726\n",
            "Training batch[11]: last record -> y: 63.49129900431694 | y_pred: 61.47212674433513\n",
            "Training batch[12]: last record -> y: 39.74829788897864 | y_pred: 32.663362586929\n",
            "Training batch[13]: last record -> y: 57.048500278582424 | y_pred: 55.36084101654046\n",
            "Training batch[14]: last record -> y: 36.49660163122326 | y_pred: 35.94549725373122\n",
            "Training batch[15]: last record -> y: 43.08390078604282 | y_pred: 42.00327454837793\n",
            "Training batch[16]: last record -> y: 55.86600153591394 | y_pred: 56.28012985646433\n",
            "Training batch[17]: last record -> y: 72.99610069051369 | y_pred: 70.34211199569927\n",
            "Training batch[18]: last record -> y: 54.441201607450694 | y_pred: 59.87765455580575\n",
            "Training batch[19]: last record -> y: 47.797999535593135 | y_pred: 46.69245554918666\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.512428479949335\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.09288081036641\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.139284659080886\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.83042419064191\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.61420544124445\n",
            "[Training] Epoch: 965 | L1Loss: 0.04990 | RMSE: 0.08160 | PLCC: 0.94770 | SROCC: 0.94867\n",
            "[Testing]  Epoch: 965 | L1Loss: 0.07544 | RMSE: 0.10195 | PLCC: 0.91859 | SROCC: 0.93467\n",
            "Training batch[0]: last record -> y: 25.810800367976896 | y_pred: 18.07524829385413\n",
            "Training batch[1]: last record -> y: 54.171601053947825 | y_pred: 54.48506134795912\n",
            "Training batch[2]: last record -> y: 40.68579863625962 | y_pred: 46.903652566142455\n",
            "Training batch[3]: last record -> y: 45.587398821758484 | y_pred: 46.415402988735195\n",
            "Training batch[4]: last record -> y: 29.01670111626305 | y_pred: 29.002860870729364\n",
            "Training batch[5]: last record -> y: 28.998800379243562 | y_pred: 28.76091507844683\n",
            "Training batch[6]: last record -> y: 48.25579783903004 | y_pred: 47.35338777876359\n",
            "Training batch[7]: last record -> y: 26.044099725567833 | y_pred: 26.300715952979914\n",
            "Training batch[8]: last record -> y: 43.92589877357773 | y_pred: 43.224207250060545\n",
            "Training batch[9]: last record -> y: 46.96450043815821 | y_pred: 45.51991104109129\n",
            "Training batch[10]: last record -> y: 53.03879951083468 | y_pred: 53.130436361063175\n",
            "Training batch[11]: last record -> y: 63.33969874556465 | y_pred: 61.98206015033429\n",
            "Training batch[12]: last record -> y: 71.94249883281668 | y_pred: 70.32289179996133\n",
            "Training batch[13]: last record -> y: 61.28480059296953 | y_pred: 61.89585229783097\n",
            "Training batch[14]: last record -> y: 50.61160012028154 | y_pred: 55.308220263115345\n",
            "Training batch[15]: last record -> y: 28.55800066006435 | y_pred: 28.43094426321386\n",
            "Training batch[16]: last record -> y: 28.118200384631052 | y_pred: 27.80331610439771\n",
            "Training batch[17]: last record -> y: 29.276899018788697 | y_pred: 33.787724740213434\n",
            "Training batch[18]: last record -> y: 26.304599953796185 | y_pred: 34.12394951639226\n",
            "Training batch[19]: last record -> y: 40.48300241436198 | y_pred: 41.47529165468109\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.523399197334015\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.838776364949695\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.25745572177516\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.950027930339616\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.714751002834788\n",
            "[Training] Epoch: 966 | L1Loss: 0.04780 | RMSE: 0.07892 | PLCC: 0.95087 | SROCC: 0.95242\n",
            "[Testing]  Epoch: 966 | L1Loss: 0.07894 | RMSE: 0.10727 | PLCC: 0.91821 | SROCC: 0.93444\n",
            "Training batch[0]: last record -> y: 60.28530217113325 | y_pred: 58.852603179629114\n",
            "Training batch[1]: last record -> y: 65.59839658409192 | y_pred: 63.02307368036759\n",
            "Training batch[2]: last record -> y: 38.89670106038284 | y_pred: 38.07319961663791\n",
            "Training batch[3]: last record -> y: 44.533298448275104 | y_pred: 44.59696856288792\n",
            "Training batch[4]: last record -> y: 48.6593994359107 | y_pred: 46.63156908240035\n",
            "Training batch[5]: last record -> y: 30.420999180982733 | y_pred: 27.16805100535737\n",
            "Training batch[6]: last record -> y: 56.977000249892626 | y_pred: 56.99479243911992\n",
            "Training batch[7]: last record -> y: 36.49660163122326 | y_pred: 37.06932390457416\n",
            "Training batch[8]: last record -> y: 49.328201782450606 | y_pred: 50.01383788141561\n",
            "Training batch[9]: last record -> y: 58.383400121492286 | y_pred: 55.32941200838263\n",
            "Training batch[10]: last record -> y: 47.01879845598717 | y_pred: 41.30836605578634\n",
            "Training batch[11]: last record -> y: 34.149799972089 | y_pred: 33.88187346675909\n",
            "Training batch[12]: last record -> y: 48.559300682237335 | y_pred: 49.867753458622246\n",
            "Training batch[13]: last record -> y: 47.28070096087913 | y_pred: 50.10200763475564\n",
            "Training batch[14]: last record -> y: 61.02579752021575 | y_pred: 61.76262636609522\n",
            "Training batch[15]: last record -> y: 69.13320156504551 | y_pred: 65.46296431797259\n",
            "Training batch[16]: last record -> y: 58.70890198391771 | y_pred: 56.44104110361377\n",
            "Training batch[17]: last record -> y: 59.229098382654456 | y_pred: 59.14294198984521\n",
            "Training batch[18]: last record -> y: 64.75720104616153 | y_pred: 62.44137973097736\n",
            "Training batch[19]: last record -> y: 40.42119932177491 | y_pred: 39.09788112626336\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.61944067575246\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 47.523082552445885\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.967167070844766\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.47477353380236\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.701810746065064\n",
            "[Training] Epoch: 967 | L1Loss: 0.05113 | RMSE: 0.08181 | PLCC: 0.94639 | SROCC: 0.94832\n",
            "[Testing]  Epoch: 967 | L1Loss: 0.08346 | RMSE: 0.11144 | PLCC: 0.91986 | SROCC: 0.93584\n",
            "Training batch[0]: last record -> y: 33.64979989516223 | y_pred: 37.84606135132617\n",
            "Training batch[1]: last record -> y: 60.22169798925347 | y_pred: 57.48367883024139\n",
            "Training batch[2]: last record -> y: 30.71260036181917 | y_pred: 34.01165159899165\n",
            "Training batch[3]: last record -> y: 58.416900382336735 | y_pred: 60.363906853836625\n",
            "Training batch[4]: last record -> y: 69.43119822098652 | y_pred: 67.71177224654434\n",
            "Training batch[5]: last record -> y: 48.6593994359107 | y_pred: 44.56014754366004\n",
            "Training batch[6]: last record -> y: 55.77679937246148 | y_pred: 52.99643531768538\n",
            "Training batch[7]: last record -> y: 68.06819816887946 | y_pred: 64.11919089406524\n",
            "Training batch[8]: last record -> y: 64.53830115624851 | y_pred: 59.45992727290081\n",
            "Training batch[9]: last record -> y: 64.75720104616153 | y_pred: 65.30119433717823\n",
            "Training batch[10]: last record -> y: 41.07529866884761 | y_pred: 43.65737726553505\n",
            "Training batch[11]: last record -> y: 43.91640124378955 | y_pred: 48.079886091057006\n",
            "Training batch[12]: last record -> y: 27.960300333642863 | y_pred: 29.086031797224393\n",
            "Training batch[13]: last record -> y: 64.97390103415273 | y_pred: 63.812571523135375\n",
            "Training batch[14]: last record -> y: 39.47439884290486 | y_pred: 38.31372705110243\n",
            "Training batch[15]: last record -> y: 72.168403673586 | y_pred: 67.07549564289229\n",
            "Training batch[16]: last record -> y: 47.959997868779965 | y_pred: 46.490563088166255\n",
            "Training batch[17]: last record -> y: 31.38129978897092 | y_pred: 32.26268293580267\n",
            "Training batch[18]: last record -> y: 39.941398782888996 | y_pred: 58.164985882442124\n",
            "Training batch[19]: last record -> y: 25.608800967279734 | y_pred: 25.168731734539108\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.47140690933884\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.11610360543432\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.49499014569892\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.0290584559275\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.213406233693178\n",
            "[Training] Epoch: 968 | L1Loss: 0.05612 | RMSE: 0.08478 | PLCC: 0.94333 | SROCC: 0.94161\n",
            "[Testing]  Epoch: 968 | L1Loss: 0.07641 | RMSE: 0.10331 | PLCC: 0.91822 | SROCC: 0.93373\n",
            "Training batch[0]: last record -> y: 59.44990228124834 | y_pred: 60.3376624098571\n",
            "Training batch[1]: last record -> y: 55.6992978569499 | y_pred: 51.978607596064876\n",
            "Training batch[2]: last record -> y: 26.46649938788346 | y_pred: 33.069286302504906\n",
            "Training batch[3]: last record -> y: 59.62590086745513 | y_pred: 58.59970772931615\n",
            "Training batch[4]: last record -> y: 25.665500705518212 | y_pred: 24.56653551937646\n",
            "Training batch[5]: last record -> y: 44.8014983332298 | y_pred: 46.37920752641344\n",
            "Training batch[6]: last record -> y: 69.13320156504551 | y_pred: 66.26994237557233\n",
            "Training batch[7]: last record -> y: 56.32199875005813 | y_pred: 54.908487791983134\n",
            "Training batch[8]: last record -> y: 63.56399868712492 | y_pred: 67.84042469796896\n",
            "Training batch[9]: last record -> y: 27.821399362519628 | y_pred: 28.13281574180678\n",
            "Training batch[10]: last record -> y: 37.80239940901686 | y_pred: 38.87557846407947\n",
            "Training batch[11]: last record -> y: 57.412301018325024 | y_pred: 58.02621516866952\n",
            "Training batch[12]: last record -> y: 61.02579752021575 | y_pred: 59.962511591339535\n",
            "Training batch[13]: last record -> y: 43.501399716555284 | y_pred: 43.68848304248945\n",
            "Training batch[14]: last record -> y: 63.439498389766186 | y_pred: 66.97411361309787\n",
            "Training batch[15]: last record -> y: 24.490699609431772 | y_pred: 25.51235746000131\n",
            "Training batch[16]: last record -> y: 66.61560035692173 | y_pred: 68.10621723411009\n",
            "Training batch[17]: last record -> y: 52.01419840698122 | y_pred: 54.611967386015976\n",
            "Training batch[18]: last record -> y: 66.4921035235975 | y_pred: 64.37121474580977\n",
            "Training batch[19]: last record -> y: 68.3981030513794 | y_pred: 64.76612608685309\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.55320385504399\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.58598207205591\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.22372228419192\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.579172234308544\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.219318623961584\n",
            "[Training] Epoch: 969 | L1Loss: 0.04879 | RMSE: 0.08031 | PLCC: 0.94857 | SROCC: 0.94966\n",
            "[Testing]  Epoch: 969 | L1Loss: 0.07569 | RMSE: 0.10316 | PLCC: 0.91982 | SROCC: 0.93542\n",
            "Training batch[0]: last record -> y: 29.528200022642977 | y_pred: 35.45327563118735\n",
            "Training batch[1]: last record -> y: 48.25579783903004 | y_pred: 47.63156280379212\n",
            "Training batch[2]: last record -> y: 27.960300333642863 | y_pred: 27.449882080838563\n",
            "Training batch[3]: last record -> y: 65.84580192829299 | y_pred: 67.45967442148958\n",
            "Training batch[4]: last record -> y: 64.53830115624851 | y_pred: 61.133184253062836\n",
            "Training batch[5]: last record -> y: 62.10859953807085 | y_pred: 58.40555351979265\n",
            "Training batch[6]: last record -> y: 51.931000946581435 | y_pred: 61.33857597328051\n",
            "Training batch[7]: last record -> y: 61.352299630444804 | y_pred: 61.94599655347861\n",
            "Training batch[8]: last record -> y: 63.33969874556465 | y_pred: 61.26528772022152\n",
            "Training batch[9]: last record -> y: 35.84120131875966 | y_pred: 47.74098862701703\n",
            "Training batch[10]: last record -> y: 54.38009965319543 | y_pred: 63.2647444849099\n",
            "Training batch[11]: last record -> y: 51.65370073635222 | y_pred: 51.73433486074077\n",
            "Training batch[12]: last record -> y: 30.71260036181917 | y_pred: 31.000386288774394\n",
            "Training batch[13]: last record -> y: 29.01670111626305 | y_pred: 30.2426422934833\n",
            "Training batch[14]: last record -> y: 65.29879824517275 | y_pred: 65.52822164252461\n",
            "Training batch[15]: last record -> y: 60.984999631504934 | y_pred: 62.10632244660792\n",
            "Training batch[16]: last record -> y: 58.383400121492286 | y_pred: 56.761869782570784\n",
            "Training batch[17]: last record -> y: 68.25509656153804 | y_pred: 68.60511896819139\n",
            "Training batch[18]: last record -> y: 55.55740096676209 | y_pred: 55.07822759289775\n",
            "Training batch[19]: last record -> y: 39.99449875471521 | y_pred: 45.13911895361048\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.11024217850934\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.964313742913646\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.19890745483849\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.160380532835916\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.393237112846833\n",
            "[Training] Epoch: 970 | L1Loss: 0.04841 | RMSE: 0.08006 | PLCC: 0.94893 | SROCC: 0.94924\n",
            "[Testing]  Epoch: 970 | L1Loss: 0.07600 | RMSE: 0.10366 | PLCC: 0.91921 | SROCC: 0.93513\n",
            "Training batch[0]: last record -> y: 62.94929977644574 | y_pred: 61.94407646364334\n",
            "Training batch[1]: last record -> y: 47.6147001052891 | y_pred: 50.438737359182596\n",
            "Training batch[2]: last record -> y: 43.779598863315414 | y_pred: 44.40767247010842\n",
            "Training batch[3]: last record -> y: 38.48249876652221 | y_pred: 38.75170693985774\n",
            "Training batch[4]: last record -> y: 29.695998828221605 | y_pred: 46.18373787846008\n",
            "Training batch[5]: last record -> y: 59.352698135366836 | y_pred: 57.05800424083236\n",
            "Training batch[6]: last record -> y: 68.96460030986236 | y_pred: 66.62926612193019\n",
            "Training batch[7]: last record -> y: 49.315497670475224 | y_pred: 49.29961913862144\n",
            "Training batch[8]: last record -> y: 34.8158990765744 | y_pred: 34.52103020516404\n",
            "Training batch[9]: last record -> y: 38.56610147201286 | y_pred: 38.487969575248826\n",
            "Training batch[10]: last record -> y: 31.658599999200135 | y_pred: 31.95064180366711\n",
            "Training batch[11]: last record -> y: 43.297297704920425 | y_pred: 42.27887015624083\n",
            "Training batch[12]: last record -> y: 62.407599658046365 | y_pred: 63.0758552453367\n",
            "Training batch[13]: last record -> y: 33.94779976733412 | y_pred: 34.11175356889589\n",
            "Training batch[14]: last record -> y: 72.626600789652 | y_pred: 70.20801768162596\n",
            "Training batch[15]: last record -> y: 58.45500306957024 | y_pred: 62.06612277684076\n",
            "Training batch[16]: last record -> y: 37.98649968080997 | y_pred: 37.2122130019884\n",
            "Training batch[17]: last record -> y: 52.01739855670667 | y_pred: 50.10149303781486\n",
            "Training batch[18]: last record -> y: 69.90390053832061 | y_pred: 63.7973298049958\n",
            "Training batch[19]: last record -> y: 43.48559998235805 | y_pred: 40.06268767371421\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.39455538017205\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.35158653561734\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.88966363951067\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.80066295204881\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.837727858989524\n",
            "[Training] Epoch: 971 | L1Loss: 0.04995 | RMSE: 0.08167 | PLCC: 0.94723 | SROCC: 0.94737\n",
            "[Testing]  Epoch: 971 | L1Loss: 0.07735 | RMSE: 0.10522 | PLCC: 0.91818 | SROCC: 0.93430\n",
            "Training batch[0]: last record -> y: 45.587398821758484 | y_pred: 47.017576288252826\n",
            "Training batch[1]: last record -> y: 69.68309663972673 | y_pred: 69.03595239193169\n",
            "Training batch[2]: last record -> y: 36.45529879426431 | y_pred: 37.89064796001344\n",
            "Training batch[3]: last record -> y: 53.02539747675837 | y_pred: 52.23641423093136\n",
            "Training batch[4]: last record -> y: 38.89670106038284 | y_pred: 37.50402002160115\n",
            "Training batch[5]: last record -> y: 28.55800066006435 | y_pred: 28.794439461022876\n",
            "Training batch[6]: last record -> y: 39.67210055508883 | y_pred: 39.90089357118825\n",
            "Training batch[7]: last record -> y: 63.91980066066935 | y_pred: 64.85033022751827\n",
            "Training batch[8]: last record -> y: 40.115501400992116 | y_pred: 36.69756138528851\n",
            "Training batch[9]: last record -> y: 57.104697480745926 | y_pred: 54.213875192632486\n",
            "Training batch[10]: last record -> y: 60.984999631504934 | y_pred: 60.34002955578467\n",
            "Training batch[11]: last record -> y: 62.93809764429125 | y_pred: 63.68326296061127\n",
            "Training batch[12]: last record -> y: 55.51060159122949 | y_pred: 65.33155234045307\n",
            "Training batch[13]: last record -> y: 47.797999535593135 | y_pred: 47.55475921038146\n",
            "Training batch[14]: last record -> y: 59.64789988667326 | y_pred: 63.82619226091151\n",
            "Training batch[15]: last record -> y: 63.37459806684183 | y_pred: 63.89843845516543\n",
            "Training batch[16]: last record -> y: 65.14040530680222 | y_pred: 66.88149581245102\n",
            "Training batch[17]: last record -> y: 53.863000484795975 | y_pred: 55.094713992387824\n",
            "Training batch[18]: last record -> y: 48.53810250450829 | y_pred: 41.159744026828776\n",
            "Training batch[19]: last record -> y: 65.65200150416626 | y_pred: 61.98931275096834\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.61878747317678\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 43.075969560693125\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 34.18246883725101\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.34047805144553\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.294594353850783\n",
            "[Training] Epoch: 972 | L1Loss: 0.05411 | RMSE: 0.08372 | PLCC: 0.94407 | SROCC: 0.94357\n",
            "[Testing]  Epoch: 972 | L1Loss: 0.08213 | RMSE: 0.10609 | PLCC: 0.91696 | SROCC: 0.92966\n",
            "Training batch[0]: last record -> y: 64.42900076602791 | y_pred: 58.10362662971647\n",
            "Training batch[1]: last record -> y: 25.520900573275895 | y_pred: 25.181906622309526\n",
            "Training batch[2]: last record -> y: 36.69869993101997 | y_pred: 36.95021883451636\n",
            "Training batch[3]: last record -> y: 54.23470028757947 | y_pred: 55.21430310519304\n",
            "Training batch[4]: last record -> y: 24.91699975574693 | y_pred: 26.61268190571866\n",
            "Training batch[5]: last record -> y: 53.64059811945481 | y_pred: 54.2883727485023\n",
            "Training batch[6]: last record -> y: 43.26480090811049 | y_pred: 48.13012361740016\n",
            "Training batch[7]: last record -> y: 68.8820042846371 | y_pred: 62.96272432413821\n",
            "Training batch[8]: last record -> y: 46.75879995977607 | y_pred: 44.77278704016544\n",
            "Training batch[9]: last record -> y: 57.104697480745926 | y_pred: 52.367880884375836\n",
            "Training batch[10]: last record -> y: 36.83800132288775 | y_pred: 37.7472876847753\n",
            "Training batch[11]: last record -> y: 42.32070206317951 | y_pred: 40.23280377364176\n",
            "Training batch[12]: last record -> y: 67.38510289230953 | y_pred: 64.81636361319624\n",
            "Training batch[13]: last record -> y: 28.923200460239684 | y_pred: 28.848065286597944\n",
            "Training batch[14]: last record -> y: 46.50630171397677 | y_pred: 46.05578817348271\n",
            "Training batch[15]: last record -> y: 65.80919800464949 | y_pred: 59.132588942643\n",
            "Training batch[16]: last record -> y: 26.304599953796185 | y_pred: 34.38732505502719\n",
            "Training batch[17]: last record -> y: 62.07709977283366 | y_pred: 60.223232131383156\n",
            "Training batch[18]: last record -> y: 40.334599089104245 | y_pred: 41.39870998120114\n",
            "Training batch[19]: last record -> y: 55.55740096676209 | y_pred: 59.38573847519547\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.78915298485106\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.243237995883646\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.9841064973283\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.78443208906015\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.529256589277054\n",
            "[Training] Epoch: 973 | L1Loss: 0.05496 | RMSE: 0.08483 | PLCC: 0.94281 | SROCC: 0.93985\n",
            "[Testing]  Epoch: 973 | L1Loss: 0.07515 | RMSE: 0.10180 | PLCC: 0.91867 | SROCC: 0.93350\n",
            "Training batch[0]: last record -> y: 25.707799769992192 | y_pred: 22.707727948308943\n",
            "Training batch[1]: last record -> y: 65.26629501590105 | y_pred: 63.00558703307388\n",
            "Training batch[2]: last record -> y: 61.54970223315695 | y_pred: 63.93746098443057\n",
            "Training batch[3]: last record -> y: 57.950199551824426 | y_pred: 56.835585794336794\n",
            "Training batch[4]: last record -> y: 59.229098382654456 | y_pred: 61.768470257603894\n",
            "Training batch[5]: last record -> y: 33.06290047609298 | y_pred: 33.1860105576539\n",
            "Training batch[6]: last record -> y: 57.048500278582424 | y_pred: 59.042103503069484\n",
            "Training batch[7]: last record -> y: 48.23669664383465 | y_pred: 47.10502721399121\n",
            "Training batch[8]: last record -> y: 65.59839658409192 | y_pred: 62.21903525779203\n",
            "Training batch[9]: last record -> y: 65.06269795251433 | y_pred: 64.51116260008439\n",
            "Training batch[10]: last record -> y: 68.83770070426726 | y_pred: 66.23779936415917\n",
            "Training batch[11]: last record -> y: 57.104697480745926 | y_pred: 53.69387176774853\n",
            "Training batch[12]: last record -> y: 40.63280158382156 | y_pred: 38.07348907741709\n",
            "Training batch[13]: last record -> y: 70.18830218633252 | y_pred: 66.67417113747456\n",
            "Training batch[14]: last record -> y: 59.62590086745513 | y_pred: 59.00887018938806\n",
            "Training batch[15]: last record -> y: 68.15290082533102 | y_pred: 63.08805440906394\n",
            "Training batch[16]: last record -> y: 27.153799082271064 | y_pred: 25.941206067259884\n",
            "Training batch[17]: last record -> y: 30.740801078231357 | y_pred: 29.146216321621182\n",
            "Training batch[18]: last record -> y: 65.36119955670347 | y_pred: 62.782486745631786\n",
            "Training batch[19]: last record -> y: 56.801496963241334 | y_pred: 53.99907278086039\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.79449208196513\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.85365786523073\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.59457958833843\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.44503465497189\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.713353798692822\n",
            "[Training] Epoch: 974 | L1Loss: 0.05031 | RMSE: 0.08092 | PLCC: 0.94765 | SROCC: 0.94783\n",
            "[Testing]  Epoch: 974 | L1Loss: 0.08169 | RMSE: 0.10954 | PLCC: 0.91940 | SROCC: 0.93519\n",
            "Training batch[0]: last record -> y: 37.30360059432371 | y_pred: 39.34884040558666\n",
            "Training batch[1]: last record -> y: 48.99319917400612 | y_pred: 58.051234228683825\n",
            "Training batch[2]: last record -> y: 64.29779784351558 | y_pred: 63.12287654080001\n",
            "Training batch[3]: last record -> y: 34.00809927198486 | y_pred: 37.455025568493056\n",
            "Training batch[4]: last record -> y: 26.635299647352298 | y_pred: 36.61994408546525\n",
            "Training batch[5]: last record -> y: 32.322799919350985 | y_pred: 29.400316250398646\n",
            "Training batch[6]: last record -> y: 21.597000101274666 | y_pred: 14.790793719515392\n",
            "Training batch[7]: last record -> y: 47.01879845598717 | y_pred: 43.10790512521453\n",
            "Training batch[8]: last record -> y: 61.76470083501272 | y_pred: 64.68358473755279\n",
            "Training batch[9]: last record -> y: 52.90630044727777 | y_pred: 55.835659613793496\n",
            "Training batch[10]: last record -> y: 25.810800367976896 | y_pred: 18.79039672838701\n",
            "Training batch[11]: last record -> y: 56.90710190418099 | y_pred: 55.1087271103313\n",
            "Training batch[12]: last record -> y: 56.49319871979219 | y_pred: 52.651867638604244\n",
            "Training batch[13]: last record -> y: 65.36119955670347 | y_pred: 62.4570106130534\n",
            "Training batch[14]: last record -> y: 27.51029978197414 | y_pred: 28.239249666255716\n",
            "Training batch[15]: last record -> y: 52.87200256117512 | y_pred: 55.40995929453743\n",
            "Training batch[16]: last record -> y: 43.501399716555284 | y_pred: 45.46750738325056\n",
            "Training batch[17]: last record -> y: 33.94779976733412 | y_pred: 35.002130101325406\n",
            "Training batch[18]: last record -> y: 42.00540208510495 | y_pred: 42.80075829242037\n",
            "Training batch[19]: last record -> y: 70.7566005341082 | y_pred: 62.54901732983308\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 44.68282745434078\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 43.54506648321092\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 33.6926947664067\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.631087025055535\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.056531510151473\n",
            "[Training] Epoch: 975 | L1Loss: 0.05431 | RMSE: 0.08398 | PLCC: 0.94439 | SROCC: 0.94341\n",
            "[Testing]  Epoch: 975 | L1Loss: 0.08138 | RMSE: 0.10591 | PLCC: 0.91510 | SROCC: 0.92872\n",
            "Training batch[0]: last record -> y: 58.27209923566443 | y_pred: 56.17224460783086\n",
            "Training batch[1]: last record -> y: 36.61619878460567 | y_pred: 39.45302377247742\n",
            "Training batch[2]: last record -> y: 62.891102078674976 | y_pred: 64.84980919811574\n",
            "Training batch[3]: last record -> y: 33.262500568553776 | y_pred: 48.22624067724428\n",
            "Training batch[4]: last record -> y: 22.98349944379808 | y_pred: 22.207580326790563\n",
            "Training batch[5]: last record -> y: 33.93240045388143 | y_pred: 32.329584558392014\n",
            "Training batch[6]: last record -> y: 53.70280002467098 | y_pred: 52.915296245048694\n",
            "Training batch[7]: last record -> y: 48.79289874727124 | y_pred: 53.17569194577345\n",
            "Training batch[8]: last record -> y: 44.271098442026755 | y_pred: 40.506061181655014\n",
            "Training batch[9]: last record -> y: 63.12649801677071 | y_pred: 61.922054930809054\n",
            "Training batch[10]: last record -> y: 44.302501720337546 | y_pred: 46.468408081750454\n",
            "Training batch[11]: last record -> y: 69.45530065520006 | y_pred: 67.83605062397237\n",
            "Training batch[12]: last record -> y: 49.19730118564098 | y_pred: 60.09380135208562\n",
            "Training batch[13]: last record -> y: 67.7326970446486 | y_pred: 67.09354513058997\n",
            "Training batch[14]: last record -> y: 66.4921035235975 | y_pred: 62.613303352659386\n",
            "Training batch[15]: last record -> y: 50.848597741355434 | y_pred: 49.49048957641662\n",
            "Training batch[16]: last record -> y: 65.36119955670347 | y_pred: 63.30477369444043\n",
            "Training batch[17]: last record -> y: 62.27109960327493 | y_pred: 57.62042975102145\n",
            "Training batch[18]: last record -> y: 34.8158990765744 | y_pred: 35.14097800463912\n",
            "Training batch[19]: last record -> y: 27.821399362519628 | y_pred: 29.47945241525491\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.38416373819928\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.85109919971228\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.958922109130754\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.96451222610699\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.326742095288466\n",
            "[Training] Epoch: 976 | L1Loss: 0.05348 | RMSE: 0.08346 | PLCC: 0.94475 | SROCC: 0.94431\n",
            "[Testing]  Epoch: 976 | L1Loss: 0.07657 | RMSE: 0.10358 | PLCC: 0.91779 | SROCC: 0.93498\n",
            "Training batch[0]: last record -> y: 60.85200205216165 | y_pred: 59.490288492406535\n",
            "Training batch[1]: last record -> y: 43.297297704920425 | y_pred: 41.441659528370565\n",
            "Training batch[2]: last record -> y: 61.02579752021575 | y_pred: 58.178957189384164\n",
            "Training batch[3]: last record -> y: 49.9361015810498 | y_pred: 49.38330868234584\n",
            "Training batch[4]: last record -> y: 68.06819816887946 | y_pred: 67.59155918494844\n",
            "Training batch[5]: last record -> y: 52.21930066641971 | y_pred: 51.24325821639013\n",
            "Training batch[6]: last record -> y: 38.48249876652221 | y_pred: 39.33549304743531\n",
            "Training batch[7]: last record -> y: 64.35050221894357 | y_pred: 64.37413829967954\n",
            "Training batch[8]: last record -> y: 51.99429958652763 | y_pred: 52.59841709761213\n",
            "Training batch[9]: last record -> y: 25.83860026161568 | y_pred: 25.200224665285674\n",
            "Training batch[10]: last record -> y: 44.97420189090019 | y_pred: 41.661517855085776\n",
            "Training batch[11]: last record -> y: 48.23669664383465 | y_pred: 45.815870214769916\n",
            "Training batch[12]: last record -> y: 21.73069952250026 | y_pred: 22.397639068317446\n",
            "Training batch[13]: last record -> y: 68.83770070426726 | y_pred: 69.1921936718436\n",
            "Training batch[14]: last record -> y: 28.118200384631052 | y_pred: 27.658067901633103\n",
            "Training batch[15]: last record -> y: 55.39879897338437 | y_pred: 54.24459341376587\n",
            "Training batch[16]: last record -> y: 49.19860054291644 | y_pred: 54.47206455897367\n",
            "Training batch[17]: last record -> y: 38.3894998425161 | y_pred: 37.67872729110968\n",
            "Training batch[18]: last record -> y: 68.8820042846371 | y_pred: 65.72368485801621\n",
            "Training batch[19]: last record -> y: 30.740801078231357 | y_pred: 30.69452675238989\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 47.01599872700626\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.8543332737155\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.464613306599404\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.89032664463252\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.962983970605023\n",
            "[Training] Epoch: 977 | L1Loss: 0.05059 | RMSE: 0.08118 | PLCC: 0.94767 | SROCC: 0.94711\n",
            "[Testing]  Epoch: 977 | L1Loss: 0.08078 | RMSE: 0.10879 | PLCC: 0.91923 | SROCC: 0.93421\n",
            "Training batch[0]: last record -> y: 72.15129975776699 | y_pred: 70.17462034016967\n",
            "Training batch[1]: last record -> y: 68.15290082533102 | y_pred: 64.1493076800241\n",
            "Training batch[2]: last record -> y: 38.38710053427974 | y_pred: 37.55148033257956\n",
            "Training batch[3]: last record -> y: 62.07709977283366 | y_pred: 56.9553196375316\n",
            "Training batch[4]: last record -> y: 65.26629501590105 | y_pred: 62.698321199737165\n",
            "Training batch[5]: last record -> y: 53.34080037422109 | y_pred: 53.170687490524415\n",
            "Training batch[6]: last record -> y: 38.51679986885574 | y_pred: 39.39572340312213\n",
            "Training batch[7]: last record -> y: 26.635299647352298 | y_pred: 40.0106876528489\n",
            "Training batch[8]: last record -> y: 36.49660163122326 | y_pred: 37.866936297851794\n",
            "Training batch[9]: last record -> y: 48.53829869459196 | y_pred: 48.37381743116589\n",
            "Training batch[10]: last record -> y: 44.75650004698991 | y_pred: 44.894457054349914\n",
            "Training batch[11]: last record -> y: 29.276899018788697 | y_pred: 32.420308803108355\n",
            "Training batch[12]: last record -> y: 64.41010219337795 | y_pred: 65.5115969451067\n",
            "Training batch[13]: last record -> y: 59.05309979644767 | y_pred: 57.15368389327682\n",
            "Training batch[14]: last record -> y: 64.08940216365613 | y_pred: 61.36005074686523\n",
            "Training batch[15]: last record -> y: 57.24819927014278 | y_pred: 55.01937378402749\n",
            "Training batch[16]: last record -> y: 62.23139844929415 | y_pred: 61.76172582144886\n",
            "Training batch[17]: last record -> y: 49.50789903416921 | y_pred: 49.656923091986755\n",
            "Training batch[18]: last record -> y: 47.01879845598717 | y_pred: 42.02281315097298\n",
            "Training batch[19]: last record -> y: 26.000999417575912 | y_pred: 28.584170326675917\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.12801330838238\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 43.73969669490464\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 34.1304205729225\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.96870764543621\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.141672378060406\n",
            "[Training] Epoch: 978 | L1Loss: 0.05316 | RMSE: 0.08194 | PLCC: 0.94677 | SROCC: 0.94586\n",
            "[Testing]  Epoch: 978 | L1Loss: 0.07983 | RMSE: 0.10509 | PLCC: 0.91470 | SROCC: 0.92762\n",
            "Training batch[0]: last record -> y: 67.35350020768419 | y_pred: 66.09512414609844\n",
            "Training batch[1]: last record -> y: 27.153799082271064 | y_pred: 26.64904340595959\n",
            "Training batch[2]: last record -> y: 69.65740217122766 | y_pred: 68.70542999310283\n",
            "Training batch[3]: last record -> y: 61.282700394204994 | y_pred: 65.09487634246716\n",
            "Training batch[4]: last record -> y: 38.3894998425161 | y_pred: 39.98436280309738\n",
            "Training batch[5]: last record -> y: 34.10250047265458 | y_pred: 35.00859311727845\n",
            "Training batch[6]: last record -> y: 31.658599999200135 | y_pred: 31.805610696486895\n",
            "Training batch[7]: last record -> y: 66.92800251097356 | y_pred: 65.77006290730355\n",
            "Training batch[8]: last record -> y: 67.704902377385 | y_pred: 62.89017902041246\n",
            "Training batch[9]: last record -> y: 54.38159841678544 | y_pred: 49.53790003581639\n",
            "Training batch[10]: last record -> y: 27.51029978197414 | y_pred: 27.076526723209668\n",
            "Training batch[11]: last record -> y: 69.64639944538771 | y_pred: 66.39447805147074\n",
            "Training batch[12]: last record -> y: 46.75879995977607 | y_pred: 47.6209235120416\n",
            "Training batch[13]: last record -> y: 54.441799826394345 | y_pred: 55.78602352262487\n",
            "Training batch[14]: last record -> y: 47.35749812182803 | y_pred: 47.04295234989479\n",
            "Training batch[15]: last record -> y: 37.4706001665287 | y_pred: 37.258541199697106\n",
            "Training batch[16]: last record -> y: 69.85279784587078 | y_pred: 60.322192339325056\n",
            "Training batch[17]: last record -> y: 69.50440285204263 | y_pred: 64.86988491126772\n",
            "Training batch[18]: last record -> y: 68.8820042846371 | y_pred: 66.03353010851856\n",
            "Training batch[19]: last record -> y: 38.56610147201286 | y_pred: 38.0962889381243\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.3216112638172\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.0284968463518\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.777910873244195\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.656866877526454\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.621344669739983\n",
            "[Training] Epoch: 979 | L1Loss: 0.05302 | RMSE: 0.08346 | PLCC: 0.94445 | SROCC: 0.94533\n",
            "[Testing]  Epoch: 979 | L1Loss: 0.07560 | RMSE: 0.10334 | PLCC: 0.91913 | SROCC: 0.93541\n",
            "Training batch[0]: last record -> y: 44.8014983332298 | y_pred: 46.00021974445622\n",
            "Training batch[1]: last record -> y: 67.85620031043459 | y_pred: 69.86244332227943\n",
            "Training batch[2]: last record -> y: 61.653901681202115 | y_pred: 64.10979306743434\n",
            "Training batch[3]: last record -> y: 25.810800367976896 | y_pred: 18.979065555637725\n",
            "Training batch[4]: last record -> y: 36.90119865156123 | y_pred: 36.886931451378246\n",
            "Training batch[5]: last record -> y: 60.9454978838387 | y_pred: 60.491626598306084\n",
            "Training batch[6]: last record -> y: 47.44130023363323 | y_pred: 47.58265679703322\n",
            "Training batch[7]: last record -> y: 64.97390103415273 | y_pred: 65.1142573497491\n",
            "Training batch[8]: last record -> y: 39.74829788897864 | y_pred: 32.882703100472554\n",
            "Training batch[9]: last record -> y: 47.959997868779965 | y_pred: 45.34172863411709\n",
            "Training batch[10]: last record -> y: 33.64979989516223 | y_pred: 36.195441812211925\n",
            "Training batch[11]: last record -> y: 59.43330009744659 | y_pred: 59.52752922976424\n",
            "Training batch[12]: last record -> y: 67.19999594025103 | y_pred: 59.91779954964795\n",
            "Training batch[13]: last record -> y: 30.96270010343983 | y_pred: 28.575595051092535\n",
            "Training batch[14]: last record -> y: 39.02769814411886 | y_pred: 40.88310636016081\n",
            "Training batch[15]: last record -> y: 30.038900499706983 | y_pred: 31.884046527069074\n",
            "Training batch[16]: last record -> y: 62.10859953807085 | y_pred: 60.036008899405715\n",
            "Training batch[17]: last record -> y: 61.716499182816506 | y_pred: 53.30977661115412\n",
            "Training batch[18]: last record -> y: 36.413502265865304 | y_pred: 35.8152656329446\n",
            "Training batch[19]: last record -> y: 62.407599658046365 | y_pred: 62.88406496550988\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.29885160599599\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.674951212623114\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.88183678781104\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.581902968179065\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.539579638168306\n",
            "[Training] Epoch: 980 | L1Loss: 0.05112 | RMSE: 0.08229 | PLCC: 0.94622 | SROCC: 0.94572\n",
            "[Testing]  Epoch: 980 | L1Loss: 0.07779 | RMSE: 0.10665 | PLCC: 0.91849 | SROCC: 0.93370\n",
            "Training batch[0]: last record -> y: 38.3894998425161 | y_pred: 38.64577072713746\n",
            "Training batch[1]: last record -> y: 63.37459806684183 | y_pred: 63.66827854094208\n",
            "Training batch[2]: last record -> y: 28.777399065763746 | y_pred: 27.69631692737164\n",
            "Training batch[3]: last record -> y: 60.22169798925347 | y_pred: 55.970184902804704\n",
            "Training batch[4]: last record -> y: 22.983800965443066 | y_pred: 22.515563379613482\n",
            "Training batch[5]: last record -> y: 58.14030131043933 | y_pred: 62.051765522193136\n",
            "Training batch[6]: last record -> y: 32.369998911570406 | y_pred: 32.672447635106835\n",
            "Training batch[7]: last record -> y: 55.84130088275674 | y_pred: 56.335027701352374\n",
            "Training batch[8]: last record -> y: 40.63280158382156 | y_pred: 37.647402810455446\n",
            "Training batch[9]: last record -> y: 31.007400888323332 | y_pred: 30.145421262389675\n",
            "Training batch[10]: last record -> y: 62.979599887564746 | y_pred: 64.06812679638597\n",
            "Training batch[11]: last record -> y: 34.10250047265458 | y_pred: 34.0861748847085\n",
            "Training batch[12]: last record -> y: 47.01879845598717 | y_pred: 41.40300525754117\n",
            "Training batch[13]: last record -> y: 29.928600665740476 | y_pred: 29.51132124298556\n",
            "Training batch[14]: last record -> y: 54.38009965319543 | y_pred: 63.113060604154725\n",
            "Training batch[15]: last record -> y: 36.413502265865304 | y_pred: 37.31961099164357\n",
            "Training batch[16]: last record -> y: 37.43249908741063 | y_pred: 49.607862706145625\n",
            "Training batch[17]: last record -> y: 25.808300150496677 | y_pred: 39.44768322110144\n",
            "Training batch[18]: last record -> y: 54.96030127145741 | y_pred: 53.147919792126004\n",
            "Training batch[19]: last record -> y: 30.420999180982733 | y_pred: 26.59131085557982\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.538896451975006\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.681837009083324\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.636457822917805\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.42139842038239\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.358854244801194\n",
            "[Training] Epoch: 981 | L1Loss: 0.04964 | RMSE: 0.08134 | PLCC: 0.94800 | SROCC: 0.94811\n",
            "[Testing]  Epoch: 981 | L1Loss: 0.07568 | RMSE: 0.10327 | PLCC: 0.91880 | SROCC: 0.93419\n",
            "Training batch[0]: last record -> y: 36.90119865156123 | y_pred: 36.245139011767264\n",
            "Training batch[1]: last record -> y: 67.08080242384403 | y_pred: 67.67031503050316\n",
            "Training batch[2]: last record -> y: 55.37589940951989 | y_pred: 58.0760345849983\n",
            "Training batch[3]: last record -> y: 55.44500012997332 | y_pred: 54.37537822626382\n",
            "Training batch[4]: last record -> y: 55.6992978569499 | y_pred: 54.44411872885871\n",
            "Training batch[5]: last record -> y: 25.608800967279734 | y_pred: 25.559356643877322\n",
            "Training batch[6]: last record -> y: 39.65230143779252 | y_pred: 39.152391415330385\n",
            "Training batch[7]: last record -> y: 39.685200263462434 | y_pred: 40.091323385352894\n",
            "Training batch[8]: last record -> y: 67.31629806509704 | y_pred: 67.73815820468258\n",
            "Training batch[9]: last record -> y: 55.86600153591394 | y_pred: 55.23956338252333\n",
            "Training batch[10]: last record -> y: 53.64059811945481 | y_pred: 55.003958389420404\n",
            "Training batch[11]: last record -> y: 38.44319964140141 | y_pred: 39.35719134906617\n",
            "Training batch[12]: last record -> y: 40.77389924063573 | y_pred: 41.39882254928193\n",
            "Training batch[13]: last record -> y: 38.3894998425161 | y_pred: 38.58400462120551\n",
            "Training batch[14]: last record -> y: 70.7566005341082 | y_pred: 65.52991981242917\n",
            "Training batch[15]: last record -> y: 55.65470159956999 | y_pred: 54.5840086909775\n",
            "Training batch[16]: last record -> y: 63.459201020136106 | y_pred: 62.12525318156668\n",
            "Training batch[17]: last record -> y: 54.821202502135066 | y_pred: 58.163313442384606\n",
            "Training batch[18]: last record -> y: 37.98649968080997 | y_pred: 39.387287229524304\n",
            "Training batch[19]: last record -> y: 69.23919566992163 | y_pred: 68.54492077481336\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.36820143048908\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.485976589078064\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.349406000660906\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.300416833459735\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.91041211084729\n",
            "[Training] Epoch: 982 | L1Loss: 0.04820 | RMSE: 0.08081 | PLCC: 0.94870 | SROCC: 0.94949\n",
            "[Testing]  Epoch: 982 | L1Loss: 0.07581 | RMSE: 0.10345 | PLCC: 0.91832 | SROCC: 0.93637\n",
            "Training batch[0]: last record -> y: 67.50669893318377 | y_pred: 68.32833335490295\n",
            "Training batch[1]: last record -> y: 58.68100118103507 | y_pred: 56.26919788770374\n",
            "Training batch[2]: last record -> y: 66.99210199240883 | y_pred: 66.65500883389245\n",
            "Training batch[3]: last record -> y: 61.282700394204994 | y_pred: 61.55485463502646\n",
            "Training batch[4]: last record -> y: 43.31629919695854 | y_pred: 43.155228746380544\n",
            "Training batch[5]: last record -> y: 44.53200069911509 | y_pred: 43.37724998836245\n",
            "Training batch[6]: last record -> y: 28.923200460239684 | y_pred: 28.905360027549193\n",
            "Training batch[7]: last record -> y: 49.19860054291644 | y_pred: 55.42801843092775\n",
            "Training batch[8]: last record -> y: 61.766399004917275 | y_pred: 56.25181415979819\n",
            "Training batch[9]: last record -> y: 58.169900283226525 | y_pred: 55.575948970246145\n",
            "Training batch[10]: last record -> y: 65.81910077952853 | y_pred: 64.95730206658186\n",
            "Training batch[11]: last record -> y: 32.14319995861649 | y_pred: 31.90734570762038\n",
            "Training batch[12]: last record -> y: 54.38009965319543 | y_pred: 65.53534559392347\n",
            "Training batch[13]: last record -> y: 60.28530217113325 | y_pred: 58.98410521161327\n",
            "Training batch[14]: last record -> y: 54.96030127145741 | y_pred: 54.38651603380072\n",
            "Training batch[15]: last record -> y: 48.22529832159648 | y_pred: 49.1324844847195\n",
            "Training batch[16]: last record -> y: 27.770800010202493 | y_pred: 26.82315125043789\n",
            "Training batch[17]: last record -> y: 58.21779960972003 | y_pred: 58.664289645383406\n",
            "Training batch[18]: last record -> y: 63.56399868712492 | y_pred: 67.09817650305695\n",
            "Training batch[19]: last record -> y: 35.46180025113438 | y_pred: 51.11871932426084\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 47.130322869861175\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 47.48620203294672\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.719293611197145\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.93558705368912\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.64216172410977\n",
            "[Training] Epoch: 983 | L1Loss: 0.04983 | RMSE: 0.08075 | PLCC: 0.94832 | SROCC: 0.94769\n",
            "[Testing]  Epoch: 983 | L1Loss: 0.08067 | RMSE: 0.10930 | PLCC: 0.91995 | SROCC: 0.93655\n",
            "Training batch[0]: last record -> y: 63.97720073318192 | y_pred: 62.89475571695448\n",
            "Training batch[1]: last record -> y: 57.606300848766296 | y_pred: 64.63996621436036\n",
            "Training batch[2]: last record -> y: 32.75839979725521 | y_pred: 32.16297736635437\n",
            "Training batch[3]: last record -> y: 60.46889749467823 | y_pred: 59.017412498604926\n",
            "Training batch[4]: last record -> y: 54.46919889725973 | y_pred: 54.32880720312369\n",
            "Training batch[5]: last record -> y: 43.91640124378955 | y_pred: 46.08390124760342\n",
            "Training batch[6]: last record -> y: 38.79510193500403 | y_pred: 42.17886467326298\n",
            "Training batch[7]: last record -> y: 46.40150083075707 | y_pred: 42.74115831798599\n",
            "Training batch[8]: last record -> y: 44.97420189090019 | y_pred: 41.544508159446195\n",
            "Training batch[9]: last record -> y: 35.84120131875966 | y_pred: 48.02700160669974\n",
            "Training batch[10]: last record -> y: 68.51670156507362 | y_pred: 66.20545694643147\n",
            "Training batch[11]: last record -> y: 56.61489768005458 | y_pred: 61.95337458711697\n",
            "Training batch[12]: last record -> y: 59.229098382654456 | y_pred: 63.07881096151527\n",
            "Training batch[13]: last record -> y: 49.19730118564098 | y_pred: 61.18920777875883\n",
            "Training batch[14]: last record -> y: 39.02769814411886 | y_pred: 40.27150467981892\n",
            "Training batch[15]: last record -> y: 61.21029982086884 | y_pred: 58.24698690495461\n",
            "Training batch[16]: last record -> y: 30.995199312422926 | y_pred: 30.394269086140355\n",
            "Training batch[17]: last record -> y: 34.10250047265458 | y_pred: 33.87689152512621\n",
            "Training batch[18]: last record -> y: 33.85829849440984 | y_pred: 32.693168202550225\n",
            "Training batch[19]: last record -> y: 58.32300252179971 | y_pred: 57.13362104504836\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.676660485482216\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.91918504546959\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.8212770761661\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.189133636901715\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.987094291542235\n",
            "[Training] Epoch: 984 | L1Loss: 0.04920 | RMSE: 0.08029 | PLCC: 0.94913 | SROCC: 0.94993\n",
            "[Testing]  Epoch: 984 | L1Loss: 0.07440 | RMSE: 0.10071 | PLCC: 0.91919 | SROCC: 0.93411\n",
            "Training batch[0]: last record -> y: 38.3894998425161 | y_pred: 37.77052495288217\n",
            "Training batch[1]: last record -> y: 44.76750116471442 | y_pred: 42.95264801195185\n",
            "Training batch[2]: last record -> y: 54.38159841678544 | y_pred: 51.038493661194025\n",
            "Training batch[3]: last record -> y: 64.74639772663613 | y_pred: 65.41478196316166\n",
            "Training batch[4]: last record -> y: 29.276899018788697 | y_pred: 33.78166053688949\n",
            "Training batch[5]: last record -> y: 73.29170125444921 | y_pred: 71.30751837020921\n",
            "Training batch[6]: last record -> y: 57.104697480745926 | y_pred: 52.1230710384948\n",
            "Training batch[7]: last record -> y: 35.50089996994063 | y_pred: 33.30638121456292\n",
            "Training batch[8]: last record -> y: 26.202400197300562 | y_pred: 27.536089129284164\n",
            "Training batch[9]: last record -> y: 54.78369803384521 | y_pred: 55.29729151058564\n",
            "Training batch[10]: last record -> y: 59.44990228124834 | y_pred: 60.66436392640071\n",
            "Training batch[11]: last record -> y: 22.983800965443066 | y_pred: 23.7534619002017\n",
            "Training batch[12]: last record -> y: 65.29879824517275 | y_pred: 66.26351956250528\n",
            "Training batch[13]: last record -> y: 48.79289874727124 | y_pred: 53.131896529882624\n",
            "Training batch[14]: last record -> y: 59.05309979644767 | y_pred: 56.653228719680556\n",
            "Training batch[15]: last record -> y: 39.685200263462434 | y_pred: 39.040464972596396\n",
            "Training batch[16]: last record -> y: 50.848597741355434 | y_pred: 49.94163671439401\n",
            "Training batch[17]: last record -> y: 61.766399004917275 | y_pred: 58.02663649491478\n",
            "Training batch[18]: last record -> y: 65.14040530680222 | y_pred: 66.080306970435\n",
            "Training batch[19]: last record -> y: 39.65230143779252 | y_pred: 39.88300650315\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.94910097085199\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.540538337839166\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.838612252901385\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.30036858999654\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.963005928338788\n",
            "[Training] Epoch: 985 | L1Loss: 0.04950 | RMSE: 0.08122 | PLCC: 0.94780 | SROCC: 0.94771\n",
            "[Testing]  Epoch: 985 | L1Loss: 0.07520 | RMSE: 0.10236 | PLCC: 0.91715 | SROCC: 0.93049\n",
            "Training batch[0]: last record -> y: 27.66959969745278 | y_pred: 27.477712930699568\n",
            "Training batch[1]: last record -> y: 38.38710053427974 | y_pred: 36.73392569973146\n",
            "Training batch[2]: last record -> y: 44.97420189090019 | y_pred: 41.15557739972394\n",
            "Training batch[3]: last record -> y: 40.03480134387053 | y_pred: 38.8469877796731\n",
            "Training batch[4]: last record -> y: 64.4571009752251 | y_pred: 66.87868804289292\n",
            "Training batch[5]: last record -> y: 38.44319964140141 | y_pred: 39.546786551317496\n",
            "Training batch[6]: last record -> y: 38.67589876990439 | y_pred: 51.70241376925833\n",
            "Training batch[7]: last record -> y: 30.71260036181917 | y_pred: 31.17026840790544\n",
            "Training batch[8]: last record -> y: 38.48249876652221 | y_pred: 37.73886920044731\n",
            "Training batch[9]: last record -> y: 60.00719790318408 | y_pred: 59.988598440005944\n",
            "Training batch[10]: last record -> y: 63.12649801677071 | y_pred: 61.694744597145245\n",
            "Training batch[11]: last record -> y: 73.29170125444921 | y_pred: 71.7982669590101\n",
            "Training batch[12]: last record -> y: 40.324099703396996 | y_pred: 42.13219877131189\n",
            "Training batch[13]: last record -> y: 39.49110072986389 | y_pred: 40.208550176576864\n",
            "Training batch[14]: last record -> y: 43.48559998235805 | y_pred: 40.56020482040174\n",
            "Training batch[15]: last record -> y: 61.01490092999484 | y_pred: 50.80621425082086\n",
            "Training batch[16]: last record -> y: 68.87090185563989 | y_pred: 65.48279238134683\n",
            "Training batch[17]: last record -> y: 63.459201020136106 | y_pred: 61.98595500592978\n",
            "Training batch[18]: last record -> y: 48.261201106908175 | y_pred: 48.46523879392544\n",
            "Training batch[19]: last record -> y: 52.90630044727777 | y_pred: 55.073946789596675\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.904258825663305\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.60113710581527\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.059892461372556\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.87711276006269\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.01029472684752\n",
            "[Training] Epoch: 986 | L1Loss: 0.05107 | RMSE: 0.08188 | PLCC: 0.94667 | SROCC: 0.94663\n",
            "[Testing]  Epoch: 986 | L1Loss: 0.07806 | RMSE: 0.10558 | PLCC: 0.91991 | SROCC: 0.93517\n",
            "Training batch[0]: last record -> y: 63.512198072574165 | y_pred: 61.401353583824175\n",
            "Training batch[1]: last record -> y: 67.35350020768419 | y_pred: 68.2505070000725\n",
            "Training batch[2]: last record -> y: 25.112100743235686 | y_pred: 25.194876475361355\n",
            "Training batch[3]: last record -> y: 40.04259909563871 | y_pred: 45.61024692592889\n",
            "Training batch[4]: last record -> y: 64.35050221894357 | y_pred: 61.608108985934905\n",
            "Training batch[5]: last record -> y: 47.66609869097988 | y_pred: 48.027979340887214\n",
            "Training batch[6]: last record -> y: 58.07179880892954 | y_pred: 57.80294120475992\n",
            "Training batch[7]: last record -> y: 29.03989978959936 | y_pred: 29.5720477022856\n",
            "Training batch[8]: last record -> y: 61.27290053871411 | y_pred: 62.3494662848932\n",
            "Training batch[9]: last record -> y: 54.78369803384521 | y_pred: 55.11075011955472\n",
            "Training batch[10]: last record -> y: 52.886501329981456 | y_pred: 52.732752629001425\n",
            "Training batch[11]: last record -> y: 38.69430204299886 | y_pred: 42.19039486096722\n",
            "Training batch[12]: last record -> y: 38.541598617054774 | y_pred: 38.90503592270795\n",
            "Training batch[13]: last record -> y: 42.995300057764894 | y_pred: 49.08562400080018\n",
            "Training batch[14]: last record -> y: 28.114999430847888 | y_pred: 28.504591930073673\n",
            "Training batch[15]: last record -> y: 68.96460030986236 | y_pred: 67.7740867198413\n",
            "Training batch[16]: last record -> y: 52.01739855670667 | y_pred: 52.73154654242148\n",
            "Training batch[17]: last record -> y: 69.90390053832061 | y_pred: 65.35603428991044\n",
            "Training batch[18]: last record -> y: 69.51529622603266 | y_pred: 66.85868308682029\n",
            "Training batch[19]: last record -> y: 65.49870307550941 | y_pred: 67.48862371563905\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.96421243164093\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.16074343578464\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.489230030046656\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.728627267063985\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.25818154776931\n",
            "[Training] Epoch: 987 | L1Loss: 0.04770 | RMSE: 0.07898 | PLCC: 0.95041 | SROCC: 0.95114\n",
            "[Testing]  Epoch: 987 | L1Loss: 0.07663 | RMSE: 0.10429 | PLCC: 0.92004 | SROCC: 0.93585\n",
            "Training batch[0]: last record -> y: 23.335699037742657 | y_pred: 22.54495691565323\n",
            "Training batch[1]: last record -> y: 63.56399868712492 | y_pred: 67.05179523753873\n",
            "Training batch[2]: last record -> y: 36.78459902535883 | y_pred: 36.536680692332425\n",
            "Training batch[3]: last record -> y: 66.99210199240883 | y_pred: 66.92401438468255\n",
            "Training batch[4]: last record -> y: 63.12649801677071 | y_pred: 62.32273297381994\n",
            "Training batch[5]: last record -> y: 50.61160012028154 | y_pred: 55.83662126682657\n",
            "Training batch[6]: last record -> y: 28.55800066006435 | y_pred: 29.49367780443646\n",
            "Training batch[7]: last record -> y: 37.80239940901686 | y_pred: 38.606243249624185\n",
            "Training batch[8]: last record -> y: 59.409998504722125 | y_pred: 59.81169287669104\n",
            "Training batch[9]: last record -> y: 48.95560143502075 | y_pred: 44.93667008464786\n",
            "Training batch[10]: last record -> y: 63.49129900431694 | y_pred: 59.61954559523656\n",
            "Training batch[11]: last record -> y: 58.21779960972003 | y_pred: 59.35017982658792\n",
            "Training batch[12]: last record -> y: 28.995599425460398 | y_pred: 29.364603222708865\n",
            "Training batch[13]: last record -> y: 55.86600153591394 | y_pred: 57.61315785300212\n",
            "Training batch[14]: last record -> y: 35.50039984603882 | y_pred: 41.38495255361261\n",
            "Training batch[15]: last record -> y: 61.76470083501272 | y_pred: 60.08338719649669\n",
            "Training batch[16]: last record -> y: 58.45500306957024 | y_pred: 61.56797364078534\n",
            "Training batch[17]: last record -> y: 30.740801078231357 | y_pred: 29.997595250574875\n",
            "Training batch[18]: last record -> y: 67.31629806509704 | y_pred: 67.38782703986476\n",
            "Training batch[19]: last record -> y: 25.810800367976896 | y_pred: 19.158879797754587\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.561378059678646\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.746391741041634\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.08227888255817\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.62147547292466\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.675934560521284\n",
            "[Training] Epoch: 988 | L1Loss: 0.04827 | RMSE: 0.07991 | PLCC: 0.94931 | SROCC: 0.94999\n",
            "[Testing]  Epoch: 988 | L1Loss: 0.07806 | RMSE: 0.10660 | PLCC: 0.91772 | SROCC: 0.93204\n",
            "Training batch[0]: last record -> y: 68.25509656153804 | y_pred: 70.15210350777988\n",
            "Training batch[1]: last record -> y: 45.30530160317198 | y_pred: 45.494993292349704\n",
            "Training batch[2]: last record -> y: 57.048500278582424 | y_pred: 57.26064286741689\n",
            "Training batch[3]: last record -> y: 56.724497179747004 | y_pred: 59.66977668911795\n",
            "Training batch[4]: last record -> y: 57.412301018325024 | y_pred: 54.49038099383438\n",
            "Training batch[5]: last record -> y: 38.89670106038284 | y_pred: 37.56011108814562\n",
            "Training batch[6]: last record -> y: 61.76470083501272 | y_pred: 61.813815896778806\n",
            "Training batch[7]: last record -> y: 48.61320149555263 | y_pred: 49.2666752857192\n",
            "Training batch[8]: last record -> y: 66.50039818303662 | y_pred: 58.6089222307869\n",
            "Training batch[9]: last record -> y: 28.114999430847888 | y_pred: 28.458794410460143\n",
            "Training batch[10]: last record -> y: 44.302501720337546 | y_pred: 43.36378202155311\n",
            "Training batch[11]: last record -> y: 73.66169645486639 | y_pred: 72.93691239363193\n",
            "Training batch[12]: last record -> y: 26.82169912219564 | y_pred: 30.00670361642659\n",
            "Training batch[13]: last record -> y: 44.53200069911509 | y_pred: 43.33739284718399\n",
            "Training batch[14]: last record -> y: 54.78369803384521 | y_pred: 56.06032942190495\n",
            "Training batch[15]: last record -> y: 55.44500012997332 | y_pred: 55.35375887614305\n",
            "Training batch[16]: last record -> y: 62.24660157266317 | y_pred: 64.67414509992045\n",
            "Training batch[17]: last record -> y: 39.97779847587162 | y_pred: 39.72240883851134\n",
            "Training batch[18]: last record -> y: 58.68100118103507 | y_pred: 56.80832823763012\n",
            "Training batch[19]: last record -> y: 48.25579783903004 | y_pred: 48.54827222655035\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.405763944788305\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.1509291072548\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.2896275254127\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.31135684279752\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.62551290496026\n",
            "[Training] Epoch: 989 | L1Loss: 0.04978 | RMSE: 0.08065 | PLCC: 0.94855 | SROCC: 0.94894\n",
            "[Testing]  Epoch: 989 | L1Loss: 0.07599 | RMSE: 0.10418 | PLCC: 0.91866 | SROCC: 0.93256\n",
            "Training batch[0]: last record -> y: 21.597000101274666 | y_pred: 15.39262810870406\n",
            "Training batch[1]: last record -> y: 39.47439884290486 | y_pred: 38.83411320746109\n",
            "Training batch[2]: last record -> y: 62.20869829174421 | y_pred: 58.583211681133434\n",
            "Training batch[3]: last record -> y: 48.045601069877875 | y_pred: 45.89387024606788\n",
            "Training batch[4]: last record -> y: 60.984999631504934 | y_pred: 60.30462528625935\n",
            "Training batch[5]: last record -> y: 33.643200189396794 | y_pred: 33.8738537950602\n",
            "Training batch[6]: last record -> y: 27.821399362519628 | y_pred: 27.73325694714208\n",
            "Training batch[7]: last record -> y: 38.541598617054774 | y_pred: 39.256904445886335\n",
            "Training batch[8]: last record -> y: 68.8820042846371 | y_pred: 66.38505771122368\n",
            "Training batch[9]: last record -> y: 56.61489768005458 | y_pred: 62.27114141427637\n",
            "Training batch[10]: last record -> y: 62.23139844929415 | y_pred: 59.842411097824424\n",
            "Training batch[11]: last record -> y: 65.45760286109589 | y_pred: 63.077521252932456\n",
            "Training batch[12]: last record -> y: 63.91980066066935 | y_pred: 61.55210475762419\n",
            "Training batch[13]: last record -> y: 57.949199304020794 | y_pred: 67.03987266966715\n",
            "Training batch[14]: last record -> y: 31.0084992311688 | y_pred: 30.545741499715177\n",
            "Training batch[15]: last record -> y: 48.53810250450829 | y_pred: 44.06417257967939\n",
            "Training batch[16]: last record -> y: 52.21930066641971 | y_pred: 52.03172686527637\n",
            "Training batch[17]: last record -> y: 35.58000156031494 | y_pred: 35.423868024137505\n",
            "Training batch[18]: last record -> y: 38.86930038140201 | y_pred: 43.024828273472735\n",
            "Training batch[19]: last record -> y: 62.41920060282996 | y_pred: 58.849682841990216\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.900888061847695\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.92290140025125\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.057032924413306\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.171281947403145\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.221536617182096\n",
            "[Training] Epoch: 990 | L1Loss: 0.05017 | RMSE: 0.08127 | PLCC: 0.94749 | SROCC: 0.94801\n",
            "[Testing]  Epoch: 990 | L1Loss: 0.07421 | RMSE: 0.10175 | PLCC: 0.91912 | SROCC: 0.93459\n",
            "Training batch[0]: last record -> y: 64.4866034610859 | y_pred: 61.714846040144266\n",
            "Training batch[1]: last record -> y: 65.06269795251433 | y_pred: 66.2166204838154\n",
            "Training batch[2]: last record -> y: 58.07179880892954 | y_pred: 58.857691256881026\n",
            "Training batch[3]: last record -> y: 65.45760286109589 | y_pred: 65.4993591866089\n",
            "Training batch[4]: last record -> y: 46.50630171397677 | y_pred: 47.79333921704824\n",
            "Training batch[5]: last record -> y: 55.6992978569499 | y_pred: 52.64668629065682\n",
            "Training batch[6]: last record -> y: 25.033300272477504 | y_pred: 24.771132026509093\n",
            "Training batch[7]: last record -> y: 55.86600153591394 | y_pred: 54.6648100593718\n",
            "Training batch[8]: last record -> y: 71.94249883281668 | y_pred: 70.024100734993\n",
            "Training batch[9]: last record -> y: 58.416900382336735 | y_pred: 60.03758806876772\n",
            "Training batch[10]: last record -> y: 38.76359895353596 | y_pred: 53.99087782457855\n",
            "Training batch[11]: last record -> y: 39.43050050762588 | y_pred: 39.19599385736842\n",
            "Training batch[12]: last record -> y: 60.54589727817256 | y_pred: 60.56383741402033\n",
            "Training batch[13]: last record -> y: 53.07020278914547 | y_pred: 56.5936416101697\n",
            "Training batch[14]: last record -> y: 47.28070096087913 | y_pred: 48.43032660772474\n",
            "Training batch[15]: last record -> y: 49.315497670475224 | y_pred: 49.353677547249845\n",
            "Training batch[16]: last record -> y: 27.831899552284597 | y_pred: 27.842189882984655\n",
            "Training batch[17]: last record -> y: 48.99319917400612 | y_pred: 55.85698000829598\n",
            "Training batch[18]: last record -> y: 72.626600789652 | y_pred: 68.17583898396606\n",
            "Training batch[19]: last record -> y: 69.6393977107623 | y_pred: 67.32223201107035\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.5566645194707\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.85264926914283\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.79583959843035\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.52395759067883\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.00960509349912\n",
            "[Training] Epoch: 991 | L1Loss: 0.04739 | RMSE: 0.07958 | PLCC: 0.95002 | SROCC: 0.95009\n",
            "[Testing]  Epoch: 991 | L1Loss: 0.07442 | RMSE: 0.10201 | PLCC: 0.91902 | SROCC: 0.93344\n",
            "Training batch[0]: last record -> y: 58.50000135581013 | y_pred: 62.77288951268633\n",
            "Training batch[1]: last record -> y: 36.45529879426431 | y_pred: 37.172933174252876\n",
            "Training batch[2]: last record -> y: 68.06819816887946 | y_pred: 68.05564521975543\n",
            "Training batch[3]: last record -> y: 30.471100017513493 | y_pred: 52.641582132250505\n",
            "Training batch[4]: last record -> y: 58.32300252179971 | y_pred: 59.4367607618733\n",
            "Training batch[5]: last record -> y: 26.387500716897307 | y_pred: 27.652136367832952\n",
            "Training batch[6]: last record -> y: 66.50039818303662 | y_pred: 56.67506692735469\n",
            "Training batch[7]: last record -> y: 33.643200189396794 | y_pred: 34.44975048828951\n",
            "Training batch[8]: last record -> y: 25.27400017732296 | y_pred: 45.198944064206444\n",
            "Training batch[9]: last record -> y: 53.90739733586133 | y_pred: 59.7041388998382\n",
            "Training batch[10]: last record -> y: 58.383400121492286 | y_pred: 56.40281941583771\n",
            "Training batch[11]: last record -> y: 39.02769814411886 | y_pred: 40.469495852782075\n",
            "Training batch[12]: last record -> y: 63.49129900431694 | y_pred: 61.57992515473484\n",
            "Training batch[13]: last record -> y: 44.97420189090019 | y_pred: 42.59172419073127\n",
            "Training batch[14]: last record -> y: 52.99509736563937 | y_pred: 53.30382015156465\n",
            "Training batch[15]: last record -> y: 61.21029982086884 | y_pred: 56.62353647619784\n",
            "Training batch[16]: last record -> y: 38.08380192173331 | y_pred: 44.33321994147093\n",
            "Training batch[17]: last record -> y: 28.998800379243562 | y_pred: 28.25867972105857\n",
            "Training batch[18]: last record -> y: 49.39419884010499 | y_pred: 55.67915781918032\n",
            "Training batch[19]: last record -> y: 38.542598864858405 | y_pred: 39.68143405710214\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.52990884863482\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.186492580208665\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 38.002621038095185\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 40.002605264647855\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 26.0501225199192\n",
            "[Training] Epoch: 992 | L1Loss: 0.04901 | RMSE: 0.08026 | PLCC: 0.94880 | SROCC: 0.94927\n",
            "[Testing]  Epoch: 992 | L1Loss: 0.07658 | RMSE: 0.10399 | PLCC: 0.91930 | SROCC: 0.93411\n",
            "Training batch[0]: last record -> y: 61.02579752021575 | y_pred: 61.12700908977354\n",
            "Training batch[1]: last record -> y: 48.559300682237335 | y_pred: 48.853537564279804\n",
            "Training batch[2]: last record -> y: 26.387500716897307 | y_pred: 27.950367004570467\n",
            "Training batch[3]: last record -> y: 72.54129669802592 | y_pred: 66.61368669954823\n",
            "Training batch[4]: last record -> y: 54.76450035172343 | y_pred: 54.70961858798978\n",
            "Training batch[5]: last record -> y: 38.38710053427974 | y_pred: 36.646949168047854\n",
            "Training batch[6]: last record -> y: 38.542598864858405 | y_pred: 38.861250155509765\n",
            "Training batch[7]: last record -> y: 53.34080037422109 | y_pred: 55.35248203248375\n",
            "Training batch[8]: last record -> y: 48.95560143502075 | y_pred: 45.53200246108406\n",
            "Training batch[9]: last record -> y: 64.08940216365613 | y_pred: 61.06417037084316\n",
            "Training batch[10]: last record -> y: 43.36610092401747 | y_pred: 48.39945722374\n",
            "Training batch[11]: last record -> y: 52.90630044727777 | y_pred: 53.97538845666122\n",
            "Training batch[12]: last record -> y: 52.48210210784259 | y_pred: 53.537788083149735\n",
            "Training batch[13]: last record -> y: 57.655602451923414 | y_pred: 57.75008888271145\n",
            "Training batch[14]: last record -> y: 21.73069952250026 | y_pred: 21.78846333062389\n",
            "Training batch[15]: last record -> y: 48.22529832159648 | y_pred: 49.159629473345376\n",
            "Training batch[16]: last record -> y: 54.38009965319543 | y_pred: 64.1582584505627\n",
            "Training batch[17]: last record -> y: 49.19730118564098 | y_pred: 59.55951464586428\n",
            "Training batch[18]: last record -> y: 64.75720104616153 | y_pred: 64.81678172321062\n",
            "Training batch[19]: last record -> y: 43.92589877357773 | y_pred: 43.63623054750008\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.285441531342485\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.266877292850495\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.65917888596846\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.682310480016895\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.629497815020386\n",
            "[Training] Epoch: 993 | L1Loss: 0.04699 | RMSE: 0.07881 | PLCC: 0.95068 | SROCC: 0.95131\n",
            "[Testing]  Epoch: 993 | L1Loss: 0.07413 | RMSE: 0.10096 | PLCC: 0.91947 | SROCC: 0.93568\n",
            "Training batch[0]: last record -> y: 48.25579783903004 | y_pred: 48.60192217385702\n",
            "Training batch[1]: last record -> y: 38.522899450719365 | y_pred: 40.58991475315429\n",
            "Training batch[2]: last record -> y: 37.30360059432371 | y_pred: 37.673084414030996\n",
            "Training batch[3]: last record -> y: 33.06290047609298 | y_pred: 33.46942321467026\n",
            "Training batch[4]: last record -> y: 55.6992978569499 | y_pred: 53.01759490064387\n",
            "Training batch[5]: last record -> y: 49.50770284408554 | y_pred: 46.985795102813654\n",
            "Training batch[6]: last record -> y: 67.35350020768419 | y_pred: 68.06078154047054\n",
            "Training batch[7]: last record -> y: 69.6393977107623 | y_pred: 68.05110068552221\n",
            "Training batch[8]: last record -> y: 53.02539747675837 | y_pred: 52.06721797303544\n",
            "Training batch[9]: last record -> y: 58.27209923566443 | y_pred: 59.4544243018654\n",
            "Training batch[10]: last record -> y: 54.441201607450694 | y_pred: 59.31401331034408\n",
            "Training batch[11]: last record -> y: 61.766399004917275 | y_pred: 57.54423402524708\n",
            "Training batch[12]: last record -> y: 61.716499182816506 | y_pred: 54.08322546183149\n",
            "Training batch[13]: last record -> y: 38.38710053427974 | y_pred: 37.4006793072009\n",
            "Training batch[14]: last record -> y: 59.43330009744659 | y_pred: 62.09363119955606\n",
            "Training batch[15]: last record -> y: 44.09870203440539 | y_pred: 43.99134103140534\n",
            "Training batch[16]: last record -> y: 58.21779960972003 | y_pred: 59.51396638414394\n",
            "Training batch[17]: last record -> y: 63.12649801677071 | y_pred: 62.322819812053694\n",
            "Training batch[18]: last record -> y: 67.38510289230953 | y_pred: 64.1314994096424\n",
            "Training batch[19]: last record -> y: 61.76470083501272 | y_pred: 62.040286794182975\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.87721338634117\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.994444848057924\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.302595368320226\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.45519633643676\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.708091240915678\n",
            "[Training] Epoch: 994 | L1Loss: 0.04685 | RMSE: 0.07939 | PLCC: 0.95021 | SROCC: 0.95115\n",
            "[Testing]  Epoch: 994 | L1Loss: 0.07462 | RMSE: 0.10136 | PLCC: 0.91830 | SROCC: 0.93518\n",
            "Training batch[0]: last record -> y: 58.383400121492286 | y_pred: 56.0113365769123\n",
            "Training batch[1]: last record -> y: 45.30530160317198 | y_pred: 45.09369773300989\n",
            "Training batch[2]: last record -> y: 29.17069987919399 | y_pred: 40.42588697828228\n",
            "Training batch[3]: last record -> y: 49.544400038424556 | y_pred: 53.95465341617887\n",
            "Training batch[4]: last record -> y: 61.282700394204994 | y_pred: 62.68728309535754\n",
            "Training batch[5]: last record -> y: 44.302501720337546 | y_pred: 45.2429340620655\n",
            "Training batch[6]: last record -> y: 61.76470083501272 | y_pred: 60.59605439874372\n",
            "Training batch[7]: last record -> y: 43.08390078604282 | y_pred: 42.195545654721286\n",
            "Training batch[8]: last record -> y: 46.12870012752876 | y_pred: 44.34798244120941\n",
            "Training batch[9]: last record -> y: 56.977000249892626 | y_pred: 57.707525283247605\n",
            "Training batch[10]: last record -> y: 22.98349944379808 | y_pred: 22.18157187375185\n",
            "Training batch[11]: last record -> y: 26.304599953796185 | y_pred: 34.245947586126476\n",
            "Training batch[12]: last record -> y: 57.950199551824426 | y_pred: 56.86516868596959\n",
            "Training batch[13]: last record -> y: 59.62590086745513 | y_pred: 57.38140912072413\n",
            "Training batch[14]: last record -> y: 49.19730118564098 | y_pred: 59.2753477827066\n",
            "Training batch[15]: last record -> y: 49.19860054291644 | y_pred: 57.064484946055245\n",
            "Training batch[16]: last record -> y: 62.93809764429125 | y_pred: 67.02890532236688\n",
            "Training batch[17]: last record -> y: 59.38040274816581 | y_pred: 61.11642447394797\n",
            "Training batch[18]: last record -> y: 55.8553011357767 | y_pred: 51.252630313173995\n",
            "Training batch[19]: last record -> y: 28.415199204941587 | y_pred: 28.020002423407675\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.02508266341931\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.082601582620896\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 34.87849497007335\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.36201714964784\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.29791672034966\n",
            "[Training] Epoch: 995 | L1Loss: 0.04878 | RMSE: 0.08021 | PLCC: 0.94918 | SROCC: 0.94962\n",
            "[Testing]  Epoch: 995 | L1Loss: 0.07665 | RMSE: 0.10327 | PLCC: 0.91747 | SROCC: 0.93305\n",
            "Training batch[0]: last record -> y: 61.02579752021575 | y_pred: 57.96083241111319\n",
            "Training batch[1]: last record -> y: 46.59859789153563 | y_pred: 44.40184144352327\n",
            "Training batch[2]: last record -> y: 54.97959865673647 | y_pred: 61.69276661515414\n",
            "Training batch[3]: last record -> y: 52.01419840698122 | y_pred: 55.5805513966352\n",
            "Training batch[4]: last record -> y: 58.07179880892954 | y_pred: 58.79881815062549\n",
            "Training batch[5]: last record -> y: 49.19860054291644 | y_pred: 56.709680004083566\n",
            "Training batch[6]: last record -> y: 25.895799721727116 | y_pred: 26.058164303205388\n",
            "Training batch[7]: last record -> y: 33.262500568553776 | y_pred: 44.77136868234743\n",
            "Training batch[8]: last record -> y: 59.38040274816581 | y_pred: 57.90561937559892\n",
            "Training batch[9]: last record -> y: 49.50770284408554 | y_pred: 48.08203131705386\n",
            "Training batch[10]: last record -> y: 65.59839658409192 | y_pred: 62.54099283378787\n",
            "Training batch[11]: last record -> y: 61.54970223315695 | y_pred: 68.02476940330894\n",
            "Training batch[12]: last record -> y: 55.6992978569499 | y_pred: 55.12154700661836\n",
            "Training batch[13]: last record -> y: 54.38009965319543 | y_pred: 65.80317078798066\n",
            "Training batch[14]: last record -> y: 33.94779976733412 | y_pred: 34.24018731662068\n",
            "Training batch[15]: last record -> y: 40.04259909563871 | y_pred: 45.98174732239784\n",
            "Training batch[16]: last record -> y: 47.55770206163652 | y_pred: 38.01807985181915\n",
            "Training batch[17]: last record -> y: 62.94929977644574 | y_pred: 62.25245511286448\n",
            "Training batch[18]: last record -> y: 61.27290053871411 | y_pred: 62.69949512400831\n",
            "Training batch[19]: last record -> y: 52.87200256117512 | y_pred: 54.39077753971651\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.26117989370039\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.759605471757936\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.60008385978222\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.00072683196049\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.076670743863332\n",
            "[Training] Epoch: 996 | L1Loss: 0.04952 | RMSE: 0.07932 | PLCC: 0.95028 | SROCC: 0.95052\n",
            "[Testing]  Epoch: 996 | L1Loss: 0.07510 | RMSE: 0.10282 | PLCC: 0.91954 | SROCC: 0.93455\n",
            "Training batch[0]: last record -> y: 32.369998911570406 | y_pred: 33.49170365409037\n",
            "Training batch[1]: last record -> y: 63.56399868712492 | y_pred: 66.53507880061397\n",
            "Training batch[2]: last record -> y: 27.261600708901653 | y_pred: 26.52005244623487\n",
            "Training batch[3]: last record -> y: 25.83860026161568 | y_pred: 25.434858758691746\n",
            "Training batch[4]: last record -> y: 63.02299970705735 | y_pred: 59.79263027626621\n",
            "Training batch[5]: last record -> y: 41.19950146484996 | y_pred: 43.78229406479272\n",
            "Training batch[6]: last record -> y: 61.97249829592852 | y_pred: 63.97568588843751\n",
            "Training batch[7]: last record -> y: 39.45899952945217 | y_pred: 41.01948902250513\n",
            "Training batch[8]: last record -> y: 54.171601053947825 | y_pred: 54.41288269455367\n",
            "Training batch[9]: last record -> y: 41.601001254850644 | y_pred: 41.492350543267776\n",
            "Training batch[10]: last record -> y: 32.14319995861649 | y_pred: 31.456409232765623\n",
            "Training batch[11]: last record -> y: 72.99610069051369 | y_pred: 68.51171962344074\n",
            "Training batch[12]: last record -> y: 35.17409875023975 | y_pred: 49.56994977653403\n",
            "Training batch[13]: last record -> y: 52.915900896454104 | y_pred: 50.124447277604304\n",
            "Training batch[14]: last record -> y: 29.49510018254307 | y_pred: 45.8228140572395\n",
            "Training batch[15]: last record -> y: 44.533298448275104 | y_pred: 45.17485931926274\n",
            "Training batch[16]: last record -> y: 55.77679937246148 | y_pred: 56.93078944461104\n",
            "Training batch[17]: last record -> y: 27.51029978197414 | y_pred: 26.77161195464612\n",
            "Training batch[18]: last record -> y: 56.801496963241334 | y_pred: 54.600269954305986\n",
            "Training batch[19]: last record -> y: 26.76479917358489 | y_pred: 26.47860447685747\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.58031185701475\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.483512956224104\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 35.565723103323876\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.77735316489361\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.862236992555836\n",
            "[Training] Epoch: 997 | L1Loss: 0.04883 | RMSE: 0.08019 | PLCC: 0.94910 | SROCC: 0.94924\n",
            "[Testing]  Epoch: 997 | L1Loss: 0.07514 | RMSE: 0.10280 | PLCC: 0.91799 | SROCC: 0.93323\n",
            "Training batch[0]: last record -> y: 53.70280002467098 | y_pred: 52.87096371860093\n",
            "Training batch[1]: last record -> y: 41.8042976006501 | y_pred: 36.384755594261264\n",
            "Training batch[2]: last record -> y: 65.06269795251433 | y_pred: 65.41861571037043\n",
            "Training batch[3]: last record -> y: 65.45760286109589 | y_pred: 65.62635849536127\n",
            "Training batch[4]: last record -> y: 32.369998911570406 | y_pred: 32.922489484571656\n",
            "Training batch[5]: last record -> y: 28.55800066006435 | y_pred: 28.47525347198774\n",
            "Training batch[6]: last record -> y: 40.48429855540655 | y_pred: 39.27357899488288\n",
            "Training batch[7]: last record -> y: 44.76750116471442 | y_pred: 43.46252191767974\n",
            "Training batch[8]: last record -> y: 28.55800066006435 | y_pred: 30.193001377968358\n",
            "Training batch[9]: last record -> y: 45.65990070636735 | y_pred: 44.44490034254261\n",
            "Training batch[10]: last record -> y: 54.441799826394345 | y_pred: 56.300723382787965\n",
            "Training batch[11]: last record -> y: 43.907299310399594 | y_pred: 50.53070226496084\n",
            "Training batch[12]: last record -> y: 30.074199437670984 | y_pred: 29.509325571724617\n",
            "Training batch[13]: last record -> y: 69.85279784587078 | y_pred: 60.054479713348655\n",
            "Training batch[14]: last record -> y: 42.00540208510495 | y_pred: 42.37135287519072\n",
            "Training batch[15]: last record -> y: 66.13350021295673 | y_pred: 65.9703376041914\n",
            "Training batch[16]: last record -> y: 22.98349944379808 | y_pred: 22.621693571258703\n",
            "Training batch[17]: last record -> y: 62.20869829174421 | y_pred: 57.57951607799896\n",
            "Training batch[18]: last record -> y: 61.21029982086884 | y_pred: 57.60457373278382\n",
            "Training batch[19]: last record -> y: 63.56399868712492 | y_pred: 65.72131127962689\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 45.05738327014558\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 44.55557567146434\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.76740666319063\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 38.04663998203216\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 24.722627646581316\n",
            "[Training] Epoch: 998 | L1Loss: 0.04858 | RMSE: 0.07910 | PLCC: 0.95037 | SROCC: 0.95175\n",
            "[Testing]  Epoch: 998 | L1Loss: 0.07495 | RMSE: 0.10156 | PLCC: 0.91889 | SROCC: 0.93321\n",
            "Training batch[0]: last record -> y: 32.322799919350985 | y_pred: 29.683714434376043\n",
            "Training batch[1]: last record -> y: 48.6593994359107 | y_pred: 47.82308292022503\n",
            "Training batch[2]: last record -> y: 56.801496963241334 | y_pred: 55.6577152079044\n",
            "Training batch[3]: last record -> y: 70.78559807172087 | y_pred: 69.36844634028989\n",
            "Training batch[4]: last record -> y: 56.15299868224588 | y_pred: 60.31188753558604\n",
            "Training batch[5]: last record -> y: 55.84130088275674 | y_pred: 56.646281660980094\n",
            "Training batch[6]: last record -> y: 64.73490291747157 | y_pred: 61.073066465456805\n",
            "Training batch[7]: last record -> y: 70.18830218633252 | y_pred: 63.774587836444425\n",
            "Training batch[8]: last record -> y: 68.86179992224993 | y_pred: 69.58114533706657\n",
            "Training batch[9]: last record -> y: 39.47439884290486 | y_pred: 39.171098622242994\n",
            "Training batch[10]: last record -> y: 43.31629919695854 | y_pred: 44.73680063285087\n",
            "Training batch[11]: last record -> y: 61.21029982086884 | y_pred: 60.78441617645262\n",
            "Training batch[12]: last record -> y: 69.51529622603266 | y_pred: 67.36666745690627\n",
            "Training batch[13]: last record -> y: 53.02539747675837 | y_pred: 50.358887995128725\n",
            "Training batch[14]: last record -> y: 65.81910077952853 | y_pred: 64.52015839785531\n",
            "Training batch[15]: last record -> y: 38.522899450719365 | y_pred: 40.02946079249455\n",
            "Training batch[16]: last record -> y: 67.43609623290945 | y_pred: 68.32052434632669\n",
            "Training batch[17]: last record -> y: 45.30530160317198 | y_pred: 45.46509681820612\n",
            "Training batch[18]: last record -> y: 53.70280002467098 | y_pred: 53.594056042392594\n",
            "Training batch[19]: last record -> y: 24.490699609431772 | y_pred: 25.432377436567947\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.36092968632329\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 45.70600215979903\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 36.95089745923201\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.26953297643604\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.087671057530116\n",
            "[Training] Epoch: 999 | L1Loss: 0.05175 | RMSE: 0.08178 | PLCC: 0.94764 | SROCC: 0.94753\n",
            "[Testing]  Epoch: 999 | L1Loss: 0.07497 | RMSE: 0.10263 | PLCC: 0.91824 | SROCC: 0.93288\n",
            "Training batch[0]: last record -> y: 67.38690076537137 | y_pred: 69.46089368070011\n",
            "Training batch[1]: last record -> y: 67.35350020768419 | y_pred: 67.68398722797338\n",
            "Training batch[2]: last record -> y: 61.76470083501272 | y_pred: 61.384915427797296\n",
            "Training batch[3]: last record -> y: 52.59410091577138 | y_pred: 52.80404681891491\n",
            "Training batch[4]: last record -> y: 32.14319995861649 | y_pred: 31.447773652853243\n",
            "Training batch[5]: last record -> y: 37.80239940901686 | y_pred: 37.68654434026314\n",
            "Training batch[6]: last record -> y: 36.83800132288775 | y_pred: 38.169525731489216\n",
            "Training batch[7]: last record -> y: 67.19999594025103 | y_pred: 63.603567975639635\n",
            "Training batch[8]: last record -> y: 29.928600665740476 | y_pred: 29.922501887934573\n",
            "Training batch[9]: last record -> y: 28.923200460239684 | y_pred: 28.89635940543195\n",
            "Training batch[10]: last record -> y: 25.219101126348335 | y_pred: 24.512457009305052\n",
            "Training batch[11]: last record -> y: 64.42900076602791 | y_pred: 59.95272138454129\n",
            "Training batch[12]: last record -> y: 62.407599658046365 | y_pred: 62.31598210520315\n",
            "Training batch[13]: last record -> y: 45.98249831230828 | y_pred: 46.216479108816884\n",
            "Training batch[14]: last record -> y: 38.44319964140141 | y_pred: 39.08332285718575\n",
            "Training batch[15]: last record -> y: 30.471100017513493 | y_pred: 52.49687425627371\n",
            "Training batch[16]: last record -> y: 60.352402395979425 | y_pred: 54.82277523903531\n",
            "Training batch[17]: last record -> y: 57.24819927014278 | y_pred: 52.71315291801966\n",
            "Training batch[18]: last record -> y: 46.50630171397677 | y_pred: 46.04406662004112\n",
            "Training batch[19]: last record -> y: 28.998800379243562 | y_pred: 28.525307673170744\n",
            "Testing  batch[0]: last record -> y: 34.75140078251002 | y_pred: 46.688094340113594\n",
            "Testing  batch[1]: last record -> y: 40.61230132819344 | y_pred: 46.01224362360051\n",
            "Testing  batch[2]: last record -> y: 36.83800132288775 | y_pred: 37.190222023347474\n",
            "Testing  batch[3]: last record -> y: 36.818999830849634 | y_pred: 39.631614640773364\n",
            "Testing  batch[4]: last record -> y: 28.967899637007747 | y_pred: 25.939558955020544\n",
            "[Training] Epoch: 1000 | L1Loss: 0.04779 | RMSE: 0.07969 | PLCC: 0.94932 | SROCC: 0.94932\n",
            "[Testing]  Epoch: 1000 | L1Loss: 0.07621 | RMSE: 0.10400 | PLCC: 0.91782 | SROCC: 0.93351\n",
            "Total training & testing time: 0:43:47 (Hour:Minute:Second)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Summary"
      ],
      "metadata": {
        "id": "-WlKx1YCckb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB1sENgfckNd",
        "outputId": "8cfda97b-3ea7-46bf-b9ed-627dec2dfa44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VSFA(\n",
            "  (ann): ANN(\n",
            "    (fc0): Linear(in_features=4096, out_features=128, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (rnn): GRU(128, 32, batch_first=True)\n",
            "  (q): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}